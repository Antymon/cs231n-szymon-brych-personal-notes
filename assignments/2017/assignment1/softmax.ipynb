{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.393702\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Classifier has initially no knowledge of data it will be classifying, so the probability of classifying given sample as an instance of any of C classes is 1/C (here 0.1). Softmax transforms scores to values that can be represented as probabilites. Weights are initially set at random thus small differences from expected value are very likely.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -2.472078 analytic: -2.472078, relative error: 3.019742e-09\n",
      "numerical: 1.580447 analytic: 1.580447, relative error: 8.832478e-09\n",
      "numerical: 2.694756 analytic: 2.694756, relative error: 1.694637e-08\n",
      "numerical: 1.950125 analytic: 1.950125, relative error: 1.335345e-08\n",
      "numerical: 0.558191 analytic: 0.558191, relative error: 7.150936e-08\n",
      "numerical: -0.764298 analytic: -0.764298, relative error: 2.852595e-08\n",
      "numerical: 0.102410 analytic: 0.102409, relative error: 1.743831e-07\n",
      "numerical: 0.548510 analytic: 0.548510, relative error: 4.400031e-08\n",
      "numerical: -0.186663 analytic: -0.186663, relative error: 4.953582e-08\n",
      "numerical: -0.195586 analytic: -0.195586, relative error: 2.263244e-07\n",
      "numerical: -0.470785 analytic: -0.470785, relative error: 4.284455e-08\n",
      "numerical: 5.136394 analytic: 5.136394, relative error: 2.047240e-08\n",
      "numerical: 1.677668 analytic: 1.677668, relative error: 4.926245e-08\n",
      "numerical: 0.988206 analytic: 0.988206, relative error: 9.169239e-09\n",
      "numerical: -2.099311 analytic: -2.099311, relative error: 8.228148e-09\n",
      "numerical: -0.579160 analytic: -0.579161, relative error: 2.767845e-08\n",
      "numerical: -1.918474 analytic: -1.918474, relative error: 1.599505e-08\n",
      "numerical: -0.729810 analytic: -0.729810, relative error: 5.926416e-09\n",
      "numerical: -0.503354 analytic: -0.503354, relative error: 8.167761e-09\n",
      "numerical: -1.458659 analytic: -1.458659, relative error: 1.265829e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.393711e+00 computed in 0.136102s\n",
      "vectorized loss: 2.393711e+00 computed in 0.005004s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for r= 8000.0  and l= 8e-09\n",
      "iteration 0 / 15000: loss 252.752123\n",
      "iteration 100 / 15000: loss 246.552040\n",
      "iteration 200 / 15000: loss 239.391816\n",
      "iteration 300 / 15000: loss 233.237623\n",
      "iteration 400 / 15000: loss 227.606876\n",
      "iteration 500 / 15000: loss 221.615816\n",
      "iteration 600 / 15000: loss 215.715174\n",
      "iteration 700 / 15000: loss 210.157642\n",
      "iteration 800 / 15000: loss 204.866536\n",
      "iteration 900 / 15000: loss 199.655359\n",
      "iteration 1000 / 15000: loss 194.462495\n",
      "iteration 1100 / 15000: loss 189.570276\n",
      "iteration 1200 / 15000: loss 184.496915\n",
      "iteration 1300 / 15000: loss 180.328052\n",
      "iteration 1400 / 15000: loss 175.934977\n",
      "iteration 1500 / 15000: loss 171.203208\n",
      "iteration 1600 / 15000: loss 166.968648\n",
      "iteration 1700 / 15000: loss 162.647116\n",
      "iteration 1800 / 15000: loss 158.097033\n",
      "iteration 1900 / 15000: loss 154.727297\n",
      "iteration 2000 / 15000: loss 150.682042\n",
      "iteration 2100 / 15000: loss 146.885257\n",
      "iteration 2200 / 15000: loss 143.260869\n",
      "iteration 2300 / 15000: loss 139.549521\n",
      "iteration 2400 / 15000: loss 136.069226\n",
      "iteration 2500 / 15000: loss 132.585898\n",
      "iteration 2600 / 15000: loss 129.234865\n",
      "iteration 2700 / 15000: loss 126.032853\n",
      "iteration 2800 / 15000: loss 122.929115\n",
      "iteration 2900 / 15000: loss 119.716433\n",
      "iteration 3000 / 15000: loss 116.699821\n",
      "iteration 3100 / 15000: loss 114.044843\n",
      "iteration 3200 / 15000: loss 110.990727\n",
      "iteration 3300 / 15000: loss 108.136085\n",
      "iteration 3400 / 15000: loss 105.611936\n",
      "iteration 3500 / 15000: loss 102.890522\n",
      "iteration 3600 / 15000: loss 100.285006\n",
      "iteration 3700 / 15000: loss 97.714068\n",
      "iteration 3800 / 15000: loss 95.186043\n",
      "iteration 3900 / 15000: loss 92.880317\n",
      "iteration 4000 / 15000: loss 90.599838\n",
      "iteration 4100 / 15000: loss 88.457179\n",
      "iteration 4200 / 15000: loss 86.161153\n",
      "iteration 4300 / 15000: loss 84.036613\n",
      "iteration 4400 / 15000: loss 81.976121\n",
      "iteration 4500 / 15000: loss 80.070752\n",
      "iteration 4600 / 15000: loss 77.902229\n",
      "iteration 4700 / 15000: loss 76.083314\n",
      "iteration 4800 / 15000: loss 74.096484\n",
      "iteration 4900 / 15000: loss 72.210812\n",
      "iteration 5000 / 15000: loss 70.495439\n",
      "iteration 5100 / 15000: loss 68.859658\n",
      "iteration 5200 / 15000: loss 67.106569\n",
      "iteration 5300 / 15000: loss 65.456780\n",
      "iteration 5400 / 15000: loss 63.611355\n",
      "iteration 5500 / 15000: loss 62.206518\n",
      "iteration 5600 / 15000: loss 60.700228\n",
      "iteration 5700 / 15000: loss 59.189970\n",
      "iteration 5800 / 15000: loss 57.708186\n",
      "iteration 5900 / 15000: loss 56.267908\n",
      "iteration 6000 / 15000: loss 54.830048\n",
      "iteration 6100 / 15000: loss 53.678741\n",
      "iteration 6200 / 15000: loss 52.133746\n",
      "iteration 6300 / 15000: loss 51.024338\n",
      "iteration 6400 / 15000: loss 49.677990\n",
      "iteration 6500 / 15000: loss 48.464194\n",
      "iteration 6600 / 15000: loss 47.265135\n",
      "iteration 6700 / 15000: loss 46.119627\n",
      "iteration 6800 / 15000: loss 44.941459\n",
      "iteration 6900 / 15000: loss 44.012300\n",
      "iteration 7000 / 15000: loss 42.851380\n",
      "iteration 7100 / 15000: loss 41.850206\n",
      "iteration 7200 / 15000: loss 40.817326\n",
      "iteration 7300 / 15000: loss 39.952757\n",
      "iteration 7400 / 15000: loss 38.941771\n",
      "iteration 7500 / 15000: loss 37.798128\n",
      "iteration 7600 / 15000: loss 37.102050\n",
      "iteration 7700 / 15000: loss 35.992439\n",
      "iteration 7800 / 15000: loss 35.171156\n",
      "iteration 7900 / 15000: loss 34.428140\n",
      "iteration 8000 / 15000: loss 33.562821\n",
      "iteration 8100 / 15000: loss 32.756159\n",
      "iteration 8200 / 15000: loss 31.991352\n",
      "iteration 8300 / 15000: loss 31.099168\n",
      "iteration 8400 / 15000: loss 30.449123\n",
      "iteration 8500 / 15000: loss 29.911386\n",
      "iteration 8600 / 15000: loss 29.165409\n",
      "iteration 8700 / 15000: loss 28.316811\n",
      "iteration 8800 / 15000: loss 27.706177\n",
      "iteration 8900 / 15000: loss 27.031751\n",
      "iteration 9000 / 15000: loss 26.351371\n",
      "iteration 9100 / 15000: loss 25.881928\n",
      "iteration 9200 / 15000: loss 25.086252\n",
      "iteration 9300 / 15000: loss 24.558864\n",
      "iteration 9400 / 15000: loss 24.070841\n",
      "iteration 9500 / 15000: loss 23.455431\n",
      "iteration 9600 / 15000: loss 22.920577\n",
      "iteration 9700 / 15000: loss 22.334274\n",
      "iteration 9800 / 15000: loss 21.860885\n",
      "iteration 9900 / 15000: loss 21.441528\n",
      "iteration 10000 / 15000: loss 20.904432\n",
      "iteration 10100 / 15000: loss 20.375845\n",
      "iteration 10200 / 15000: loss 20.000480\n",
      "iteration 10300 / 15000: loss 19.530469\n",
      "iteration 10400 / 15000: loss 19.047964\n",
      "iteration 10500 / 15000: loss 18.617110\n",
      "iteration 10600 / 15000: loss 18.216826\n",
      "iteration 10700 / 15000: loss 17.818963\n",
      "iteration 10800 / 15000: loss 17.441194\n",
      "iteration 10900 / 15000: loss 17.022052\n",
      "iteration 11000 / 15000: loss 16.564921\n",
      "iteration 11100 / 15000: loss 16.210100\n",
      "iteration 11200 / 15000: loss 15.786138\n",
      "iteration 11300 / 15000: loss 15.480115\n",
      "iteration 11400 / 15000: loss 15.083570\n",
      "iteration 11500 / 15000: loss 14.854776\n",
      "iteration 11600 / 15000: loss 14.481815\n",
      "iteration 11700 / 15000: loss 14.228855\n",
      "iteration 11800 / 15000: loss 13.846360\n",
      "iteration 11900 / 15000: loss 13.580081\n",
      "iteration 12000 / 15000: loss 13.331799\n",
      "iteration 12100 / 15000: loss 12.957546\n",
      "iteration 12200 / 15000: loss 12.661156\n",
      "iteration 12300 / 15000: loss 12.419905\n",
      "iteration 12400 / 15000: loss 12.167049\n",
      "iteration 12500 / 15000: loss 11.959623\n",
      "iteration 12600 / 15000: loss 11.678296\n",
      "iteration 12700 / 15000: loss 11.428126\n",
      "iteration 12800 / 15000: loss 11.177243\n",
      "iteration 12900 / 15000: loss 10.904290\n",
      "iteration 13000 / 15000: loss 10.799797\n",
      "iteration 13100 / 15000: loss 10.487579\n",
      "iteration 13200 / 15000: loss 10.269995\n",
      "iteration 13300 / 15000: loss 10.052273\n",
      "iteration 13400 / 15000: loss 9.876171\n",
      "iteration 13500 / 15000: loss 9.583645\n",
      "iteration 13600 / 15000: loss 9.542865\n",
      "iteration 13700 / 15000: loss 9.206286\n",
      "iteration 13800 / 15000: loss 9.054944\n",
      "iteration 13900 / 15000: loss 8.992226\n",
      "iteration 14000 / 15000: loss 8.717031\n",
      "iteration 14100 / 15000: loss 8.586661\n",
      "iteration 14200 / 15000: loss 8.468695\n",
      "iteration 14300 / 15000: loss 8.335359\n",
      "iteration 14400 / 15000: loss 8.038640\n",
      "iteration 14500 / 15000: loss 7.916150\n",
      "iteration 14600 / 15000: loss 7.804884\n",
      "iteration 14700 / 15000: loss 7.533218\n",
      "iteration 14800 / 15000: loss 7.461870\n",
      "iteration 14900 / 15000: loss 7.337994\n",
      "\n",
      "for r= 10000.0  and l= 8e-09\n",
      "iteration 0 / 15000: loss 314.564666\n",
      "iteration 100 / 15000: loss 304.869089\n",
      "iteration 200 / 15000: loss 295.018868\n",
      "iteration 300 / 15000: loss 285.683377\n",
      "iteration 400 / 15000: loss 276.787872\n",
      "iteration 500 / 15000: loss 267.510892\n",
      "iteration 600 / 15000: loss 259.251987\n",
      "iteration 700 / 15000: loss 251.248524\n",
      "iteration 800 / 15000: loss 243.055356\n",
      "iteration 900 / 15000: loss 235.368733\n",
      "iteration 1000 / 15000: loss 228.100596\n",
      "iteration 1100 / 15000: loss 220.765154\n",
      "iteration 1200 / 15000: loss 213.993678\n",
      "iteration 1300 / 15000: loss 206.893433\n",
      "iteration 1400 / 15000: loss 200.305475\n",
      "iteration 1500 / 15000: loss 194.277191\n",
      "iteration 1600 / 15000: loss 188.079710\n",
      "iteration 1700 / 15000: loss 182.176961\n",
      "iteration 1800 / 15000: loss 176.151948\n",
      "iteration 1900 / 15000: loss 170.728505\n",
      "iteration 2000 / 15000: loss 165.592090\n",
      "iteration 2100 / 15000: loss 160.366964\n",
      "iteration 2200 / 15000: loss 155.366317\n",
      "iteration 2300 / 15000: loss 150.187251\n",
      "iteration 2400 / 15000: loss 145.532126\n",
      "iteration 2500 / 15000: loss 141.263457\n",
      "iteration 2600 / 15000: loss 136.817127\n",
      "iteration 2700 / 15000: loss 132.433825\n",
      "iteration 2800 / 15000: loss 128.374531\n",
      "iteration 2900 / 15000: loss 124.277277\n",
      "iteration 3000 / 15000: loss 120.721042\n",
      "iteration 3100 / 15000: loss 116.771081\n",
      "iteration 3200 / 15000: loss 112.934736\n",
      "iteration 3300 / 15000: loss 109.650133\n",
      "iteration 3400 / 15000: loss 106.321008\n",
      "iteration 3500 / 15000: loss 102.842829\n",
      "iteration 3600 / 15000: loss 99.563717\n",
      "iteration 3700 / 15000: loss 96.541910\n",
      "iteration 3800 / 15000: loss 93.626659\n",
      "iteration 3900 / 15000: loss 90.688490\n",
      "iteration 4000 / 15000: loss 87.797093\n",
      "iteration 4100 / 15000: loss 85.006741\n",
      "iteration 4200 / 15000: loss 82.543442\n",
      "iteration 4300 / 15000: loss 79.938216\n",
      "iteration 4400 / 15000: loss 77.451482\n",
      "iteration 4500 / 15000: loss 75.018781\n",
      "iteration 4600 / 15000: loss 72.882483\n",
      "iteration 4700 / 15000: loss 70.367387\n",
      "iteration 4800 / 15000: loss 68.270916\n",
      "iteration 4900 / 15000: loss 66.311074\n",
      "iteration 5000 / 15000: loss 64.307174\n",
      "iteration 5100 / 15000: loss 62.322466\n",
      "iteration 5200 / 15000: loss 60.139800\n",
      "iteration 5300 / 15000: loss 58.504352\n",
      "iteration 5400 / 15000: loss 56.544818\n",
      "iteration 5500 / 15000: loss 54.996023\n",
      "iteration 5600 / 15000: loss 53.276137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5700 / 15000: loss 51.630864\n",
      "iteration 5800 / 15000: loss 50.110574\n",
      "iteration 5900 / 15000: loss 48.611757\n",
      "iteration 6000 / 15000: loss 47.080420\n",
      "iteration 6100 / 15000: loss 45.587756\n",
      "iteration 6200 / 15000: loss 44.293922\n",
      "iteration 6300 / 15000: loss 42.921514\n",
      "iteration 6400 / 15000: loss 41.648869\n",
      "iteration 6500 / 15000: loss 40.439735\n",
      "iteration 6600 / 15000: loss 39.128394\n",
      "iteration 6700 / 15000: loss 38.088224\n",
      "iteration 6800 / 15000: loss 36.776974\n",
      "iteration 6900 / 15000: loss 35.751752\n",
      "iteration 7000 / 15000: loss 34.588053\n",
      "iteration 7100 / 15000: loss 33.588513\n",
      "iteration 7200 / 15000: loss 32.665721\n",
      "iteration 7300 / 15000: loss 31.648661\n",
      "iteration 7400 / 15000: loss 30.772139\n",
      "iteration 7500 / 15000: loss 29.802458\n",
      "iteration 7600 / 15000: loss 28.933235\n",
      "iteration 7700 / 15000: loss 28.079750\n",
      "iteration 7800 / 15000: loss 27.277442\n",
      "iteration 7900 / 15000: loss 26.508630\n",
      "iteration 8000 / 15000: loss 25.634655\n",
      "iteration 8100 / 15000: loss 24.919489\n",
      "iteration 8200 / 15000: loss 24.284267\n",
      "iteration 8300 / 15000: loss 23.537301\n",
      "iteration 8400 / 15000: loss 22.715166\n",
      "iteration 8500 / 15000: loss 22.237708\n",
      "iteration 8600 / 15000: loss 21.509823\n",
      "iteration 8700 / 15000: loss 20.970567\n",
      "iteration 8800 / 15000: loss 20.387117\n",
      "iteration 8900 / 15000: loss 19.682227\n",
      "iteration 9000 / 15000: loss 19.271422\n",
      "iteration 9100 / 15000: loss 18.619025\n",
      "iteration 9200 / 15000: loss 18.112598\n",
      "iteration 9300 / 15000: loss 17.518185\n",
      "iteration 9400 / 15000: loss 17.141073\n",
      "iteration 9500 / 15000: loss 16.617452\n",
      "iteration 9600 / 15000: loss 16.163508\n",
      "iteration 9700 / 15000: loss 15.692811\n",
      "iteration 9800 / 15000: loss 15.278374\n",
      "iteration 9900 / 15000: loss 14.810533\n",
      "iteration 10000 / 15000: loss 14.445896\n",
      "iteration 10100 / 15000: loss 14.035032\n",
      "iteration 10200 / 15000: loss 13.683693\n",
      "iteration 10300 / 15000: loss 13.423717\n",
      "iteration 10400 / 15000: loss 13.019254\n",
      "iteration 10500 / 15000: loss 12.581893\n",
      "iteration 10600 / 15000: loss 12.301835\n",
      "iteration 10700 / 15000: loss 11.990490\n",
      "iteration 10800 / 15000: loss 11.620681\n",
      "iteration 10900 / 15000: loss 11.263564\n",
      "iteration 11000 / 15000: loss 11.082055\n",
      "iteration 11100 / 15000: loss 10.773710\n",
      "iteration 11200 / 15000: loss 10.526482\n",
      "iteration 11300 / 15000: loss 10.236815\n",
      "iteration 11400 / 15000: loss 9.984108\n",
      "iteration 11500 / 15000: loss 9.678363\n",
      "iteration 11600 / 15000: loss 9.443987\n",
      "iteration 11700 / 15000: loss 9.297556\n",
      "iteration 11800 / 15000: loss 8.925755\n",
      "iteration 11900 / 15000: loss 8.836465\n",
      "iteration 12000 / 15000: loss 8.545292\n",
      "iteration 12100 / 15000: loss 8.310489\n",
      "iteration 12200 / 15000: loss 8.105215\n",
      "iteration 12300 / 15000: loss 8.020842\n",
      "iteration 12400 / 15000: loss 7.709650\n",
      "iteration 12500 / 15000: loss 7.564189\n",
      "iteration 12600 / 15000: loss 7.407880\n",
      "iteration 12700 / 15000: loss 7.195491\n",
      "iteration 12800 / 15000: loss 7.063193\n",
      "iteration 12900 / 15000: loss 6.829126\n",
      "iteration 13000 / 15000: loss 6.728859\n",
      "iteration 13100 / 15000: loss 6.617538\n",
      "iteration 13200 / 15000: loss 6.466110\n",
      "iteration 13300 / 15000: loss 6.303288\n",
      "iteration 13400 / 15000: loss 6.194072\n",
      "iteration 13500 / 15000: loss 6.054611\n",
      "iteration 13600 / 15000: loss 5.896344\n",
      "iteration 13700 / 15000: loss 5.802478\n",
      "iteration 13800 / 15000: loss 5.701343\n",
      "iteration 13900 / 15000: loss 5.556256\n",
      "iteration 14000 / 15000: loss 5.497700\n",
      "iteration 14100 / 15000: loss 5.350576\n",
      "iteration 14200 / 15000: loss 5.165745\n",
      "iteration 14300 / 15000: loss 5.134272\n",
      "iteration 14400 / 15000: loss 5.026930\n",
      "iteration 14500 / 15000: loss 4.931760\n",
      "iteration 14600 / 15000: loss 4.808276\n",
      "iteration 14700 / 15000: loss 4.812735\n",
      "iteration 14800 / 15000: loss 4.714599\n",
      "iteration 14900 / 15000: loss 4.544615\n",
      "\n",
      "for r= 12500.0  and l= 8e-09\n",
      "iteration 0 / 15000: loss 384.838283\n",
      "iteration 100 / 15000: loss 369.394890\n",
      "iteration 200 / 15000: loss 355.151609\n",
      "iteration 300 / 15000: loss 341.278584\n",
      "iteration 400 / 15000: loss 327.641709\n",
      "iteration 500 / 15000: loss 314.740926\n",
      "iteration 600 / 15000: loss 302.420224\n",
      "iteration 700 / 15000: loss 290.487083\n",
      "iteration 800 / 15000: loss 278.838990\n",
      "iteration 900 / 15000: loss 267.988500\n",
      "iteration 1000 / 15000: loss 257.452077\n",
      "iteration 1100 / 15000: loss 247.306601\n",
      "iteration 1200 / 15000: loss 237.428770\n",
      "iteration 1300 / 15000: loss 228.498020\n",
      "iteration 1400 / 15000: loss 219.541566\n",
      "iteration 1500 / 15000: loss 210.724768\n",
      "iteration 1600 / 15000: loss 202.526277\n",
      "iteration 1700 / 15000: loss 194.966310\n",
      "iteration 1800 / 15000: loss 187.223817\n",
      "iteration 1900 / 15000: loss 179.737933\n",
      "iteration 2000 / 15000: loss 172.794811\n",
      "iteration 2100 / 15000: loss 166.037814\n",
      "iteration 2200 / 15000: loss 159.748801\n",
      "iteration 2300 / 15000: loss 153.245099\n",
      "iteration 2400 / 15000: loss 147.409710\n",
      "iteration 2500 / 15000: loss 141.571298\n",
      "iteration 2600 / 15000: loss 136.370481\n",
      "iteration 2700 / 15000: loss 130.910221\n",
      "iteration 2800 / 15000: loss 125.693714\n",
      "iteration 2900 / 15000: loss 120.865858\n",
      "iteration 3000 / 15000: loss 116.215878\n",
      "iteration 3100 / 15000: loss 111.841336\n",
      "iteration 3200 / 15000: loss 107.571006\n",
      "iteration 3300 / 15000: loss 103.304708\n",
      "iteration 3400 / 15000: loss 99.306919\n",
      "iteration 3500 / 15000: loss 95.463433\n",
      "iteration 3600 / 15000: loss 91.804638\n",
      "iteration 3700 / 15000: loss 88.157304\n",
      "iteration 3800 / 15000: loss 84.729495\n",
      "iteration 3900 / 15000: loss 81.560100\n",
      "iteration 4000 / 15000: loss 78.353681\n",
      "iteration 4100 / 15000: loss 75.423383\n",
      "iteration 4200 / 15000: loss 72.502314\n",
      "iteration 4300 / 15000: loss 69.753555\n",
      "iteration 4400 / 15000: loss 66.986503\n",
      "iteration 4500 / 15000: loss 64.541030\n",
      "iteration 4600 / 15000: loss 62.024870\n",
      "iteration 4700 / 15000: loss 59.669199\n",
      "iteration 4800 / 15000: loss 57.389579\n",
      "iteration 4900 / 15000: loss 55.181352\n",
      "iteration 5000 / 15000: loss 53.058785\n",
      "iteration 5100 / 15000: loss 51.154347\n",
      "iteration 5200 / 15000: loss 49.307923\n",
      "iteration 5300 / 15000: loss 47.287012\n",
      "iteration 5400 / 15000: loss 45.641013\n",
      "iteration 5500 / 15000: loss 43.924788\n",
      "iteration 5600 / 15000: loss 42.172774\n",
      "iteration 5700 / 15000: loss 40.629056\n",
      "iteration 5800 / 15000: loss 39.038320\n",
      "iteration 5900 / 15000: loss 37.555283\n",
      "iteration 6000 / 15000: loss 36.255527\n",
      "iteration 6100 / 15000: loss 34.904005\n",
      "iteration 6200 / 15000: loss 33.649610\n",
      "iteration 6300 / 15000: loss 32.306406\n",
      "iteration 6400 / 15000: loss 31.109222\n",
      "iteration 6500 / 15000: loss 30.008234\n",
      "iteration 6600 / 15000: loss 28.840331\n",
      "iteration 6700 / 15000: loss 27.853722\n",
      "iteration 6800 / 15000: loss 26.804366\n",
      "iteration 6900 / 15000: loss 25.881778\n",
      "iteration 7000 / 15000: loss 24.854919\n",
      "iteration 7100 / 15000: loss 24.058339\n",
      "iteration 7200 / 15000: loss 23.133132\n",
      "iteration 7300 / 15000: loss 22.346762\n",
      "iteration 7400 / 15000: loss 21.514986\n",
      "iteration 7500 / 15000: loss 20.758746\n",
      "iteration 7600 / 15000: loss 19.987064\n",
      "iteration 7700 / 15000: loss 19.262924\n",
      "iteration 7800 / 15000: loss 18.608370\n",
      "iteration 7900 / 15000: loss 17.927182\n",
      "iteration 8000 / 15000: loss 17.320214\n",
      "iteration 8100 / 15000: loss 16.746732\n",
      "iteration 8200 / 15000: loss 16.193243\n",
      "iteration 8300 / 15000: loss 15.605688\n",
      "iteration 8400 / 15000: loss 15.107622\n",
      "iteration 8500 / 15000: loss 14.578100\n",
      "iteration 8600 / 15000: loss 13.970496\n",
      "iteration 8700 / 15000: loss 13.468447\n",
      "iteration 8800 / 15000: loss 13.184944\n",
      "iteration 8900 / 15000: loss 12.646934\n",
      "iteration 9000 / 15000: loss 12.319045\n",
      "iteration 9100 / 15000: loss 11.911274\n",
      "iteration 9200 / 15000: loss 11.473633\n",
      "iteration 9300 / 15000: loss 11.144141\n",
      "iteration 9400 / 15000: loss 10.738287\n",
      "iteration 9500 / 15000: loss 10.365883\n",
      "iteration 9600 / 15000: loss 10.087959\n",
      "iteration 9700 / 15000: loss 9.730775\n",
      "iteration 9800 / 15000: loss 9.483462\n",
      "iteration 9900 / 15000: loss 9.142112\n",
      "iteration 10000 / 15000: loss 8.834739\n",
      "iteration 10100 / 15000: loss 8.645686\n",
      "iteration 10200 / 15000: loss 8.355694\n",
      "iteration 10300 / 15000: loss 8.078009\n",
      "iteration 10400 / 15000: loss 7.924680\n",
      "iteration 10500 / 15000: loss 7.612990\n",
      "iteration 10600 / 15000: loss 7.449835\n",
      "iteration 10700 / 15000: loss 7.232499\n",
      "iteration 10800 / 15000: loss 7.043898\n",
      "iteration 10900 / 15000: loss 6.793490\n",
      "iteration 11000 / 15000: loss 6.655630\n",
      "iteration 11100 / 15000: loss 6.473420\n",
      "iteration 11200 / 15000: loss 6.195983\n",
      "iteration 11300 / 15000: loss 6.027734\n",
      "iteration 11400 / 15000: loss 5.868923\n",
      "iteration 11500 / 15000: loss 5.751606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11600 / 15000: loss 5.660914\n",
      "iteration 11700 / 15000: loss 5.530702\n",
      "iteration 11800 / 15000: loss 5.324069\n",
      "iteration 11900 / 15000: loss 5.203950\n",
      "iteration 12000 / 15000: loss 5.151467\n",
      "iteration 12100 / 15000: loss 5.008450\n",
      "iteration 12200 / 15000: loss 4.798388\n",
      "iteration 12300 / 15000: loss 4.737577\n",
      "iteration 12400 / 15000: loss 4.599275\n",
      "iteration 12500 / 15000: loss 4.578937\n",
      "iteration 12600 / 15000: loss 4.441281\n",
      "iteration 12700 / 15000: loss 4.381971\n",
      "iteration 12800 / 15000: loss 4.250731\n",
      "iteration 12900 / 15000: loss 4.185591\n",
      "iteration 13000 / 15000: loss 4.089492\n",
      "iteration 13100 / 15000: loss 4.024511\n",
      "iteration 13200 / 15000: loss 3.939313\n",
      "iteration 13300 / 15000: loss 3.890887\n",
      "iteration 13400 / 15000: loss 3.759804\n",
      "iteration 13500 / 15000: loss 3.673930\n",
      "iteration 13600 / 15000: loss 3.607477\n",
      "iteration 13700 / 15000: loss 3.545198\n",
      "iteration 13800 / 15000: loss 3.478368\n",
      "iteration 13900 / 15000: loss 3.427031\n",
      "iteration 14000 / 15000: loss 3.341348\n",
      "iteration 14100 / 15000: loss 3.331984\n",
      "iteration 14200 / 15000: loss 3.305938\n",
      "iteration 14300 / 15000: loss 3.335773\n",
      "iteration 14400 / 15000: loss 3.196366\n",
      "iteration 14500 / 15000: loss 3.125318\n",
      "iteration 14600 / 15000: loss 3.042031\n",
      "iteration 14700 / 15000: loss 3.046093\n",
      "iteration 14800 / 15000: loss 3.023121\n",
      "iteration 14900 / 15000: loss 3.022243\n",
      "\n",
      "for r= 15000.0  and l= 8e-09\n",
      "iteration 0 / 15000: loss 467.189710\n",
      "iteration 100 / 15000: loss 444.965441\n",
      "iteration 200 / 15000: loss 424.134813\n",
      "iteration 300 / 15000: loss 403.827105\n",
      "iteration 400 / 15000: loss 385.149023\n",
      "iteration 500 / 15000: loss 367.058632\n",
      "iteration 600 / 15000: loss 349.753222\n",
      "iteration 700 / 15000: loss 333.181150\n",
      "iteration 800 / 15000: loss 317.631057\n",
      "iteration 900 / 15000: loss 302.884399\n",
      "iteration 1000 / 15000: loss 288.228380\n",
      "iteration 1100 / 15000: loss 274.869187\n",
      "iteration 1200 / 15000: loss 262.089288\n",
      "iteration 1300 / 15000: loss 249.848317\n",
      "iteration 1400 / 15000: loss 238.179720\n",
      "iteration 1500 / 15000: loss 227.162218\n",
      "iteration 1600 / 15000: loss 216.587360\n",
      "iteration 1700 / 15000: loss 206.518447\n",
      "iteration 1800 / 15000: loss 196.862058\n",
      "iteration 1900 / 15000: loss 187.539809\n",
      "iteration 2000 / 15000: loss 178.736473\n",
      "iteration 2100 / 15000: loss 170.420605\n",
      "iteration 2200 / 15000: loss 162.381500\n",
      "iteration 2300 / 15000: loss 155.019726\n",
      "iteration 2400 / 15000: loss 147.837332\n",
      "iteration 2500 / 15000: loss 141.127668\n",
      "iteration 2600 / 15000: loss 134.277936\n",
      "iteration 2700 / 15000: loss 128.278490\n",
      "iteration 2800 / 15000: loss 122.291876\n",
      "iteration 2900 / 15000: loss 116.545711\n",
      "iteration 3000 / 15000: loss 111.184878\n",
      "iteration 3100 / 15000: loss 106.115792\n",
      "iteration 3200 / 15000: loss 101.106742\n",
      "iteration 3300 / 15000: loss 96.557344\n",
      "iteration 3400 / 15000: loss 92.022582\n",
      "iteration 3500 / 15000: loss 87.759586\n",
      "iteration 3600 / 15000: loss 83.778846\n",
      "iteration 3700 / 15000: loss 79.952347\n",
      "iteration 3800 / 15000: loss 76.268618\n",
      "iteration 3900 / 15000: loss 72.778733\n",
      "iteration 4000 / 15000: loss 69.437967\n",
      "iteration 4100 / 15000: loss 66.249506\n",
      "iteration 4200 / 15000: loss 63.159567\n",
      "iteration 4300 / 15000: loss 60.332274\n",
      "iteration 4400 / 15000: loss 57.644113\n",
      "iteration 4500 / 15000: loss 54.869934\n",
      "iteration 4600 / 15000: loss 52.483140\n",
      "iteration 4700 / 15000: loss 50.103200\n",
      "iteration 4800 / 15000: loss 47.974621\n",
      "iteration 4900 / 15000: loss 45.760458\n",
      "iteration 5000 / 15000: loss 43.623283\n",
      "iteration 5100 / 15000: loss 41.699416\n",
      "iteration 5200 / 15000: loss 39.908553\n",
      "iteration 5300 / 15000: loss 38.121968\n",
      "iteration 5400 / 15000: loss 36.360865\n",
      "iteration 5500 / 15000: loss 34.801766\n",
      "iteration 5600 / 15000: loss 33.172770\n",
      "iteration 5700 / 15000: loss 31.745626\n",
      "iteration 5800 / 15000: loss 30.371559\n",
      "iteration 5900 / 15000: loss 29.007627\n",
      "iteration 6000 / 15000: loss 27.746039\n",
      "iteration 6100 / 15000: loss 26.514965\n",
      "iteration 6200 / 15000: loss 25.376831\n",
      "iteration 6300 / 15000: loss 24.362429\n",
      "iteration 6400 / 15000: loss 23.167688\n",
      "iteration 6500 / 15000: loss 22.352450\n",
      "iteration 6600 / 15000: loss 21.379396\n",
      "iteration 6700 / 15000: loss 20.382640\n",
      "iteration 6800 / 15000: loss 19.608714\n",
      "iteration 6900 / 15000: loss 18.710832\n",
      "iteration 7000 / 15000: loss 17.907681\n",
      "iteration 7100 / 15000: loss 17.112124\n",
      "iteration 7200 / 15000: loss 16.402520\n",
      "iteration 7300 / 15000: loss 15.778923\n",
      "iteration 7400 / 15000: loss 15.144702\n",
      "iteration 7500 / 15000: loss 14.542306\n",
      "iteration 7600 / 15000: loss 13.984843\n",
      "iteration 7700 / 15000: loss 13.326151\n",
      "iteration 7800 / 15000: loss 12.867697\n",
      "iteration 7900 / 15000: loss 12.301666\n",
      "iteration 8000 / 15000: loss 11.770511\n",
      "iteration 8100 / 15000: loss 11.451283\n",
      "iteration 8200 / 15000: loss 10.916195\n",
      "iteration 8300 / 15000: loss 10.570788\n",
      "iteration 8400 / 15000: loss 10.148368\n",
      "iteration 8500 / 15000: loss 9.749193\n",
      "iteration 8600 / 15000: loss 9.354950\n",
      "iteration 8700 / 15000: loss 9.066711\n",
      "iteration 8800 / 15000: loss 8.779229\n",
      "iteration 8900 / 15000: loss 8.427485\n",
      "iteration 9000 / 15000: loss 8.168317\n",
      "iteration 9100 / 15000: loss 7.830810\n",
      "iteration 9200 / 15000: loss 7.609960\n",
      "iteration 9300 / 15000: loss 7.269483\n",
      "iteration 9400 / 15000: loss 7.044225\n",
      "iteration 9500 / 15000: loss 6.894903\n",
      "iteration 9600 / 15000: loss 6.548552\n",
      "iteration 9700 / 15000: loss 6.411008\n",
      "iteration 9800 / 15000: loss 6.181598\n",
      "iteration 9900 / 15000: loss 6.008201\n",
      "iteration 10000 / 15000: loss 5.759443\n",
      "iteration 10100 / 15000: loss 5.638182\n",
      "iteration 10200 / 15000: loss 5.502531\n",
      "iteration 10300 / 15000: loss 5.238172\n",
      "iteration 10400 / 15000: loss 5.093001\n",
      "iteration 10500 / 15000: loss 5.033291\n",
      "iteration 10600 / 15000: loss 4.876778\n",
      "iteration 10700 / 15000: loss 4.737740\n",
      "iteration 10800 / 15000: loss 4.571740\n",
      "iteration 10900 / 15000: loss 4.478459\n",
      "iteration 11000 / 15000: loss 4.308761\n",
      "iteration 11100 / 15000: loss 4.240569\n",
      "iteration 11200 / 15000: loss 4.132661\n",
      "iteration 11300 / 15000: loss 4.053254\n",
      "iteration 11400 / 15000: loss 3.969707\n",
      "iteration 11500 / 15000: loss 3.802891\n",
      "iteration 11600 / 15000: loss 3.766225\n",
      "iteration 11700 / 15000: loss 3.689987\n",
      "iteration 11800 / 15000: loss 3.574203\n",
      "iteration 11900 / 15000: loss 3.508960\n",
      "iteration 12000 / 15000: loss 3.447758\n",
      "iteration 12100 / 15000: loss 3.397079\n",
      "iteration 12200 / 15000: loss 3.304854\n",
      "iteration 12300 / 15000: loss 3.302232\n",
      "iteration 12400 / 15000: loss 3.229581\n",
      "iteration 12500 / 15000: loss 3.214872\n",
      "iteration 12600 / 15000: loss 3.094203\n",
      "iteration 12700 / 15000: loss 3.019788\n",
      "iteration 12800 / 15000: loss 2.963345\n",
      "iteration 12900 / 15000: loss 2.992789\n",
      "iteration 13000 / 15000: loss 2.855331\n",
      "iteration 13100 / 15000: loss 2.915324\n",
      "iteration 13200 / 15000: loss 2.916068\n",
      "iteration 13300 / 15000: loss 2.853862\n",
      "iteration 13400 / 15000: loss 2.765962\n",
      "iteration 13500 / 15000: loss 2.770403\n",
      "iteration 13600 / 15000: loss 2.785034\n",
      "iteration 13700 / 15000: loss 2.703023\n",
      "iteration 13800 / 15000: loss 2.590132\n",
      "iteration 13900 / 15000: loss 2.558464\n",
      "iteration 14000 / 15000: loss 2.599073\n",
      "iteration 14100 / 15000: loss 2.536963\n",
      "iteration 14200 / 15000: loss 2.518716\n",
      "iteration 14300 / 15000: loss 2.559059\n",
      "iteration 14400 / 15000: loss 2.526691\n",
      "iteration 14500 / 15000: loss 2.517317\n",
      "iteration 14600 / 15000: loss 2.387580\n",
      "iteration 14700 / 15000: loss 2.389325\n",
      "iteration 14800 / 15000: loss 2.437425\n",
      "iteration 14900 / 15000: loss 2.409683\n",
      "\n",
      "for r= 20000.0  and l= 8e-09\n",
      "iteration 0 / 15000: loss 616.793439\n",
      "iteration 100 / 15000: loss 577.934194\n",
      "iteration 200 / 15000: loss 541.463624\n",
      "iteration 300 / 15000: loss 508.188847\n",
      "iteration 400 / 15000: loss 476.626099\n",
      "iteration 500 / 15000: loss 447.253549\n",
      "iteration 600 / 15000: loss 419.417137\n",
      "iteration 700 / 15000: loss 393.714793\n",
      "iteration 800 / 15000: loss 369.135513\n",
      "iteration 900 / 15000: loss 346.425020\n",
      "iteration 1000 / 15000: loss 324.636445\n",
      "iteration 1100 / 15000: loss 304.739376\n",
      "iteration 1200 / 15000: loss 285.964177\n",
      "iteration 1300 / 15000: loss 268.429635\n",
      "iteration 1400 / 15000: loss 251.587536\n",
      "iteration 1500 / 15000: loss 236.032950\n",
      "iteration 1600 / 15000: loss 221.544559\n",
      "iteration 1700 / 15000: loss 207.811317\n",
      "iteration 1800 / 15000: loss 195.262984\n",
      "iteration 1900 / 15000: loss 183.161388\n",
      "iteration 2000 / 15000: loss 171.748606\n",
      "iteration 2100 / 15000: loss 161.264670\n",
      "iteration 2200 / 15000: loss 151.429230\n",
      "iteration 2300 / 15000: loss 142.026809\n",
      "iteration 2400 / 15000: loss 133.304717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2500 / 15000: loss 125.333573\n",
      "iteration 2600 / 15000: loss 117.611146\n",
      "iteration 2700 / 15000: loss 110.374917\n",
      "iteration 2800 / 15000: loss 103.631927\n",
      "iteration 2900 / 15000: loss 97.302898\n",
      "iteration 3000 / 15000: loss 91.488882\n",
      "iteration 3100 / 15000: loss 85.804174\n",
      "iteration 3200 / 15000: loss 80.604982\n",
      "iteration 3300 / 15000: loss 75.735764\n",
      "iteration 3400 / 15000: loss 71.135645\n",
      "iteration 3500 / 15000: loss 66.857593\n",
      "iteration 3600 / 15000: loss 62.862016\n",
      "iteration 3700 / 15000: loss 59.073395\n",
      "iteration 3800 / 15000: loss 55.517065\n",
      "iteration 3900 / 15000: loss 52.099663\n",
      "iteration 4000 / 15000: loss 49.050348\n",
      "iteration 4100 / 15000: loss 46.075734\n",
      "iteration 4200 / 15000: loss 43.440599\n",
      "iteration 4300 / 15000: loss 40.831038\n",
      "iteration 4400 / 15000: loss 38.377714\n",
      "iteration 4500 / 15000: loss 36.211284\n",
      "iteration 4600 / 15000: loss 33.998740\n",
      "iteration 4700 / 15000: loss 32.059190\n",
      "iteration 4800 / 15000: loss 30.144802\n",
      "iteration 4900 / 15000: loss 28.465002\n",
      "iteration 5000 / 15000: loss 26.790895\n",
      "iteration 5100 / 15000: loss 25.336657\n",
      "iteration 5200 / 15000: loss 23.834062\n",
      "iteration 5300 / 15000: loss 22.422014\n",
      "iteration 5400 / 15000: loss 21.116727\n",
      "iteration 5500 / 15000: loss 20.057893\n",
      "iteration 5600 / 15000: loss 18.885877\n",
      "iteration 5700 / 15000: loss 17.822139\n",
      "iteration 5800 / 15000: loss 16.822443\n",
      "iteration 5900 / 15000: loss 15.966725\n",
      "iteration 6000 / 15000: loss 15.061287\n",
      "iteration 6100 / 15000: loss 14.264889\n",
      "iteration 6200 / 15000: loss 13.523371\n",
      "iteration 6300 / 15000: loss 12.763215\n",
      "iteration 6400 / 15000: loss 12.038448\n",
      "iteration 6500 / 15000: loss 11.474582\n",
      "iteration 6600 / 15000: loss 10.945558\n",
      "iteration 6700 / 15000: loss 10.439931\n",
      "iteration 6800 / 15000: loss 9.907678\n",
      "iteration 6900 / 15000: loss 9.386038\n",
      "iteration 7000 / 15000: loss 8.935759\n",
      "iteration 7100 / 15000: loss 8.502694\n",
      "iteration 7200 / 15000: loss 8.026943\n",
      "iteration 7300 / 15000: loss 7.773553\n",
      "iteration 7400 / 15000: loss 7.385975\n",
      "iteration 7500 / 15000: loss 7.005956\n",
      "iteration 7600 / 15000: loss 6.764314\n",
      "iteration 7700 / 15000: loss 6.450965\n",
      "iteration 7800 / 15000: loss 6.169456\n",
      "iteration 7900 / 15000: loss 5.901576\n",
      "iteration 8000 / 15000: loss 5.683692\n",
      "iteration 8100 / 15000: loss 5.424395\n",
      "iteration 8200 / 15000: loss 5.265094\n",
      "iteration 8300 / 15000: loss 5.048471\n",
      "iteration 8400 / 15000: loss 4.846070\n",
      "iteration 8500 / 15000: loss 4.703195\n",
      "iteration 8600 / 15000: loss 4.494116\n",
      "iteration 8700 / 15000: loss 4.395224\n",
      "iteration 8800 / 15000: loss 4.168255\n",
      "iteration 8900 / 15000: loss 4.163916\n",
      "iteration 9000 / 15000: loss 3.984308\n",
      "iteration 9100 / 15000: loss 3.836424\n",
      "iteration 9200 / 15000: loss 3.682869\n",
      "iteration 9300 / 15000: loss 3.632298\n",
      "iteration 9400 / 15000: loss 3.491828\n",
      "iteration 9500 / 15000: loss 3.460753\n",
      "iteration 9600 / 15000: loss 3.436495\n",
      "iteration 9700 / 15000: loss 3.250660\n",
      "iteration 9800 / 15000: loss 3.240678\n",
      "iteration 9900 / 15000: loss 3.135487\n",
      "iteration 10000 / 15000: loss 3.082351\n",
      "iteration 10100 / 15000: loss 3.038850\n",
      "iteration 10200 / 15000: loss 2.913648\n",
      "iteration 10300 / 15000: loss 2.849331\n",
      "iteration 10400 / 15000: loss 2.790717\n",
      "iteration 10500 / 15000: loss 2.803356\n",
      "iteration 10600 / 15000: loss 2.752723\n",
      "iteration 10700 / 15000: loss 2.691073\n",
      "iteration 10800 / 15000: loss 2.631233\n",
      "iteration 10900 / 15000: loss 2.640706\n",
      "iteration 11000 / 15000: loss 2.586919\n",
      "iteration 11100 / 15000: loss 2.523792\n",
      "iteration 11200 / 15000: loss 2.527112\n",
      "iteration 11300 / 15000: loss 2.471921\n",
      "iteration 11400 / 15000: loss 2.482755\n",
      "iteration 11500 / 15000: loss 2.411659\n",
      "iteration 11600 / 15000: loss 2.386426\n",
      "iteration 11700 / 15000: loss 2.388864\n",
      "iteration 11800 / 15000: loss 2.469773\n",
      "iteration 11900 / 15000: loss 2.340444\n",
      "iteration 12000 / 15000: loss 2.327674\n",
      "iteration 12100 / 15000: loss 2.411357\n",
      "iteration 12200 / 15000: loss 2.359167\n",
      "iteration 12300 / 15000: loss 2.236537\n",
      "iteration 12400 / 15000: loss 2.215542\n",
      "iteration 12500 / 15000: loss 2.239501\n",
      "iteration 12600 / 15000: loss 2.215200\n",
      "iteration 12700 / 15000: loss 2.283677\n",
      "iteration 12800 / 15000: loss 2.197853\n",
      "iteration 12900 / 15000: loss 2.237580\n",
      "iteration 13000 / 15000: loss 2.200626\n",
      "iteration 13100 / 15000: loss 2.224306\n",
      "iteration 13200 / 15000: loss 2.205735\n",
      "iteration 13300 / 15000: loss 2.224810\n",
      "iteration 13400 / 15000: loss 2.136642\n",
      "iteration 13500 / 15000: loss 2.166207\n",
      "iteration 13600 / 15000: loss 2.153901\n",
      "iteration 13700 / 15000: loss 2.173992\n",
      "iteration 13800 / 15000: loss 2.058823\n",
      "iteration 13900 / 15000: loss 2.064759\n",
      "iteration 14000 / 15000: loss 2.178613\n",
      "iteration 14100 / 15000: loss 2.199405\n",
      "iteration 14200 / 15000: loss 2.149739\n",
      "iteration 14300 / 15000: loss 2.098973\n",
      "iteration 14400 / 15000: loss 2.103623\n",
      "iteration 14500 / 15000: loss 2.138119\n",
      "iteration 14600 / 15000: loss 2.100203\n",
      "iteration 14700 / 15000: loss 2.098305\n",
      "iteration 14800 / 15000: loss 2.066957\n",
      "iteration 14900 / 15000: loss 2.071664\n",
      "\n",
      "for r= 8000.0  and l= 1e-08\n",
      "iteration 0 / 15000: loss 248.952559\n",
      "iteration 100 / 15000: loss 241.148310\n",
      "iteration 200 / 15000: loss 233.170732\n",
      "iteration 300 / 15000: loss 225.413550\n",
      "iteration 400 / 15000: loss 217.948812\n",
      "iteration 500 / 15000: loss 211.476121\n",
      "iteration 600 / 15000: loss 204.226131\n",
      "iteration 700 / 15000: loss 197.902537\n",
      "iteration 800 / 15000: loss 191.861118\n",
      "iteration 900 / 15000: loss 185.921429\n",
      "iteration 1000 / 15000: loss 179.767107\n",
      "iteration 1100 / 15000: loss 174.160429\n",
      "iteration 1200 / 15000: loss 168.625085\n",
      "iteration 1300 / 15000: loss 163.222313\n",
      "iteration 1400 / 15000: loss 158.357525\n",
      "iteration 1500 / 15000: loss 153.050260\n",
      "iteration 1600 / 15000: loss 148.426046\n",
      "iteration 1700 / 15000: loss 143.734910\n",
      "iteration 1800 / 15000: loss 139.446600\n",
      "iteration 1900 / 15000: loss 134.807574\n",
      "iteration 2000 / 15000: loss 130.690363\n",
      "iteration 2100 / 15000: loss 126.548767\n",
      "iteration 2200 / 15000: loss 122.748888\n",
      "iteration 2300 / 15000: loss 118.865020\n",
      "iteration 2400 / 15000: loss 115.114133\n",
      "iteration 2500 / 15000: loss 111.639069\n",
      "iteration 2600 / 15000: loss 108.193598\n",
      "iteration 2700 / 15000: loss 104.689920\n",
      "iteration 2800 / 15000: loss 101.342039\n",
      "iteration 2900 / 15000: loss 98.170306\n",
      "iteration 3000 / 15000: loss 95.185665\n",
      "iteration 3100 / 15000: loss 92.330520\n",
      "iteration 3200 / 15000: loss 89.310938\n",
      "iteration 3300 / 15000: loss 86.667214\n",
      "iteration 3400 / 15000: loss 84.010885\n",
      "iteration 3500 / 15000: loss 81.229152\n",
      "iteration 3600 / 15000: loss 78.731179\n",
      "iteration 3700 / 15000: loss 76.387275\n",
      "iteration 3800 / 15000: loss 74.012318\n",
      "iteration 3900 / 15000: loss 71.701854\n",
      "iteration 4000 / 15000: loss 69.540638\n",
      "iteration 4100 / 15000: loss 67.468575\n",
      "iteration 4200 / 15000: loss 65.255302\n",
      "iteration 4300 / 15000: loss 63.287316\n",
      "iteration 4400 / 15000: loss 61.400590\n",
      "iteration 4500 / 15000: loss 59.419631\n",
      "iteration 4600 / 15000: loss 57.519321\n",
      "iteration 4700 / 15000: loss 55.828933\n",
      "iteration 4800 / 15000: loss 54.242613\n",
      "iteration 4900 / 15000: loss 52.458659\n",
      "iteration 5000 / 15000: loss 50.868262\n",
      "iteration 5100 / 15000: loss 49.470711\n",
      "iteration 5200 / 15000: loss 47.901963\n",
      "iteration 5300 / 15000: loss 46.437162\n",
      "iteration 5400 / 15000: loss 45.037422\n",
      "iteration 5500 / 15000: loss 43.674461\n",
      "iteration 5600 / 15000: loss 42.273111\n",
      "iteration 5700 / 15000: loss 41.003426\n",
      "iteration 5800 / 15000: loss 39.796469\n",
      "iteration 5900 / 15000: loss 38.669399\n",
      "iteration 6000 / 15000: loss 37.512801\n",
      "iteration 6100 / 15000: loss 36.293286\n",
      "iteration 6200 / 15000: loss 35.128783\n",
      "iteration 6300 / 15000: loss 34.148817\n",
      "iteration 6400 / 15000: loss 33.251232\n",
      "iteration 6500 / 15000: loss 32.133950\n",
      "iteration 6600 / 15000: loss 31.175304\n",
      "iteration 6700 / 15000: loss 30.326228\n",
      "iteration 6800 / 15000: loss 29.405437\n",
      "iteration 6900 / 15000: loss 28.471870\n",
      "iteration 7000 / 15000: loss 27.620243\n",
      "iteration 7100 / 15000: loss 26.852771\n",
      "iteration 7200 / 15000: loss 26.084422\n",
      "iteration 7300 / 15000: loss 25.258192\n",
      "iteration 7400 / 15000: loss 24.577137\n",
      "iteration 7500 / 15000: loss 23.806028\n",
      "iteration 7600 / 15000: loss 23.181226\n",
      "iteration 7700 / 15000: loss 22.557709\n",
      "iteration 7800 / 15000: loss 21.791336\n",
      "iteration 7900 / 15000: loss 21.154330\n",
      "iteration 8000 / 15000: loss 20.570310\n",
      "iteration 8100 / 15000: loss 20.193761\n",
      "iteration 8200 / 15000: loss 19.471930\n",
      "iteration 8300 / 15000: loss 18.914439\n",
      "iteration 8400 / 15000: loss 18.271586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8500 / 15000: loss 17.944631\n",
      "iteration 8600 / 15000: loss 17.339287\n",
      "iteration 8700 / 15000: loss 16.822783\n",
      "iteration 8800 / 15000: loss 16.370656\n",
      "iteration 8900 / 15000: loss 15.973142\n",
      "iteration 9000 / 15000: loss 15.462131\n",
      "iteration 9100 / 15000: loss 15.162753\n",
      "iteration 9200 / 15000: loss 14.659836\n",
      "iteration 9300 / 15000: loss 14.327755\n",
      "iteration 9400 / 15000: loss 13.776145\n",
      "iteration 9500 / 15000: loss 13.429341\n",
      "iteration 9600 / 15000: loss 13.133707\n",
      "iteration 9700 / 15000: loss 12.809114\n",
      "iteration 9800 / 15000: loss 12.406277\n",
      "iteration 9900 / 15000: loss 12.064785\n",
      "iteration 10000 / 15000: loss 11.706566\n",
      "iteration 10100 / 15000: loss 11.432409\n",
      "iteration 10200 / 15000: loss 11.153941\n",
      "iteration 10300 / 15000: loss 10.852551\n",
      "iteration 10400 / 15000: loss 10.609739\n",
      "iteration 10500 / 15000: loss 10.302291\n",
      "iteration 10600 / 15000: loss 10.121624\n",
      "iteration 10700 / 15000: loss 9.797875\n",
      "iteration 10800 / 15000: loss 9.505260\n",
      "iteration 10900 / 15000: loss 9.285150\n",
      "iteration 11000 / 15000: loss 9.083255\n",
      "iteration 11100 / 15000: loss 8.822320\n",
      "iteration 11200 / 15000: loss 8.554395\n",
      "iteration 11300 / 15000: loss 8.438579\n",
      "iteration 11400 / 15000: loss 8.193883\n",
      "iteration 11500 / 15000: loss 8.029167\n",
      "iteration 11600 / 15000: loss 7.782981\n",
      "iteration 11700 / 15000: loss 7.666515\n",
      "iteration 11800 / 15000: loss 7.486177\n",
      "iteration 11900 / 15000: loss 7.309323\n",
      "iteration 12000 / 15000: loss 7.148985\n",
      "iteration 12100 / 15000: loss 6.966224\n",
      "iteration 12200 / 15000: loss 6.766195\n",
      "iteration 12300 / 15000: loss 6.650723\n",
      "iteration 12400 / 15000: loss 6.454970\n",
      "iteration 12500 / 15000: loss 6.371553\n",
      "iteration 12600 / 15000: loss 6.262846\n",
      "iteration 12700 / 15000: loss 6.110128\n",
      "iteration 12800 / 15000: loss 5.973109\n",
      "iteration 12900 / 15000: loss 5.827574\n",
      "iteration 13000 / 15000: loss 5.780610\n",
      "iteration 13100 / 15000: loss 5.568684\n",
      "iteration 13200 / 15000: loss 5.409827\n",
      "iteration 13300 / 15000: loss 5.329562\n",
      "iteration 13400 / 15000: loss 5.269396\n",
      "iteration 13500 / 15000: loss 5.168348\n",
      "iteration 13600 / 15000: loss 4.979565\n",
      "iteration 13700 / 15000: loss 4.904041\n",
      "iteration 13800 / 15000: loss 4.910567\n",
      "iteration 13900 / 15000: loss 4.722187\n",
      "iteration 14000 / 15000: loss 4.709401\n",
      "iteration 14100 / 15000: loss 4.567367\n",
      "iteration 14200 / 15000: loss 4.570771\n",
      "iteration 14300 / 15000: loss 4.344942\n",
      "iteration 14400 / 15000: loss 4.380327\n",
      "iteration 14500 / 15000: loss 4.282978\n",
      "iteration 14600 / 15000: loss 4.232958\n",
      "iteration 14700 / 15000: loss 4.152119\n",
      "iteration 14800 / 15000: loss 4.125148\n",
      "iteration 14900 / 15000: loss 3.993827\n",
      "\n",
      "for r= 10000.0  and l= 1e-08\n",
      "iteration 0 / 15000: loss 313.165750\n",
      "iteration 100 / 15000: loss 300.314801\n",
      "iteration 200 / 15000: loss 288.357370\n",
      "iteration 300 / 15000: loss 276.414470\n",
      "iteration 400 / 15000: loss 265.444892\n",
      "iteration 500 / 15000: loss 255.216111\n",
      "iteration 600 / 15000: loss 244.913309\n",
      "iteration 700 / 15000: loss 235.194707\n",
      "iteration 800 / 15000: loss 225.944970\n",
      "iteration 900 / 15000: loss 216.725447\n",
      "iteration 1000 / 15000: loss 208.245302\n",
      "iteration 1100 / 15000: loss 200.017570\n",
      "iteration 1200 / 15000: loss 192.264818\n",
      "iteration 1300 / 15000: loss 184.664048\n",
      "iteration 1400 / 15000: loss 177.583356\n",
      "iteration 1500 / 15000: loss 170.840628\n",
      "iteration 1600 / 15000: loss 163.950961\n",
      "iteration 1700 / 15000: loss 157.557414\n",
      "iteration 1800 / 15000: loss 151.082187\n",
      "iteration 1900 / 15000: loss 145.530706\n",
      "iteration 2000 / 15000: loss 140.067169\n",
      "iteration 2100 / 15000: loss 134.333163\n",
      "iteration 2200 / 15000: loss 129.269685\n",
      "iteration 2300 / 15000: loss 124.028314\n",
      "iteration 2400 / 15000: loss 119.209513\n",
      "iteration 2500 / 15000: loss 114.679718\n",
      "iteration 2600 / 15000: loss 110.116437\n",
      "iteration 2700 / 15000: loss 105.823452\n",
      "iteration 2800 / 15000: loss 101.720644\n",
      "iteration 2900 / 15000: loss 97.945225\n",
      "iteration 3000 / 15000: loss 94.135157\n",
      "iteration 3100 / 15000: loss 90.362745\n",
      "iteration 3200 / 15000: loss 86.986995\n",
      "iteration 3300 / 15000: loss 83.529803\n",
      "iteration 3400 / 15000: loss 80.338235\n",
      "iteration 3500 / 15000: loss 77.328208\n",
      "iteration 3600 / 15000: loss 74.201213\n",
      "iteration 3700 / 15000: loss 71.438174\n",
      "iteration 3800 / 15000: loss 68.826540\n",
      "iteration 3900 / 15000: loss 66.091627\n",
      "iteration 4000 / 15000: loss 63.639370\n",
      "iteration 4100 / 15000: loss 61.140343\n",
      "iteration 4200 / 15000: loss 58.727643\n",
      "iteration 4300 / 15000: loss 56.550935\n",
      "iteration 4400 / 15000: loss 54.377215\n",
      "iteration 4500 / 15000: loss 52.292903\n",
      "iteration 4600 / 15000: loss 50.450601\n",
      "iteration 4700 / 15000: loss 48.377827\n",
      "iteration 4800 / 15000: loss 46.641905\n",
      "iteration 4900 / 15000: loss 44.820411\n",
      "iteration 5000 / 15000: loss 43.226899\n",
      "iteration 5100 / 15000: loss 41.628356\n",
      "iteration 5200 / 15000: loss 39.991467\n",
      "iteration 5300 / 15000: loss 38.601571\n",
      "iteration 5400 / 15000: loss 36.975894\n",
      "iteration 5500 / 15000: loss 35.617809\n",
      "iteration 5600 / 15000: loss 34.327855\n",
      "iteration 5700 / 15000: loss 33.197593\n",
      "iteration 5800 / 15000: loss 31.826903\n",
      "iteration 5900 / 15000: loss 30.664330\n",
      "iteration 6000 / 15000: loss 29.582823\n",
      "iteration 6100 / 15000: loss 28.433074\n",
      "iteration 6200 / 15000: loss 27.281508\n",
      "iteration 6300 / 15000: loss 26.342918\n",
      "iteration 6400 / 15000: loss 25.500213\n",
      "iteration 6500 / 15000: loss 24.568699\n",
      "iteration 6600 / 15000: loss 23.530125\n",
      "iteration 6700 / 15000: loss 22.782715\n",
      "iteration 6800 / 15000: loss 21.899402\n",
      "iteration 6900 / 15000: loss 21.129858\n",
      "iteration 7000 / 15000: loss 20.471751\n",
      "iteration 7100 / 15000: loss 19.681267\n",
      "iteration 7200 / 15000: loss 18.989762\n",
      "iteration 7300 / 15000: loss 18.301486\n",
      "iteration 7400 / 15000: loss 17.642979\n",
      "iteration 7500 / 15000: loss 17.016609\n",
      "iteration 7600 / 15000: loss 16.455083\n",
      "iteration 7700 / 15000: loss 15.891091\n",
      "iteration 7800 / 15000: loss 15.368824\n",
      "iteration 7900 / 15000: loss 14.769197\n",
      "iteration 8000 / 15000: loss 14.446121\n",
      "iteration 8100 / 15000: loss 13.833572\n",
      "iteration 8200 / 15000: loss 13.359454\n",
      "iteration 8300 / 15000: loss 13.011063\n",
      "iteration 8400 / 15000: loss 12.442895\n",
      "iteration 8500 / 15000: loss 12.122789\n",
      "iteration 8600 / 15000: loss 11.689088\n",
      "iteration 8700 / 15000: loss 11.269623\n",
      "iteration 8800 / 15000: loss 11.002515\n",
      "iteration 8900 / 15000: loss 10.573401\n",
      "iteration 9000 / 15000: loss 10.294524\n",
      "iteration 9100 / 15000: loss 9.838856\n",
      "iteration 9200 / 15000: loss 9.552909\n",
      "iteration 9300 / 15000: loss 9.315063\n",
      "iteration 9400 / 15000: loss 9.067312\n",
      "iteration 9500 / 15000: loss 8.717145\n",
      "iteration 9600 / 15000: loss 8.489343\n",
      "iteration 9700 / 15000: loss 8.263981\n",
      "iteration 9800 / 15000: loss 7.920447\n",
      "iteration 9900 / 15000: loss 7.787786\n",
      "iteration 10000 / 15000: loss 7.504667\n",
      "iteration 10100 / 15000: loss 7.274209\n",
      "iteration 10200 / 15000: loss 7.096653\n",
      "iteration 10300 / 15000: loss 6.806904\n",
      "iteration 10400 / 15000: loss 6.740685\n",
      "iteration 10500 / 15000: loss 6.532186\n",
      "iteration 10600 / 15000: loss 6.355381\n",
      "iteration 10700 / 15000: loss 6.086577\n",
      "iteration 10800 / 15000: loss 6.031166\n",
      "iteration 10900 / 15000: loss 5.820057\n",
      "iteration 11000 / 15000: loss 5.720994\n",
      "iteration 11100 / 15000: loss 5.563167\n",
      "iteration 11200 / 15000: loss 5.406459\n",
      "iteration 11300 / 15000: loss 5.294616\n",
      "iteration 11400 / 15000: loss 5.077988\n",
      "iteration 11500 / 15000: loss 4.970709\n",
      "iteration 11600 / 15000: loss 4.927120\n",
      "iteration 11700 / 15000: loss 4.798675\n",
      "iteration 11800 / 15000: loss 4.696437\n",
      "iteration 11900 / 15000: loss 4.548092\n",
      "iteration 12000 / 15000: loss 4.434813\n",
      "iteration 12100 / 15000: loss 4.363414\n",
      "iteration 12200 / 15000: loss 4.287328\n",
      "iteration 12300 / 15000: loss 4.222258\n",
      "iteration 12400 / 15000: loss 4.018837\n",
      "iteration 12500 / 15000: loss 4.022075\n",
      "iteration 12600 / 15000: loss 3.988827\n",
      "iteration 12700 / 15000: loss 3.838316\n",
      "iteration 12800 / 15000: loss 3.802122\n",
      "iteration 12900 / 15000: loss 3.693540\n",
      "iteration 13000 / 15000: loss 3.681916\n",
      "iteration 13100 / 15000: loss 3.636906\n",
      "iteration 13200 / 15000: loss 3.576493\n",
      "iteration 13300 / 15000: loss 3.430465\n",
      "iteration 13400 / 15000: loss 3.390503\n",
      "iteration 13500 / 15000: loss 3.360574\n",
      "iteration 13600 / 15000: loss 3.310287\n",
      "iteration 13700 / 15000: loss 3.267523\n",
      "iteration 13800 / 15000: loss 3.207946\n",
      "iteration 13900 / 15000: loss 3.187586\n",
      "iteration 14000 / 15000: loss 3.117439\n",
      "iteration 14100 / 15000: loss 3.071544\n",
      "iteration 14200 / 15000: loss 3.028220\n",
      "iteration 14300 / 15000: loss 3.000845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14400 / 15000: loss 2.982854\n",
      "iteration 14500 / 15000: loss 2.937681\n",
      "iteration 14600 / 15000: loss 2.846720\n",
      "iteration 14700 / 15000: loss 2.784041\n",
      "iteration 14800 / 15000: loss 2.837165\n",
      "iteration 14900 / 15000: loss 2.743753\n",
      "\n",
      "for r= 12500.0  and l= 1e-08\n",
      "iteration 0 / 15000: loss 394.332046\n",
      "iteration 100 / 15000: loss 374.287492\n",
      "iteration 200 / 15000: loss 355.685120\n",
      "iteration 300 / 15000: loss 338.076294\n",
      "iteration 400 / 15000: loss 321.393105\n",
      "iteration 500 / 15000: loss 304.872364\n",
      "iteration 600 / 15000: loss 290.621560\n",
      "iteration 700 / 15000: loss 276.244724\n",
      "iteration 800 / 15000: loss 262.661887\n",
      "iteration 900 / 15000: loss 249.718945\n",
      "iteration 1000 / 15000: loss 237.658580\n",
      "iteration 1100 / 15000: loss 226.107787\n",
      "iteration 1200 / 15000: loss 215.079724\n",
      "iteration 1300 / 15000: loss 204.421957\n",
      "iteration 1400 / 15000: loss 194.596575\n",
      "iteration 1500 / 15000: loss 185.298366\n",
      "iteration 1600 / 15000: loss 175.941748\n",
      "iteration 1700 / 15000: loss 167.635029\n",
      "iteration 1800 / 15000: loss 159.514947\n",
      "iteration 1900 / 15000: loss 151.693951\n",
      "iteration 2000 / 15000: loss 144.435961\n",
      "iteration 2100 / 15000: loss 137.346460\n",
      "iteration 2200 / 15000: loss 130.715383\n",
      "iteration 2300 / 15000: loss 124.454354\n",
      "iteration 2400 / 15000: loss 118.487731\n",
      "iteration 2500 / 15000: loss 112.754668\n",
      "iteration 2600 / 15000: loss 107.260298\n",
      "iteration 2700 / 15000: loss 102.152718\n",
      "iteration 2800 / 15000: loss 97.192612\n",
      "iteration 2900 / 15000: loss 92.594046\n",
      "iteration 3000 / 15000: loss 88.135636\n",
      "iteration 3100 / 15000: loss 84.027304\n",
      "iteration 3200 / 15000: loss 79.871275\n",
      "iteration 3300 / 15000: loss 76.221530\n",
      "iteration 3400 / 15000: loss 72.423136\n",
      "iteration 3500 / 15000: loss 68.980065\n",
      "iteration 3600 / 15000: loss 65.676889\n",
      "iteration 3700 / 15000: loss 62.563768\n",
      "iteration 3800 / 15000: loss 59.525030\n",
      "iteration 3900 / 15000: loss 56.752275\n",
      "iteration 4000 / 15000: loss 54.028218\n",
      "iteration 4100 / 15000: loss 51.540575\n",
      "iteration 4200 / 15000: loss 49.078228\n",
      "iteration 4300 / 15000: loss 46.876937\n",
      "iteration 4400 / 15000: loss 44.599544\n",
      "iteration 4500 / 15000: loss 42.541493\n",
      "iteration 4600 / 15000: loss 40.573984\n",
      "iteration 4700 / 15000: loss 38.691005\n",
      "iteration 4800 / 15000: loss 36.878546\n",
      "iteration 4900 / 15000: loss 35.140998\n",
      "iteration 5000 / 15000: loss 33.476709\n",
      "iteration 5100 / 15000: loss 31.994373\n",
      "iteration 5200 / 15000: loss 30.535006\n",
      "iteration 5300 / 15000: loss 29.170922\n",
      "iteration 5400 / 15000: loss 27.881160\n",
      "iteration 5500 / 15000: loss 26.500188\n",
      "iteration 5600 / 15000: loss 25.347342\n",
      "iteration 5700 / 15000: loss 24.173799\n",
      "iteration 5800 / 15000: loss 23.122476\n",
      "iteration 5900 / 15000: loss 22.080607\n",
      "iteration 6000 / 15000: loss 21.183704\n",
      "iteration 6100 / 15000: loss 20.183616\n",
      "iteration 6200 / 15000: loss 19.276392\n",
      "iteration 6300 / 15000: loss 18.462178\n",
      "iteration 6400 / 15000: loss 17.649243\n",
      "iteration 6500 / 15000: loss 16.889301\n",
      "iteration 6600 / 15000: loss 16.211542\n",
      "iteration 6700 / 15000: loss 15.504324\n",
      "iteration 6800 / 15000: loss 14.737579\n",
      "iteration 6900 / 15000: loss 14.186991\n",
      "iteration 7000 / 15000: loss 13.509576\n",
      "iteration 7100 / 15000: loss 13.009093\n",
      "iteration 7200 / 15000: loss 12.414680\n",
      "iteration 7300 / 15000: loss 11.956005\n",
      "iteration 7400 / 15000: loss 11.437530\n",
      "iteration 7500 / 15000: loss 10.980882\n",
      "iteration 7600 / 15000: loss 10.546793\n",
      "iteration 7700 / 15000: loss 10.210105\n",
      "iteration 7800 / 15000: loss 9.736258\n",
      "iteration 7900 / 15000: loss 9.416935\n",
      "iteration 8000 / 15000: loss 9.077964\n",
      "iteration 8100 / 15000: loss 8.668096\n",
      "iteration 8200 / 15000: loss 8.281150\n",
      "iteration 8300 / 15000: loss 8.083704\n",
      "iteration 8400 / 15000: loss 7.760137\n",
      "iteration 8500 / 15000: loss 7.461912\n",
      "iteration 8600 / 15000: loss 7.215321\n",
      "iteration 8700 / 15000: loss 6.883398\n",
      "iteration 8800 / 15000: loss 6.764550\n",
      "iteration 8900 / 15000: loss 6.517829\n",
      "iteration 9000 / 15000: loss 6.251647\n",
      "iteration 9100 / 15000: loss 5.993956\n",
      "iteration 9200 / 15000: loss 5.870024\n",
      "iteration 9300 / 15000: loss 5.684393\n",
      "iteration 9400 / 15000: loss 5.457428\n",
      "iteration 9500 / 15000: loss 5.351062\n",
      "iteration 9600 / 15000: loss 5.164340\n",
      "iteration 9700 / 15000: loss 4.994824\n",
      "iteration 9800 / 15000: loss 4.855299\n",
      "iteration 9900 / 15000: loss 4.674184\n",
      "iteration 10000 / 15000: loss 4.493272\n",
      "iteration 10100 / 15000: loss 4.467052\n",
      "iteration 10200 / 15000: loss 4.440542\n",
      "iteration 10300 / 15000: loss 4.163488\n",
      "iteration 10400 / 15000: loss 4.079011\n",
      "iteration 10500 / 15000: loss 3.971487\n",
      "iteration 10600 / 15000: loss 3.907359\n",
      "iteration 10700 / 15000: loss 3.818334\n",
      "iteration 10800 / 15000: loss 3.703795\n",
      "iteration 10900 / 15000: loss 3.665776\n",
      "iteration 11000 / 15000: loss 3.598582\n",
      "iteration 11100 / 15000: loss 3.448325\n",
      "iteration 11200 / 15000: loss 3.480285\n",
      "iteration 11300 / 15000: loss 3.399776\n",
      "iteration 11400 / 15000: loss 3.300636\n",
      "iteration 11500 / 15000: loss 3.359821\n",
      "iteration 11600 / 15000: loss 3.236431\n",
      "iteration 11700 / 15000: loss 3.132896\n",
      "iteration 11800 / 15000: loss 3.075711\n",
      "iteration 11900 / 15000: loss 3.022900\n",
      "iteration 12000 / 15000: loss 2.942701\n",
      "iteration 12100 / 15000: loss 2.912770\n",
      "iteration 12200 / 15000: loss 2.903118\n",
      "iteration 12300 / 15000: loss 2.817966\n",
      "iteration 12400 / 15000: loss 2.808683\n",
      "iteration 12500 / 15000: loss 2.658165\n",
      "iteration 12600 / 15000: loss 2.690198\n",
      "iteration 12700 / 15000: loss 2.687826\n",
      "iteration 12800 / 15000: loss 2.641828\n",
      "iteration 12900 / 15000: loss 2.606181\n",
      "iteration 13000 / 15000: loss 2.574043\n",
      "iteration 13100 / 15000: loss 2.572578\n",
      "iteration 13200 / 15000: loss 2.545896\n",
      "iteration 13300 / 15000: loss 2.484663\n",
      "iteration 13400 / 15000: loss 2.441943\n",
      "iteration 13500 / 15000: loss 2.428124\n",
      "iteration 13600 / 15000: loss 2.392817\n",
      "iteration 13700 / 15000: loss 2.454903\n",
      "iteration 13800 / 15000: loss 2.393294\n",
      "iteration 13900 / 15000: loss 2.409054\n",
      "iteration 14000 / 15000: loss 2.380751\n",
      "iteration 14100 / 15000: loss 2.355084\n",
      "iteration 14200 / 15000: loss 2.410930\n",
      "iteration 14300 / 15000: loss 2.300274\n",
      "iteration 14400 / 15000: loss 2.339980\n",
      "iteration 14500 / 15000: loss 2.298614\n",
      "iteration 14600 / 15000: loss 2.256307\n",
      "iteration 14700 / 15000: loss 2.241553\n",
      "iteration 14800 / 15000: loss 2.279562\n",
      "iteration 14900 / 15000: loss 2.224592\n",
      "\n",
      "for r= 15000.0  and l= 1e-08\n",
      "iteration 0 / 15000: loss 464.434278\n",
      "iteration 100 / 15000: loss 437.214032\n",
      "iteration 200 / 15000: loss 411.839285\n",
      "iteration 300 / 15000: loss 387.740826\n",
      "iteration 400 / 15000: loss 364.766327\n",
      "iteration 500 / 15000: loss 344.118822\n",
      "iteration 600 / 15000: loss 323.401401\n",
      "iteration 700 / 15000: loss 304.614833\n",
      "iteration 800 / 15000: loss 286.629959\n",
      "iteration 900 / 15000: loss 270.037887\n",
      "iteration 1000 / 15000: loss 254.591727\n",
      "iteration 1100 / 15000: loss 239.663322\n",
      "iteration 1200 / 15000: loss 226.047867\n",
      "iteration 1300 / 15000: loss 212.436298\n",
      "iteration 1400 / 15000: loss 200.352913\n",
      "iteration 1500 / 15000: loss 188.723085\n",
      "iteration 1600 / 15000: loss 177.844575\n",
      "iteration 1700 / 15000: loss 167.533927\n",
      "iteration 1800 / 15000: loss 157.857975\n",
      "iteration 1900 / 15000: loss 148.670291\n",
      "iteration 2000 / 15000: loss 140.178595\n",
      "iteration 2100 / 15000: loss 132.102321\n",
      "iteration 2200 / 15000: loss 124.327900\n",
      "iteration 2300 / 15000: loss 117.175274\n",
      "iteration 2400 / 15000: loss 110.555566\n",
      "iteration 2500 / 15000: loss 104.049328\n",
      "iteration 2600 / 15000: loss 98.128951\n",
      "iteration 2700 / 15000: loss 92.564740\n",
      "iteration 2800 / 15000: loss 87.196893\n",
      "iteration 2900 / 15000: loss 82.292085\n",
      "iteration 3000 / 15000: loss 77.601816\n",
      "iteration 3100 / 15000: loss 73.209137\n",
      "iteration 3200 / 15000: loss 68.993956\n",
      "iteration 3300 / 15000: loss 65.063920\n",
      "iteration 3400 / 15000: loss 61.321191\n",
      "iteration 3500 / 15000: loss 57.929647\n",
      "iteration 3600 / 15000: loss 54.643872\n",
      "iteration 3700 / 15000: loss 51.521535\n",
      "iteration 3800 / 15000: loss 48.586038\n",
      "iteration 3900 / 15000: loss 46.051349\n",
      "iteration 4000 / 15000: loss 43.388895\n",
      "iteration 4100 / 15000: loss 40.948915\n",
      "iteration 4200 / 15000: loss 38.694761\n",
      "iteration 4300 / 15000: loss 36.527352\n",
      "iteration 4400 / 15000: loss 34.472574\n",
      "iteration 4500 / 15000: loss 32.574398\n",
      "iteration 4600 / 15000: loss 30.811582\n",
      "iteration 4700 / 15000: loss 29.100380\n",
      "iteration 4800 / 15000: loss 27.583493\n",
      "iteration 4900 / 15000: loss 26.100495\n",
      "iteration 5000 / 15000: loss 24.578590\n",
      "iteration 5100 / 15000: loss 23.221633\n",
      "iteration 5200 / 15000: loss 22.134428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5300 / 15000: loss 20.935840\n",
      "iteration 5400 / 15000: loss 19.828746\n",
      "iteration 5500 / 15000: loss 18.791723\n",
      "iteration 5600 / 15000: loss 17.847391\n",
      "iteration 5700 / 15000: loss 16.945846\n",
      "iteration 5800 / 15000: loss 16.057967\n",
      "iteration 5900 / 15000: loss 15.242687\n",
      "iteration 6000 / 15000: loss 14.465740\n",
      "iteration 6100 / 15000: loss 13.819959\n",
      "iteration 6200 / 15000: loss 13.079327\n",
      "iteration 6300 / 15000: loss 12.371308\n",
      "iteration 6400 / 15000: loss 11.808624\n",
      "iteration 6500 / 15000: loss 11.238529\n",
      "iteration 6600 / 15000: loss 10.703460\n",
      "iteration 6700 / 15000: loss 10.146521\n",
      "iteration 6800 / 15000: loss 9.656967\n",
      "iteration 6900 / 15000: loss 9.239845\n",
      "iteration 7000 / 15000: loss 8.796051\n",
      "iteration 7100 / 15000: loss 8.430696\n",
      "iteration 7200 / 15000: loss 8.060969\n",
      "iteration 7300 / 15000: loss 7.733655\n",
      "iteration 7400 / 15000: loss 7.350663\n",
      "iteration 7500 / 15000: loss 7.098541\n",
      "iteration 7600 / 15000: loss 6.804764\n",
      "iteration 7700 / 15000: loss 6.541239\n",
      "iteration 7800 / 15000: loss 6.225330\n",
      "iteration 7900 / 15000: loss 5.960839\n",
      "iteration 8000 / 15000: loss 5.755454\n",
      "iteration 8100 / 15000: loss 5.531162\n",
      "iteration 8200 / 15000: loss 5.296622\n",
      "iteration 8300 / 15000: loss 5.144867\n",
      "iteration 8400 / 15000: loss 4.915544\n",
      "iteration 8500 / 15000: loss 4.728686\n",
      "iteration 8600 / 15000: loss 4.636433\n",
      "iteration 8700 / 15000: loss 4.467290\n",
      "iteration 8800 / 15000: loss 4.307754\n",
      "iteration 8900 / 15000: loss 4.187353\n",
      "iteration 9000 / 15000: loss 4.069206\n",
      "iteration 9100 / 15000: loss 3.986988\n",
      "iteration 9200 / 15000: loss 3.835936\n",
      "iteration 9300 / 15000: loss 3.709354\n",
      "iteration 9400 / 15000: loss 3.655674\n",
      "iteration 9500 / 15000: loss 3.583929\n",
      "iteration 9600 / 15000: loss 3.500587\n",
      "iteration 9700 / 15000: loss 3.342291\n",
      "iteration 9800 / 15000: loss 3.329586\n",
      "iteration 9900 / 15000: loss 3.305460\n",
      "iteration 10000 / 15000: loss 3.112757\n",
      "iteration 10100 / 15000: loss 3.105666\n",
      "iteration 10200 / 15000: loss 3.054123\n",
      "iteration 10300 / 15000: loss 2.950589\n",
      "iteration 10400 / 15000: loss 2.997535\n",
      "iteration 10500 / 15000: loss 2.930020\n",
      "iteration 10600 / 15000: loss 2.824320\n",
      "iteration 10700 / 15000: loss 2.746625\n",
      "iteration 10800 / 15000: loss 2.685830\n",
      "iteration 10900 / 15000: loss 2.650536\n",
      "iteration 11000 / 15000: loss 2.697508\n",
      "iteration 11100 / 15000: loss 2.574872\n",
      "iteration 11200 / 15000: loss 2.592437\n",
      "iteration 11300 / 15000: loss 2.604773\n",
      "iteration 11400 / 15000: loss 2.473750\n",
      "iteration 11500 / 15000: loss 2.544744\n",
      "iteration 11600 / 15000: loss 2.410937\n",
      "iteration 11700 / 15000: loss 2.508319\n",
      "iteration 11800 / 15000: loss 2.444241\n",
      "iteration 11900 / 15000: loss 2.476525\n",
      "iteration 12000 / 15000: loss 2.340657\n",
      "iteration 12100 / 15000: loss 2.380050\n",
      "iteration 12200 / 15000: loss 2.342651\n",
      "iteration 12300 / 15000: loss 2.337454\n",
      "iteration 12400 / 15000: loss 2.303111\n",
      "iteration 12500 / 15000: loss 2.241064\n",
      "iteration 12600 / 15000: loss 2.189172\n",
      "iteration 12700 / 15000: loss 2.270810\n",
      "iteration 12800 / 15000: loss 2.268382\n",
      "iteration 12900 / 15000: loss 2.211294\n",
      "iteration 13000 / 15000: loss 2.223507\n",
      "iteration 13100 / 15000: loss 2.246430\n",
      "iteration 13200 / 15000: loss 2.139642\n",
      "iteration 13300 / 15000: loss 2.171540\n",
      "iteration 13400 / 15000: loss 2.261784\n",
      "iteration 13500 / 15000: loss 2.179634\n",
      "iteration 13600 / 15000: loss 2.144995\n",
      "iteration 13700 / 15000: loss 2.162182\n",
      "iteration 13800 / 15000: loss 2.161091\n",
      "iteration 13900 / 15000: loss 2.172239\n",
      "iteration 14000 / 15000: loss 2.142182\n",
      "iteration 14100 / 15000: loss 2.175410\n",
      "iteration 14200 / 15000: loss 2.110350\n",
      "iteration 14300 / 15000: loss 2.135335\n",
      "iteration 14400 / 15000: loss 2.157953\n",
      "iteration 14500 / 15000: loss 2.080191\n",
      "iteration 14600 / 15000: loss 2.095400\n",
      "iteration 14700 / 15000: loss 2.099687\n",
      "iteration 14800 / 15000: loss 2.098189\n",
      "iteration 14900 / 15000: loss 2.131033\n",
      "\n",
      "for r= 20000.0  and l= 1e-08\n",
      "iteration 0 / 15000: loss 625.337927\n",
      "iteration 100 / 15000: loss 577.028631\n",
      "iteration 200 / 15000: loss 532.790175\n",
      "iteration 300 / 15000: loss 491.658097\n",
      "iteration 400 / 15000: loss 453.585157\n",
      "iteration 500 / 15000: loss 418.605848\n",
      "iteration 600 / 15000: loss 386.337818\n",
      "iteration 700 / 15000: loss 356.710141\n",
      "iteration 800 / 15000: loss 329.391061\n",
      "iteration 900 / 15000: loss 304.076652\n",
      "iteration 1000 / 15000: loss 280.760679\n",
      "iteration 1100 / 15000: loss 259.202960\n",
      "iteration 1200 / 15000: loss 239.525112\n",
      "iteration 1300 / 15000: loss 221.014349\n",
      "iteration 1400 / 15000: loss 204.166863\n",
      "iteration 1500 / 15000: loss 188.639199\n",
      "iteration 1600 / 15000: loss 174.120554\n",
      "iteration 1700 / 15000: loss 161.025190\n",
      "iteration 1800 / 15000: loss 148.678975\n",
      "iteration 1900 / 15000: loss 137.402383\n",
      "iteration 2000 / 15000: loss 126.876324\n",
      "iteration 2100 / 15000: loss 117.223907\n",
      "iteration 2200 / 15000: loss 108.437534\n",
      "iteration 2300 / 15000: loss 100.125017\n",
      "iteration 2400 / 15000: loss 92.501135\n",
      "iteration 2500 / 15000: loss 85.664771\n",
      "iteration 2600 / 15000: loss 79.224976\n",
      "iteration 2700 / 15000: loss 73.094902\n",
      "iteration 2800 / 15000: loss 67.738532\n",
      "iteration 2900 / 15000: loss 62.700702\n",
      "iteration 3000 / 15000: loss 58.087670\n",
      "iteration 3100 / 15000: loss 53.671978\n",
      "iteration 3200 / 15000: loss 49.648260\n",
      "iteration 3300 / 15000: loss 46.108803\n",
      "iteration 3400 / 15000: loss 42.578623\n",
      "iteration 3500 / 15000: loss 39.442899\n",
      "iteration 3600 / 15000: loss 36.556782\n",
      "iteration 3700 / 15000: loss 33.891126\n",
      "iteration 3800 / 15000: loss 31.499096\n",
      "iteration 3900 / 15000: loss 29.223369\n",
      "iteration 4000 / 15000: loss 27.214763\n",
      "iteration 4100 / 15000: loss 25.226132\n",
      "iteration 4200 / 15000: loss 23.455334\n",
      "iteration 4300 / 15000: loss 21.769569\n",
      "iteration 4400 / 15000: loss 20.247933\n",
      "iteration 4500 / 15000: loss 18.834336\n",
      "iteration 4600 / 15000: loss 17.589868\n",
      "iteration 4700 / 15000: loss 16.343760\n",
      "iteration 4800 / 15000: loss 15.227380\n",
      "iteration 4900 / 15000: loss 14.279597\n",
      "iteration 5000 / 15000: loss 13.199260\n",
      "iteration 5100 / 15000: loss 12.441720\n",
      "iteration 5200 / 15000: loss 11.618636\n",
      "iteration 5300 / 15000: loss 10.837324\n",
      "iteration 5400 / 15000: loss 10.209913\n",
      "iteration 5500 / 15000: loss 9.554010\n",
      "iteration 5600 / 15000: loss 8.952193\n",
      "iteration 5700 / 15000: loss 8.431114\n",
      "iteration 5800 / 15000: loss 7.988977\n",
      "iteration 5900 / 15000: loss 7.601123\n",
      "iteration 6000 / 15000: loss 7.147545\n",
      "iteration 6100 / 15000: loss 6.725595\n",
      "iteration 6200 / 15000: loss 6.337332\n",
      "iteration 6300 / 15000: loss 5.988524\n",
      "iteration 6400 / 15000: loss 5.722574\n",
      "iteration 6500 / 15000: loss 5.425928\n",
      "iteration 6600 / 15000: loss 5.151647\n",
      "iteration 6700 / 15000: loss 4.845712\n",
      "iteration 6800 / 15000: loss 4.707725\n",
      "iteration 6900 / 15000: loss 4.531340\n",
      "iteration 7000 / 15000: loss 4.287971\n",
      "iteration 7100 / 15000: loss 4.115593\n",
      "iteration 7200 / 15000: loss 3.934938\n",
      "iteration 7300 / 15000: loss 3.837110\n",
      "iteration 7400 / 15000: loss 3.670519\n",
      "iteration 7500 / 15000: loss 3.663444\n",
      "iteration 7600 / 15000: loss 3.480746\n",
      "iteration 7700 / 15000: loss 3.346582\n",
      "iteration 7800 / 15000: loss 3.246166\n",
      "iteration 7900 / 15000: loss 3.118950\n",
      "iteration 8000 / 15000: loss 3.086952\n",
      "iteration 8100 / 15000: loss 3.006687\n",
      "iteration 8200 / 15000: loss 2.935108\n",
      "iteration 8300 / 15000: loss 2.848812\n",
      "iteration 8400 / 15000: loss 2.747870\n",
      "iteration 8500 / 15000: loss 2.697065\n",
      "iteration 8600 / 15000: loss 2.703672\n",
      "iteration 8700 / 15000: loss 2.611835\n",
      "iteration 8800 / 15000: loss 2.558160\n",
      "iteration 8900 / 15000: loss 2.537739\n",
      "iteration 9000 / 15000: loss 2.463242\n",
      "iteration 9100 / 15000: loss 2.500198\n",
      "iteration 9200 / 15000: loss 2.481639\n",
      "iteration 9300 / 15000: loss 2.400978\n",
      "iteration 9400 / 15000: loss 2.380339\n",
      "iteration 9500 / 15000: loss 2.354570\n",
      "iteration 9600 / 15000: loss 2.363397\n",
      "iteration 9700 / 15000: loss 2.320552\n",
      "iteration 9800 / 15000: loss 2.302824\n",
      "iteration 9900 / 15000: loss 2.330697\n",
      "iteration 10000 / 15000: loss 2.177361\n",
      "iteration 10100 / 15000: loss 2.323419\n",
      "iteration 10200 / 15000: loss 2.253963\n",
      "iteration 10300 / 15000: loss 2.187141\n",
      "iteration 10400 / 15000: loss 2.202049\n",
      "iteration 10500 / 15000: loss 2.192027\n",
      "iteration 10600 / 15000: loss 2.212352\n",
      "iteration 10700 / 15000: loss 2.174599\n",
      "iteration 10800 / 15000: loss 2.196747\n",
      "iteration 10900 / 15000: loss 2.163892\n",
      "iteration 11000 / 15000: loss 2.171572\n",
      "iteration 11100 / 15000: loss 2.152005\n",
      "iteration 11200 / 15000: loss 2.151386\n",
      "iteration 11300 / 15000: loss 2.110701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11400 / 15000: loss 2.103973\n",
      "iteration 11500 / 15000: loss 2.160950\n",
      "iteration 11600 / 15000: loss 2.057186\n",
      "iteration 11700 / 15000: loss 2.096889\n",
      "iteration 11800 / 15000: loss 2.068514\n",
      "iteration 11900 / 15000: loss 2.097960\n",
      "iteration 12000 / 15000: loss 2.130682\n",
      "iteration 12100 / 15000: loss 2.083882\n",
      "iteration 12200 / 15000: loss 2.001674\n",
      "iteration 12300 / 15000: loss 2.171168\n",
      "iteration 12400 / 15000: loss 2.094818\n",
      "iteration 12500 / 15000: loss 2.124340\n",
      "iteration 12600 / 15000: loss 2.017529\n",
      "iteration 12700 / 15000: loss 2.101124\n",
      "iteration 12800 / 15000: loss 2.062949\n",
      "iteration 12900 / 15000: loss 2.074625\n",
      "iteration 13000 / 15000: loss 2.097737\n",
      "iteration 13100 / 15000: loss 2.136568\n",
      "iteration 13200 / 15000: loss 2.116429\n",
      "iteration 13300 / 15000: loss 2.120943\n",
      "iteration 13400 / 15000: loss 2.006867\n",
      "iteration 13500 / 15000: loss 2.120566\n",
      "iteration 13600 / 15000: loss 2.058775\n",
      "iteration 13700 / 15000: loss 2.067711\n",
      "iteration 13800 / 15000: loss 2.150959\n",
      "iteration 13900 / 15000: loss 2.086069\n",
      "iteration 14000 / 15000: loss 2.108029\n",
      "iteration 14100 / 15000: loss 2.088264\n",
      "iteration 14200 / 15000: loss 2.110054\n",
      "iteration 14300 / 15000: loss 2.045169\n",
      "iteration 14400 / 15000: loss 2.088762\n",
      "iteration 14500 / 15000: loss 2.046881\n",
      "iteration 14600 / 15000: loss 2.116824\n",
      "iteration 14700 / 15000: loss 2.067926\n",
      "iteration 14800 / 15000: loss 2.093621\n",
      "iteration 14900 / 15000: loss 2.102354\n",
      "\n",
      "for r= 8000.0  and l= 1.5e-08\n",
      "iteration 0 / 15000: loss 250.478603\n",
      "iteration 100 / 15000: loss 237.866440\n",
      "iteration 200 / 15000: loss 226.770672\n",
      "iteration 300 / 15000: loss 215.868446\n",
      "iteration 400 / 15000: loss 206.080250\n",
      "iteration 500 / 15000: loss 195.895047\n",
      "iteration 600 / 15000: loss 186.551416\n",
      "iteration 700 / 15000: loss 177.656171\n",
      "iteration 800 / 15000: loss 169.481654\n",
      "iteration 900 / 15000: loss 161.469120\n",
      "iteration 1000 / 15000: loss 154.144986\n",
      "iteration 1100 / 15000: loss 146.604569\n",
      "iteration 1200 / 15000: loss 139.956620\n",
      "iteration 1300 / 15000: loss 133.450925\n",
      "iteration 1400 / 15000: loss 127.212752\n",
      "iteration 1500 / 15000: loss 121.097875\n",
      "iteration 1600 / 15000: loss 115.969352\n",
      "iteration 1700 / 15000: loss 110.346981\n",
      "iteration 1800 / 15000: loss 104.979203\n",
      "iteration 1900 / 15000: loss 100.077627\n",
      "iteration 2000 / 15000: loss 95.734448\n",
      "iteration 2100 / 15000: loss 91.346163\n",
      "iteration 2200 / 15000: loss 87.014963\n",
      "iteration 2300 / 15000: loss 83.075410\n",
      "iteration 2400 / 15000: loss 79.112108\n",
      "iteration 2500 / 15000: loss 75.325693\n",
      "iteration 2600 / 15000: loss 72.037855\n",
      "iteration 2700 / 15000: loss 68.723277\n",
      "iteration 2800 / 15000: loss 65.499755\n",
      "iteration 2900 / 15000: loss 62.565847\n",
      "iteration 3000 / 15000: loss 59.651201\n",
      "iteration 3100 / 15000: loss 56.989798\n",
      "iteration 3200 / 15000: loss 54.409370\n",
      "iteration 3300 / 15000: loss 51.848732\n",
      "iteration 3400 / 15000: loss 49.520582\n",
      "iteration 3500 / 15000: loss 47.212332\n",
      "iteration 3600 / 15000: loss 45.098517\n",
      "iteration 3700 / 15000: loss 43.221578\n",
      "iteration 3800 / 15000: loss 41.138293\n",
      "iteration 3900 / 15000: loss 39.380339\n",
      "iteration 4000 / 15000: loss 37.525374\n",
      "iteration 4100 / 15000: loss 35.816780\n",
      "iteration 4200 / 15000: loss 34.344575\n",
      "iteration 4300 / 15000: loss 32.755244\n",
      "iteration 4400 / 15000: loss 31.314682\n",
      "iteration 4500 / 15000: loss 29.950560\n",
      "iteration 4600 / 15000: loss 28.641162\n",
      "iteration 4700 / 15000: loss 27.492681\n",
      "iteration 4800 / 15000: loss 26.204443\n",
      "iteration 4900 / 15000: loss 25.013445\n",
      "iteration 5000 / 15000: loss 23.914881\n",
      "iteration 5100 / 15000: loss 22.874780\n",
      "iteration 5200 / 15000: loss 21.892163\n",
      "iteration 5300 / 15000: loss 20.981114\n",
      "iteration 5400 / 15000: loss 20.047256\n",
      "iteration 5500 / 15000: loss 19.345931\n",
      "iteration 5600 / 15000: loss 18.382874\n",
      "iteration 5700 / 15000: loss 17.665328\n",
      "iteration 5800 / 15000: loss 16.859035\n",
      "iteration 5900 / 15000: loss 16.178282\n",
      "iteration 6000 / 15000: loss 15.589026\n",
      "iteration 6100 / 15000: loss 14.975397\n",
      "iteration 6200 / 15000: loss 14.348598\n",
      "iteration 6300 / 15000: loss 13.726874\n",
      "iteration 6400 / 15000: loss 13.185118\n",
      "iteration 6500 / 15000: loss 12.491676\n",
      "iteration 6600 / 15000: loss 12.055209\n",
      "iteration 6700 / 15000: loss 11.584886\n",
      "iteration 6800 / 15000: loss 11.206967\n",
      "iteration 6900 / 15000: loss 10.786206\n",
      "iteration 7000 / 15000: loss 10.326798\n",
      "iteration 7100 / 15000: loss 9.859393\n",
      "iteration 7200 / 15000: loss 9.660564\n",
      "iteration 7300 / 15000: loss 9.280457\n",
      "iteration 7400 / 15000: loss 8.875771\n",
      "iteration 7500 / 15000: loss 8.561968\n",
      "iteration 7600 / 15000: loss 8.182703\n",
      "iteration 7700 / 15000: loss 7.984575\n",
      "iteration 7800 / 15000: loss 7.598101\n",
      "iteration 7900 / 15000: loss 7.347543\n",
      "iteration 8000 / 15000: loss 7.191148\n",
      "iteration 8100 / 15000: loss 6.936212\n",
      "iteration 8200 / 15000: loss 6.623649\n",
      "iteration 8300 / 15000: loss 6.411087\n",
      "iteration 8400 / 15000: loss 6.144059\n",
      "iteration 8500 / 15000: loss 5.963970\n",
      "iteration 8600 / 15000: loss 5.777241\n",
      "iteration 8700 / 15000: loss 5.574580\n",
      "iteration 8800 / 15000: loss 5.485609\n",
      "iteration 8900 / 15000: loss 5.300155\n",
      "iteration 9000 / 15000: loss 5.153329\n",
      "iteration 9100 / 15000: loss 5.060446\n",
      "iteration 9200 / 15000: loss 4.931171\n",
      "iteration 9300 / 15000: loss 4.775981\n",
      "iteration 9400 / 15000: loss 4.653818\n",
      "iteration 9500 / 15000: loss 4.550941\n",
      "iteration 9600 / 15000: loss 4.366281\n",
      "iteration 9700 / 15000: loss 4.221529\n",
      "iteration 9800 / 15000: loss 4.101083\n",
      "iteration 9900 / 15000: loss 4.030593\n",
      "iteration 10000 / 15000: loss 3.973910\n",
      "iteration 10100 / 15000: loss 3.835309\n",
      "iteration 10200 / 15000: loss 3.851631\n",
      "iteration 10300 / 15000: loss 3.646922\n",
      "iteration 10400 / 15000: loss 3.600448\n",
      "iteration 10500 / 15000: loss 3.564147\n",
      "iteration 10600 / 15000: loss 3.497024\n",
      "iteration 10700 / 15000: loss 3.361364\n",
      "iteration 10800 / 15000: loss 3.225604\n",
      "iteration 10900 / 15000: loss 3.228665\n",
      "iteration 11000 / 15000: loss 3.178624\n",
      "iteration 11100 / 15000: loss 3.065586\n",
      "iteration 11200 / 15000: loss 3.063474\n",
      "iteration 11300 / 15000: loss 2.982739\n",
      "iteration 11400 / 15000: loss 3.009958\n",
      "iteration 11500 / 15000: loss 2.939520\n",
      "iteration 11600 / 15000: loss 2.862550\n",
      "iteration 11700 / 15000: loss 2.920695\n",
      "iteration 11800 / 15000: loss 2.745570\n",
      "iteration 11900 / 15000: loss 2.735433\n",
      "iteration 12000 / 15000: loss 2.726719\n",
      "iteration 12100 / 15000: loss 2.674000\n",
      "iteration 12200 / 15000: loss 2.684098\n",
      "iteration 12300 / 15000: loss 2.631271\n",
      "iteration 12400 / 15000: loss 2.588037\n",
      "iteration 12500 / 15000: loss 2.459262\n",
      "iteration 12600 / 15000: loss 2.527137\n",
      "iteration 12700 / 15000: loss 2.448924\n",
      "iteration 12800 / 15000: loss 2.450712\n",
      "iteration 12900 / 15000: loss 2.577862\n",
      "iteration 13000 / 15000: loss 2.449498\n",
      "iteration 13100 / 15000: loss 2.389340\n",
      "iteration 13200 / 15000: loss 2.333024\n",
      "iteration 13300 / 15000: loss 2.335055\n",
      "iteration 13400 / 15000: loss 2.260307\n",
      "iteration 13500 / 15000: loss 2.354920\n",
      "iteration 13600 / 15000: loss 2.247901\n",
      "iteration 13700 / 15000: loss 2.303059\n",
      "iteration 13800 / 15000: loss 2.364551\n",
      "iteration 13900 / 15000: loss 2.288305\n",
      "iteration 14000 / 15000: loss 2.314062\n",
      "iteration 14100 / 15000: loss 2.210485\n",
      "iteration 14200 / 15000: loss 2.292163\n",
      "iteration 14300 / 15000: loss 2.258614\n",
      "iteration 14400 / 15000: loss 2.209063\n",
      "iteration 14500 / 15000: loss 2.263049\n",
      "iteration 14600 / 15000: loss 2.195954\n",
      "iteration 14700 / 15000: loss 2.186110\n",
      "iteration 14800 / 15000: loss 2.175713\n",
      "iteration 14900 / 15000: loss 2.175215\n",
      "\n",
      "for r= 10000.0  and l= 1.5e-08\n",
      "iteration 0 / 15000: loss 314.165825\n",
      "iteration 100 / 15000: loss 295.901257\n",
      "iteration 200 / 15000: loss 278.271506\n",
      "iteration 300 / 15000: loss 261.589999\n",
      "iteration 400 / 15000: loss 246.540197\n",
      "iteration 500 / 15000: loss 232.028559\n",
      "iteration 600 / 15000: loss 218.873717\n",
      "iteration 700 / 15000: loss 205.631903\n",
      "iteration 800 / 15000: loss 193.837823\n",
      "iteration 900 / 15000: loss 182.542929\n",
      "iteration 1000 / 15000: loss 171.917307\n",
      "iteration 1100 / 15000: loss 161.954379\n",
      "iteration 1200 / 15000: loss 152.650312\n",
      "iteration 1300 / 15000: loss 144.120239\n",
      "iteration 1400 / 15000: loss 135.399238\n",
      "iteration 1500 / 15000: loss 127.649073\n",
      "iteration 1600 / 15000: loss 120.341046\n",
      "iteration 1700 / 15000: loss 113.341832\n",
      "iteration 1800 / 15000: loss 106.742276\n",
      "iteration 1900 / 15000: loss 100.545165\n",
      "iteration 2000 / 15000: loss 95.054921\n",
      "iteration 2100 / 15000: loss 89.298739\n",
      "iteration 2200 / 15000: loss 84.294614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2300 / 15000: loss 79.327010\n",
      "iteration 2400 / 15000: loss 74.984616\n",
      "iteration 2500 / 15000: loss 70.537571\n",
      "iteration 2600 / 15000: loss 66.638225\n",
      "iteration 2700 / 15000: loss 62.839579\n",
      "iteration 2800 / 15000: loss 59.301317\n",
      "iteration 2900 / 15000: loss 55.958578\n",
      "iteration 3000 / 15000: loss 52.726221\n",
      "iteration 3100 / 15000: loss 49.646606\n",
      "iteration 3200 / 15000: loss 46.942334\n",
      "iteration 3300 / 15000: loss 44.434263\n",
      "iteration 3400 / 15000: loss 41.815513\n",
      "iteration 3500 / 15000: loss 39.616235\n",
      "iteration 3600 / 15000: loss 37.247963\n",
      "iteration 3700 / 15000: loss 35.289774\n",
      "iteration 3800 / 15000: loss 33.320278\n",
      "iteration 3900 / 15000: loss 31.443478\n",
      "iteration 4000 / 15000: loss 29.753177\n",
      "iteration 4100 / 15000: loss 28.143571\n",
      "iteration 4200 / 15000: loss 26.602537\n",
      "iteration 4300 / 15000: loss 25.139565\n",
      "iteration 4400 / 15000: loss 23.788585\n",
      "iteration 4500 / 15000: loss 22.614474\n",
      "iteration 4600 / 15000: loss 21.313683\n",
      "iteration 4700 / 15000: loss 20.237147\n",
      "iteration 4800 / 15000: loss 19.155505\n",
      "iteration 4900 / 15000: loss 18.081403\n",
      "iteration 5000 / 15000: loss 17.177465\n",
      "iteration 5100 / 15000: loss 16.289714\n",
      "iteration 5200 / 15000: loss 15.373539\n",
      "iteration 5300 / 15000: loss 14.682757\n",
      "iteration 5400 / 15000: loss 13.960808\n",
      "iteration 5500 / 15000: loss 13.213741\n",
      "iteration 5600 / 15000: loss 12.560436\n",
      "iteration 5700 / 15000: loss 11.975654\n",
      "iteration 5800 / 15000: loss 11.430413\n",
      "iteration 5900 / 15000: loss 10.898112\n",
      "iteration 6000 / 15000: loss 10.388868\n",
      "iteration 6100 / 15000: loss 9.881755\n",
      "iteration 6200 / 15000: loss 9.343556\n",
      "iteration 6300 / 15000: loss 8.990375\n",
      "iteration 6400 / 15000: loss 8.523482\n",
      "iteration 6500 / 15000: loss 8.249296\n",
      "iteration 6600 / 15000: loss 7.814714\n",
      "iteration 6700 / 15000: loss 7.419227\n",
      "iteration 6800 / 15000: loss 7.066166\n",
      "iteration 6900 / 15000: loss 6.847208\n",
      "iteration 7000 / 15000: loss 6.495910\n",
      "iteration 7100 / 15000: loss 6.297139\n",
      "iteration 7200 / 15000: loss 6.108756\n",
      "iteration 7300 / 15000: loss 5.808752\n",
      "iteration 7400 / 15000: loss 5.621823\n",
      "iteration 7500 / 15000: loss 5.348122\n",
      "iteration 7600 / 15000: loss 5.195283\n",
      "iteration 7700 / 15000: loss 5.019266\n",
      "iteration 7800 / 15000: loss 4.868395\n",
      "iteration 7900 / 15000: loss 4.690144\n",
      "iteration 8000 / 15000: loss 4.469936\n",
      "iteration 8100 / 15000: loss 4.345519\n",
      "iteration 8200 / 15000: loss 4.253367\n",
      "iteration 8300 / 15000: loss 4.032695\n",
      "iteration 8400 / 15000: loss 3.922886\n",
      "iteration 8500 / 15000: loss 3.858879\n",
      "iteration 8600 / 15000: loss 3.723320\n",
      "iteration 8700 / 15000: loss 3.711927\n",
      "iteration 8800 / 15000: loss 3.476131\n",
      "iteration 8900 / 15000: loss 3.487034\n",
      "iteration 9000 / 15000: loss 3.351192\n",
      "iteration 9100 / 15000: loss 3.266129\n",
      "iteration 9200 / 15000: loss 3.151166\n",
      "iteration 9300 / 15000: loss 3.155632\n",
      "iteration 9400 / 15000: loss 3.101703\n",
      "iteration 9500 / 15000: loss 2.966904\n",
      "iteration 9600 / 15000: loss 2.971415\n",
      "iteration 9700 / 15000: loss 2.860854\n",
      "iteration 9800 / 15000: loss 2.876478\n",
      "iteration 9900 / 15000: loss 2.768291\n",
      "iteration 10000 / 15000: loss 2.737501\n",
      "iteration 10100 / 15000: loss 2.750731\n",
      "iteration 10200 / 15000: loss 2.738289\n",
      "iteration 10300 / 15000: loss 2.637456\n",
      "iteration 10400 / 15000: loss 2.634893\n",
      "iteration 10500 / 15000: loss 2.623992\n",
      "iteration 10600 / 15000: loss 2.530257\n",
      "iteration 10700 / 15000: loss 2.418090\n",
      "iteration 10800 / 15000: loss 2.458183\n",
      "iteration 10900 / 15000: loss 2.448232\n",
      "iteration 11000 / 15000: loss 2.399689\n",
      "iteration 11100 / 15000: loss 2.355997\n",
      "iteration 11200 / 15000: loss 2.401312\n",
      "iteration 11300 / 15000: loss 2.376539\n",
      "iteration 11400 / 15000: loss 2.278997\n",
      "iteration 11500 / 15000: loss 2.301082\n",
      "iteration 11600 / 15000: loss 2.249947\n",
      "iteration 11700 / 15000: loss 2.269837\n",
      "iteration 11800 / 15000: loss 2.261957\n",
      "iteration 11900 / 15000: loss 2.308368\n",
      "iteration 12000 / 15000: loss 2.256147\n",
      "iteration 12100 / 15000: loss 2.185467\n",
      "iteration 12200 / 15000: loss 2.235011\n",
      "iteration 12300 / 15000: loss 2.132231\n",
      "iteration 12400 / 15000: loss 2.173414\n",
      "iteration 12500 / 15000: loss 2.165804\n",
      "iteration 12600 / 15000: loss 2.134504\n",
      "iteration 12700 / 15000: loss 2.200442\n",
      "iteration 12800 / 15000: loss 2.227630\n",
      "iteration 12900 / 15000: loss 2.138771\n",
      "iteration 13000 / 15000: loss 2.125372\n",
      "iteration 13100 / 15000: loss 2.093130\n",
      "iteration 13200 / 15000: loss 2.046528\n",
      "iteration 13300 / 15000: loss 2.046213\n",
      "iteration 13400 / 15000: loss 2.166155\n",
      "iteration 13500 / 15000: loss 2.053077\n",
      "iteration 13600 / 15000: loss 1.999758\n",
      "iteration 13700 / 15000: loss 2.033237\n",
      "iteration 13800 / 15000: loss 2.045208\n",
      "iteration 13900 / 15000: loss 2.172097\n",
      "iteration 14000 / 15000: loss 2.065844\n",
      "iteration 14100 / 15000: loss 2.041837\n",
      "iteration 14200 / 15000: loss 2.098366\n",
      "iteration 14300 / 15000: loss 2.127642\n",
      "iteration 14400 / 15000: loss 2.061725\n",
      "iteration 14500 / 15000: loss 2.087232\n",
      "iteration 14600 / 15000: loss 2.032137\n",
      "iteration 14700 / 15000: loss 2.046124\n",
      "iteration 14800 / 15000: loss 2.002765\n",
      "iteration 14900 / 15000: loss 2.157104\n",
      "\n",
      "for r= 12500.0  and l= 1.5e-08\n",
      "iteration 0 / 15000: loss 392.137397\n",
      "iteration 100 / 15000: loss 363.536941\n",
      "iteration 200 / 15000: loss 337.030987\n",
      "iteration 300 / 15000: loss 312.616926\n",
      "iteration 400 / 15000: loss 289.625096\n",
      "iteration 500 / 15000: loss 268.432089\n",
      "iteration 600 / 15000: loss 248.860759\n",
      "iteration 700 / 15000: loss 231.093415\n",
      "iteration 800 / 15000: loss 214.104936\n",
      "iteration 900 / 15000: loss 198.977411\n",
      "iteration 1000 / 15000: loss 184.669660\n",
      "iteration 1100 / 15000: loss 171.321654\n",
      "iteration 1200 / 15000: loss 158.668084\n",
      "iteration 1300 / 15000: loss 147.614776\n",
      "iteration 1400 / 15000: loss 136.873935\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "#learning_rates = [1e-7, 5e-7]\n",
    "#regularization_strengths = [2.5e4, 5e4]\n",
    "learning_rates = [0.08e-7,0.1e-7,0.15e-7,0.25e-7]\n",
    "regularization_strengths = [0.8e4,1.0e4,1.25e4,1.5e4,2e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "num_iters=15000\n",
    "\n",
    "for l in learning_rates:\n",
    "    for r in regularization_strengths:\n",
    "        print(\"for r=\",r,\" and l=\",l)\n",
    "        softmax = Softmax()\n",
    "\n",
    "        softmax.train(X_train, y_train, learning_rate=l, reg=r,\n",
    "                              num_iters=num_iters, verbose=True)\n",
    "        \n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        training_accuracy = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        validation_accuracy=np.mean(y_val == y_val_pred)\n",
    "        results[(l,r)]=(training_accuracy,validation_accuracy) \n",
    "        if validation_accuracy > best_val:\n",
    "            best_val = validation_accuracy\n",
    "            best_softmax = softmax\n",
    "            \n",
    "        print()\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
