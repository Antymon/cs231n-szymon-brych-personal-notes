{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.393702\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Classifier has initially no knowledge of data it will be classifying, so the probability of classifying given sample as an instance of any of C classes is 1/C (here 0.1). Softmax transforms scores to values that can be represented as probabilites. Weights are initially set at random thus small differences from expected value are very likely.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -2.472078 analytic: -2.472078, relative error: 3.019742e-09\n",
      "numerical: 1.580447 analytic: 1.580447, relative error: 8.832478e-09\n",
      "numerical: 2.694756 analytic: 2.694756, relative error: 1.694637e-08\n",
      "numerical: 1.950125 analytic: 1.950125, relative error: 1.335345e-08\n",
      "numerical: 0.558191 analytic: 0.558191, relative error: 7.150936e-08\n",
      "numerical: -0.764298 analytic: -0.764298, relative error: 2.852595e-08\n",
      "numerical: 0.102410 analytic: 0.102409, relative error: 1.743831e-07\n",
      "numerical: 0.548510 analytic: 0.548510, relative error: 4.400031e-08\n",
      "numerical: -0.186663 analytic: -0.186663, relative error: 4.953582e-08\n",
      "numerical: -0.195586 analytic: -0.195586, relative error: 2.263244e-07\n",
      "numerical: -0.470785 analytic: -0.470785, relative error: 4.284455e-08\n",
      "numerical: 5.136394 analytic: 5.136394, relative error: 2.047240e-08\n",
      "numerical: 1.677668 analytic: 1.677668, relative error: 4.926245e-08\n",
      "numerical: 0.988206 analytic: 0.988206, relative error: 9.169239e-09\n",
      "numerical: -2.099311 analytic: -2.099311, relative error: 8.228148e-09\n",
      "numerical: -0.579160 analytic: -0.579161, relative error: 2.767845e-08\n",
      "numerical: -1.918474 analytic: -1.918474, relative error: 1.599505e-08\n",
      "numerical: -0.729810 analytic: -0.729810, relative error: 5.926416e-09\n",
      "numerical: -0.503354 analytic: -0.503354, relative error: 8.167761e-09\n",
      "numerical: -1.458659 analytic: -1.458659, relative error: 1.265829e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.393711e+00 computed in 0.136102s\n",
      "vectorized loss: 2.393711e+00 computed in 0.005004s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for r= 8000.0  and l= 8e-09\n",
      "iteration 0 / 15000: loss 252.752123\n",
      "iteration 100 / 15000: loss 246.552040\n",
      "iteration 200 / 15000: loss 239.391816\n",
      "iteration 300 / 15000: loss 233.237623\n",
      "iteration 400 / 15000: loss 227.606876\n",
      "iteration 500 / 15000: loss 221.615816\n",
      "iteration 600 / 15000: loss 215.715174\n",
      "iteration 700 / 15000: loss 210.157642\n",
      "iteration 800 / 15000: loss 204.866536\n",
      "iteration 900 / 15000: loss 199.655359\n",
      "iteration 1000 / 15000: loss 194.462495\n",
      "iteration 1100 / 15000: loss 189.570276\n",
      "iteration 1200 / 15000: loss 184.496915\n",
      "iteration 1300 / 15000: loss 180.328052\n",
      "iteration 1400 / 15000: loss 175.934977\n",
      "iteration 1500 / 15000: loss 171.203208\n",
      "iteration 1600 / 15000: loss 166.968648\n",
      "iteration 1700 / 15000: loss 162.647116\n",
      "iteration 1800 / 15000: loss 158.097033\n",
      "iteration 1900 / 15000: loss 154.727297\n",
      "iteration 2000 / 15000: loss 150.682042\n",
      "iteration 2100 / 15000: loss 146.885257\n",
      "iteration 2200 / 15000: loss 143.260869\n",
      "iteration 2300 / 15000: loss 139.549521\n",
      "iteration 2400 / 15000: loss 136.069226\n",
      "iteration 2500 / 15000: loss 132.585898\n",
      "iteration 2600 / 15000: loss 129.234865\n",
      "iteration 2700 / 15000: loss 126.032853\n",
      "iteration 2800 / 15000: loss 122.929115\n",
      "iteration 2900 / 15000: loss 119.716433\n",
      "iteration 3000 / 15000: loss 116.699821\n",
      "iteration 3100 / 15000: loss 114.044843\n",
      "iteration 3200 / 15000: loss 110.990727\n",
      "iteration 3300 / 15000: loss 108.136085\n",
      "iteration 3400 / 15000: loss 105.611936\n",
      "iteration 3500 / 15000: loss 102.890522\n",
      "iteration 3600 / 15000: loss 100.285006\n",
      "iteration 3700 / 15000: loss 97.714068\n",
      "iteration 3800 / 15000: loss 95.186043\n",
      "iteration 3900 / 15000: loss 92.880317\n",
      "iteration 4000 / 15000: loss 90.599838\n",
      "iteration 4100 / 15000: loss 88.457179\n",
      "iteration 4200 / 15000: loss 86.161153\n",
      "iteration 4300 / 15000: loss 84.036613\n",
      "iteration 4400 / 15000: loss 81.976121\n",
      "iteration 4500 / 15000: loss 80.070752\n",
      "iteration 4600 / 15000: loss 77.902229\n",
      "iteration 4700 / 15000: loss 76.083314\n",
      "iteration 4800 / 15000: loss 74.096484\n",
      "iteration 4900 / 15000: loss 72.210812\n",
      "iteration 5000 / 15000: loss 70.495439\n",
      "iteration 5100 / 15000: loss 68.859658\n",
      "iteration 5200 / 15000: loss 67.106569\n",
      "iteration 5300 / 15000: loss 65.456780\n",
      "iteration 5400 / 15000: loss 63.611355\n",
      "iteration 5500 / 15000: loss 62.206518\n",
      "iteration 5600 / 15000: loss 60.700228\n",
      "iteration 5700 / 15000: loss 59.189970\n",
      "iteration 5800 / 15000: loss 57.708186\n",
      "iteration 5900 / 15000: loss 56.267908\n",
      "iteration 6000 / 15000: loss 54.830048\n",
      "iteration 6100 / 15000: loss 53.678741\n",
      "iteration 6200 / 15000: loss 52.133746\n",
      "iteration 6300 / 15000: loss 51.024338\n",
      "iteration 6400 / 15000: loss 49.677990\n",
      "iteration 6500 / 15000: loss 48.464194\n",
      "iteration 6600 / 15000: loss 47.265135\n",
      "iteration 6700 / 15000: loss 46.119627\n",
      "iteration 6800 / 15000: loss 44.941459\n",
      "iteration 6900 / 15000: loss 44.012300\n",
      "iteration 7000 / 15000: loss 42.851380\n",
      "iteration 7100 / 15000: loss 41.850206\n",
      "iteration 7200 / 15000: loss 40.817326\n",
      "iteration 7300 / 15000: loss 39.952757\n",
      "iteration 7400 / 15000: loss 38.941771\n",
      "iteration 7500 / 15000: loss 37.798128\n",
      "iteration 7600 / 15000: loss 37.102050\n",
      "iteration 7700 / 15000: loss 35.992439\n",
      "iteration 7800 / 15000: loss 35.171156\n",
      "iteration 7900 / 15000: loss 34.428140\n",
      "iteration 8000 / 15000: loss 33.562821\n",
      "iteration 8100 / 15000: loss 32.756159\n",
      "iteration 8200 / 15000: loss 31.991352\n",
      "iteration 8300 / 15000: loss 31.099168\n",
      "iteration 8400 / 15000: loss 30.449123\n",
      "iteration 8500 / 15000: loss 29.911386\n",
      "iteration 8600 / 15000: loss 29.165409\n",
      "iteration 8700 / 15000: loss 28.316811\n",
      "iteration 8800 / 15000: loss 27.706177\n",
      "iteration 8900 / 15000: loss 27.031751\n",
      "iteration 9000 / 15000: loss 26.351371\n",
      "iteration 9100 / 15000: loss 25.881928\n",
      "iteration 9200 / 15000: loss 25.086252\n",
      "iteration 9300 / 15000: loss 24.558864\n",
      "iteration 9400 / 15000: loss 24.070841\n",
      "iteration 9500 / 15000: loss 23.455431\n",
      "iteration 9600 / 15000: loss 22.920577\n",
      "iteration 9700 / 15000: loss 22.334274\n",
      "iteration 9800 / 15000: loss 21.860885\n",
      "iteration 9900 / 15000: loss 21.441528\n",
      "iteration 10000 / 15000: loss 20.904432\n",
      "iteration 10100 / 15000: loss 20.375845\n",
      "iteration 10200 / 15000: loss 20.000480\n",
      "iteration 10300 / 15000: loss 19.530469\n",
      "iteration 10400 / 15000: loss 19.047964\n",
      "iteration 10500 / 15000: loss 18.617110\n",
      "iteration 10600 / 15000: loss 18.216826\n",
      "iteration 10700 / 15000: loss 17.818963\n",
      "iteration 10800 / 15000: loss 17.441194\n",
      "iteration 10900 / 15000: loss 17.022052\n",
      "iteration 11000 / 15000: loss 16.564921\n",
      "iteration 11100 / 15000: loss 16.210100\n",
      "iteration 11200 / 15000: loss 15.786138\n",
      "iteration 11300 / 15000: loss 15.480115\n",
      "iteration 11400 / 15000: loss 15.083570\n",
      "iteration 11500 / 15000: loss 14.854776\n",
      "iteration 11600 / 15000: loss 14.481815\n",
      "iteration 11700 / 15000: loss 14.228855\n",
      "iteration 11800 / 15000: loss 13.846360\n",
      "iteration 11900 / 15000: loss 13.580081\n",
      "iteration 12000 / 15000: loss 13.331799\n",
      "iteration 12100 / 15000: loss 12.957546\n",
      "iteration 12200 / 15000: loss 12.661156\n",
      "iteration 12300 / 15000: loss 12.419905\n",
      "iteration 12400 / 15000: loss 12.167049\n",
      "iteration 12500 / 15000: loss 11.959623\n",
      "iteration 12600 / 15000: loss 11.678296\n",
      "iteration 12700 / 15000: loss 11.428126\n",
      "iteration 12800 / 15000: loss 11.177243\n",
      "iteration 12900 / 15000: loss 10.904290\n",
      "iteration 13000 / 15000: loss 10.799797\n",
      "iteration 13100 / 15000: loss 10.487579\n",
      "iteration 13200 / 15000: loss 10.269995\n",
      "iteration 13300 / 15000: loss 10.052273\n",
      "iteration 13400 / 15000: loss 9.876171\n",
      "iteration 13500 / 15000: loss 9.583645\n",
      "iteration 13600 / 15000: loss 9.542865\n",
      "iteration 13700 / 15000: loss 9.206286\n",
      "iteration 13800 / 15000: loss 9.054944\n",
      "iteration 13900 / 15000: loss 8.992226\n",
      "iteration 14000 / 15000: loss 8.717031\n",
      "iteration 14100 / 15000: loss 8.586661\n",
      "iteration 14200 / 15000: loss 8.468695\n",
      "iteration 14300 / 15000: loss 8.335359\n",
      "iteration 14400 / 15000: loss 8.038640\n",
      "iteration 14500 / 15000: loss 7.916150\n",
      "iteration 14600 / 15000: loss 7.804884\n",
      "iteration 14700 / 15000: loss 7.533218\n",
      "iteration 14800 / 15000: loss 7.461870\n",
      "iteration 14900 / 15000: loss 7.337994\n",
      "\n",
      "for r= 10000.0  and l= 8e-09\n",
      "iteration 0 / 15000: loss 314.564666\n",
      "iteration 100 / 15000: loss 304.869089\n",
      "iteration 200 / 15000: loss 295.018868\n",
      "iteration 300 / 15000: loss 285.683377\n",
      "iteration 400 / 15000: loss 276.787872\n",
      "iteration 500 / 15000: loss 267.510892\n",
      "iteration 600 / 15000: loss 259.251987\n",
      "iteration 700 / 15000: loss 251.248524\n",
      "iteration 800 / 15000: loss 243.055356\n",
      "iteration 900 / 15000: loss 235.368733\n",
      "iteration 1000 / 15000: loss 228.100596\n",
      "iteration 1100 / 15000: loss 220.765154\n",
      "iteration 1200 / 15000: loss 213.993678\n",
      "iteration 1300 / 15000: loss 206.893433\n",
      "iteration 1400 / 15000: loss 200.305475\n",
      "iteration 1500 / 15000: loss 194.277191\n",
      "iteration 1600 / 15000: loss 188.079710\n",
      "iteration 1700 / 15000: loss 182.176961\n",
      "iteration 1800 / 15000: loss 176.151948\n",
      "iteration 1900 / 15000: loss 170.728505\n",
      "iteration 2000 / 15000: loss 165.592090\n",
      "iteration 2100 / 15000: loss 160.366964\n",
      "iteration 2200 / 15000: loss 155.366317\n",
      "iteration 2300 / 15000: loss 150.187251\n",
      "iteration 2400 / 15000: loss 145.532126\n",
      "iteration 2500 / 15000: loss 141.263457\n",
      "iteration 2600 / 15000: loss 136.817127\n",
      "iteration 2700 / 15000: loss 132.433825\n",
      "iteration 2800 / 15000: loss 128.374531\n",
      "iteration 2900 / 15000: loss 124.277277\n",
      "iteration 3000 / 15000: loss 120.721042\n",
      "iteration 3100 / 15000: loss 116.771081\n",
      "iteration 3200 / 15000: loss 112.934736\n",
      "iteration 3300 / 15000: loss 109.650133\n",
      "iteration 3400 / 15000: loss 106.321008\n",
      "iteration 3500 / 15000: loss 102.842829\n",
      "iteration 3600 / 15000: loss 99.563717\n",
      "iteration 3700 / 15000: loss 96.541910\n",
      "iteration 3800 / 15000: loss 93.626659\n",
      "iteration 3900 / 15000: loss 90.688490\n",
      "iteration 4000 / 15000: loss 87.797093\n",
      "iteration 4100 / 15000: loss 85.006741\n",
      "iteration 4200 / 15000: loss 82.543442\n",
      "iteration 4300 / 15000: loss 79.938216\n",
      "iteration 4400 / 15000: loss 77.451482\n",
      "iteration 4500 / 15000: loss 75.018781\n",
      "iteration 4600 / 15000: loss 72.882483\n",
      "iteration 4700 / 15000: loss 70.367387\n",
      "iteration 4800 / 15000: loss 68.270916\n",
      "iteration 4900 / 15000: loss 66.311074\n",
      "iteration 5000 / 15000: loss 64.307174\n",
      "iteration 5100 / 15000: loss 62.322466\n",
      "iteration 5200 / 15000: loss 60.139800\n",
      "iteration 5300 / 15000: loss 58.504352\n",
      "iteration 5400 / 15000: loss 56.544818\n",
      "iteration 5500 / 15000: loss 54.996023\n",
      "iteration 5600 / 15000: loss 53.276137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5700 / 15000: loss 51.630864\n",
      "iteration 5800 / 15000: loss 50.110574\n",
      "iteration 5900 / 15000: loss 48.611757\n",
      "iteration 6000 / 15000: loss 47.080420\n",
      "iteration 6100 / 15000: loss 45.587756\n",
      "iteration 6200 / 15000: loss 44.293922\n",
      "iteration 6300 / 15000: loss 42.921514\n",
      "iteration 6400 / 15000: loss 41.648869\n",
      "iteration 6500 / 15000: loss 40.439735\n",
      "iteration 6600 / 15000: loss 39.128394\n",
      "iteration 6700 / 15000: loss 38.088224\n",
      "iteration 6800 / 15000: loss 36.776974\n",
      "iteration 6900 / 15000: loss 35.751752\n",
      "iteration 7000 / 15000: loss 34.588053\n",
      "iteration 7100 / 15000: loss 33.588513\n",
      "iteration 7200 / 15000: loss 32.665721\n",
      "iteration 7300 / 15000: loss 31.648661\n",
      "iteration 7400 / 15000: loss 30.772139\n",
      "iteration 7500 / 15000: loss 29.802458\n",
      "iteration 7600 / 15000: loss 28.933235\n",
      "iteration 7700 / 15000: loss 28.079750\n",
      "iteration 7800 / 15000: loss 27.277442\n",
      "iteration 7900 / 15000: loss 26.508630\n",
      "iteration 8000 / 15000: loss 25.634655\n",
      "iteration 8100 / 15000: loss 24.919489\n",
      "iteration 8200 / 15000: loss 24.284267\n",
      "iteration 8300 / 15000: loss 23.537301\n",
      "iteration 8400 / 15000: loss 22.715166\n",
      "iteration 8500 / 15000: loss 22.237708\n",
      "iteration 8600 / 15000: loss 21.509823\n",
      "iteration 8700 / 15000: loss 20.970567\n",
      "iteration 8800 / 15000: loss 20.387117\n",
      "iteration 8900 / 15000: loss 19.682227\n",
      "iteration 9000 / 15000: loss 19.271422\n",
      "iteration 9100 / 15000: loss 18.619025\n",
      "iteration 9200 / 15000: loss 18.112598\n",
      "iteration 9300 / 15000: loss 17.518185\n",
      "iteration 9400 / 15000: loss 17.141073\n",
      "iteration 9500 / 15000: loss 16.617452\n",
      "iteration 9600 / 15000: loss 16.163508\n",
      "iteration 9700 / 15000: loss 15.692811\n",
      "iteration 9800 / 15000: loss 15.278374\n",
      "iteration 9900 / 15000: loss 14.810533\n",
      "iteration 10000 / 15000: loss 14.445896\n",
      "iteration 10100 / 15000: loss 14.035032\n",
      "iteration 10200 / 15000: loss 13.683693\n",
      "iteration 10300 / 15000: loss 13.423717\n",
      "iteration 10400 / 15000: loss 13.019254\n",
      "iteration 10500 / 15000: loss 12.581893\n",
      "iteration 10600 / 15000: loss 12.301835\n",
      "iteration 10700 / 15000: loss 11.990490\n",
      "iteration 10800 / 15000: loss 11.620681\n",
      "iteration 10900 / 15000: loss 11.263564\n",
      "iteration 11000 / 15000: loss 11.082055\n",
      "iteration 11100 / 15000: loss 10.773710\n",
      "iteration 11200 / 15000: loss 10.526482\n",
      "iteration 11300 / 15000: loss 10.236815\n",
      "iteration 11400 / 15000: loss 9.984108\n",
      "iteration 11500 / 15000: loss 9.678363\n",
      "iteration 11600 / 15000: loss 9.443987\n",
      "iteration 11700 / 15000: loss 9.297556\n",
      "iteration 11800 / 15000: loss 8.925755\n",
      "iteration 11900 / 15000: loss 8.836465\n",
      "iteration 12000 / 15000: loss 8.545292\n",
      "iteration 12100 / 15000: loss 8.310489\n",
      "iteration 12200 / 15000: loss 8.105215\n",
      "iteration 12300 / 15000: loss 8.020842\n",
      "iteration 12400 / 15000: loss 7.709650\n",
      "iteration 12500 / 15000: loss 7.564189\n",
      "iteration 12600 / 15000: loss 7.407880\n",
      "iteration 12700 / 15000: loss 7.195491\n",
      "iteration 12800 / 15000: loss 7.063193\n",
      "iteration 12900 / 15000: loss 6.829126\n",
      "iteration 13000 / 15000: loss 6.728859\n",
      "iteration 13100 / 15000: loss 6.617538\n",
      "iteration 13200 / 15000: loss 6.466110\n",
      "iteration 13300 / 15000: loss 6.303288\n",
      "iteration 13400 / 15000: loss 6.194072\n",
      "iteration 13500 / 15000: loss 6.054611\n",
      "iteration 13600 / 15000: loss 5.896344\n",
      "iteration 13700 / 15000: loss 5.802478\n",
      "iteration 13800 / 15000: loss 5.701343\n",
      "iteration 13900 / 15000: loss 5.556256\n",
      "iteration 14000 / 15000: loss 5.497700\n",
      "iteration 14100 / 15000: loss 5.350576\n",
      "iteration 14200 / 15000: loss 5.165745\n",
      "iteration 14300 / 15000: loss 5.134272\n",
      "iteration 14400 / 15000: loss 5.026930\n",
      "iteration 14500 / 15000: loss 4.931760\n",
      "iteration 14600 / 15000: loss 4.808276\n",
      "iteration 14700 / 15000: loss 4.812735\n",
      "iteration 14800 / 15000: loss 4.714599\n",
      "iteration 14900 / 15000: loss 4.544615\n",
      "\n",
      "for r= 12500.0  and l= 8e-09\n",
      "iteration 0 / 15000: loss 384.838283\n",
      "iteration 100 / 15000: loss 369.394890\n",
      "iteration 200 / 15000: loss 355.151609\n",
      "iteration 300 / 15000: loss 341.278584\n",
      "iteration 400 / 15000: loss 327.641709\n",
      "iteration 500 / 15000: loss 314.740926\n",
      "iteration 600 / 15000: loss 302.420224\n",
      "iteration 700 / 15000: loss 290.487083\n",
      "iteration 800 / 15000: loss 278.838990\n",
      "iteration 900 / 15000: loss 267.988500\n",
      "iteration 1000 / 15000: loss 257.452077\n",
      "iteration 1100 / 15000: loss 247.306601\n",
      "iteration 1200 / 15000: loss 237.428770\n",
      "iteration 1300 / 15000: loss 228.498020\n",
      "iteration 1400 / 15000: loss 219.541566\n",
      "iteration 1500 / 15000: loss 210.724768\n",
      "iteration 1600 / 15000: loss 202.526277\n",
      "iteration 1700 / 15000: loss 194.966310\n",
      "iteration 1800 / 15000: loss 187.223817\n",
      "iteration 1900 / 15000: loss 179.737933\n",
      "iteration 2000 / 15000: loss 172.794811\n",
      "iteration 2100 / 15000: loss 166.037814\n",
      "iteration 2200 / 15000: loss 159.748801\n",
      "iteration 2300 / 15000: loss 153.245099\n",
      "iteration 2400 / 15000: loss 147.409710\n",
      "iteration 2500 / 15000: loss 141.571298\n",
      "iteration 2600 / 15000: loss 136.370481\n",
      "iteration 2700 / 15000: loss 130.910221\n",
      "iteration 2800 / 15000: loss 125.693714\n",
      "iteration 2900 / 15000: loss 120.865858\n",
      "iteration 3000 / 15000: loss 116.215878\n",
      "iteration 3100 / 15000: loss 111.841336\n",
      "iteration 3200 / 15000: loss 107.571006\n",
      "iteration 3300 / 15000: loss 103.304708\n",
      "iteration 3400 / 15000: loss 99.306919\n",
      "iteration 3500 / 15000: loss 95.463433\n",
      "iteration 3600 / 15000: loss 91.804638\n",
      "iteration 3700 / 15000: loss 88.157304\n",
      "iteration 3800 / 15000: loss 84.729495\n",
      "iteration 3900 / 15000: loss 81.560100\n",
      "iteration 4000 / 15000: loss 78.353681\n",
      "iteration 4100 / 15000: loss 75.423383\n",
      "iteration 4200 / 15000: loss 72.502314\n",
      "iteration 4300 / 15000: loss 69.753555\n",
      "iteration 4400 / 15000: loss 66.986503\n",
      "iteration 4500 / 15000: loss 64.541030\n",
      "iteration 4600 / 15000: loss 62.024870\n",
      "iteration 4700 / 15000: loss 59.669199\n",
      "iteration 4800 / 15000: loss 57.389579\n",
      "iteration 4900 / 15000: loss 55.181352\n",
      "iteration 5000 / 15000: loss 53.058785\n",
      "iteration 5100 / 15000: loss 51.154347\n",
      "iteration 5200 / 15000: loss 49.307923\n",
      "iteration 5300 / 15000: loss 47.287012\n",
      "iteration 5400 / 15000: loss 45.641013\n",
      "iteration 5500 / 15000: loss 43.924788\n",
      "iteration 5600 / 15000: loss 42.172774\n",
      "iteration 5700 / 15000: loss 40.629056\n",
      "iteration 5800 / 15000: loss 39.038320\n",
      "iteration 5900 / 15000: loss 37.555283\n",
      "iteration 6000 / 15000: loss 36.255527\n",
      "iteration 6100 / 15000: loss 34.904005\n",
      "iteration 6200 / 15000: loss 33.649610\n",
      "iteration 6300 / 15000: loss 32.306406\n",
      "iteration 6400 / 15000: loss 31.109222\n",
      "iteration 6500 / 15000: loss 30.008234\n",
      "iteration 6600 / 15000: loss 28.840331\n",
      "iteration 6700 / 15000: loss 27.853722\n",
      "iteration 6800 / 15000: loss 26.804366\n",
      "iteration 6900 / 15000: loss 25.881778\n",
      "iteration 7000 / 15000: loss 24.854919\n",
      "iteration 7100 / 15000: loss 24.058339\n",
      "iteration 7200 / 15000: loss 23.133132\n",
      "iteration 7300 / 15000: loss 22.346762\n",
      "iteration 7400 / 15000: loss 21.514986\n",
      "iteration 7500 / 15000: loss 20.758746\n",
      "iteration 7600 / 15000: loss 19.987064\n",
      "iteration 7700 / 15000: loss 19.262924\n",
      "iteration 7800 / 15000: loss 18.608370\n",
      "iteration 7900 / 15000: loss 17.927182\n",
      "iteration 8000 / 15000: loss 17.320214\n",
      "iteration 8100 / 15000: loss 16.746732\n",
      "iteration 8200 / 15000: loss 16.193243\n",
      "iteration 8300 / 15000: loss 15.605688\n",
      "iteration 8400 / 15000: loss 15.107622\n",
      "iteration 8500 / 15000: loss 14.578100\n",
      "iteration 8600 / 15000: loss 13.970496\n",
      "iteration 8700 / 15000: loss 13.468447\n",
      "iteration 8800 / 15000: loss 13.184944\n",
      "iteration 8900 / 15000: loss 12.646934\n",
      "iteration 9000 / 15000: loss 12.319045\n",
      "iteration 9100 / 15000: loss 11.911274\n",
      "iteration 9200 / 15000: loss 11.473633\n",
      "iteration 9300 / 15000: loss 11.144141\n",
      "iteration 9400 / 15000: loss 10.738287\n",
      "iteration 9500 / 15000: loss 10.365883\n",
      "iteration 9600 / 15000: loss 10.087959\n",
      "iteration 9700 / 15000: loss 9.730775\n",
      "iteration 9800 / 15000: loss 9.483462\n",
      "iteration 9900 / 15000: loss 9.142112\n",
      "iteration 10000 / 15000: loss 8.834739\n",
      "iteration 10100 / 15000: loss 8.645686\n",
      "iteration 10200 / 15000: loss 8.355694\n",
      "iteration 10300 / 15000: loss 8.078009\n",
      "iteration 10400 / 15000: loss 7.924680\n",
      "iteration 10500 / 15000: loss 7.612990\n",
      "iteration 10600 / 15000: loss 7.449835\n",
      "iteration 10700 / 15000: loss 7.232499\n",
      "iteration 10800 / 15000: loss 7.043898\n",
      "iteration 10900 / 15000: loss 6.793490\n",
      "iteration 11000 / 15000: loss 6.655630\n",
      "iteration 11100 / 15000: loss 6.473420\n",
      "iteration 11200 / 15000: loss 6.195983\n",
      "iteration 11300 / 15000: loss 6.027734\n",
      "iteration 11400 / 15000: loss 5.868923\n",
      "iteration 11500 / 15000: loss 5.751606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11600 / 15000: loss 5.660914\n",
      "iteration 11700 / 15000: loss 5.530702\n",
      "iteration 11800 / 15000: loss 5.324069\n",
      "iteration 11900 / 15000: loss 5.203950\n",
      "iteration 12000 / 15000: loss 5.151467\n",
      "iteration 12100 / 15000: loss 5.008450\n",
      "iteration 12200 / 15000: loss 4.798388\n",
      "iteration 12300 / 15000: loss 4.737577\n",
      "iteration 12400 / 15000: loss 4.599275\n",
      "iteration 12500 / 15000: loss 4.578937\n",
      "iteration 12600 / 15000: loss 4.441281\n",
      "iteration 12700 / 15000: loss 4.381971\n",
      "iteration 12800 / 15000: loss 4.250731\n",
      "iteration 12900 / 15000: loss 4.185591\n",
      "iteration 13000 / 15000: loss 4.089492\n",
      "iteration 13100 / 15000: loss 4.024511\n",
      "iteration 13200 / 15000: loss 3.939313\n",
      "iteration 13300 / 15000: loss 3.890887\n",
      "iteration 13400 / 15000: loss 3.759804\n",
      "iteration 13500 / 15000: loss 3.673930\n",
      "iteration 13600 / 15000: loss 3.607477\n",
      "iteration 13700 / 15000: loss 3.545198\n",
      "iteration 13800 / 15000: loss 3.478368\n",
      "iteration 13900 / 15000: loss 3.427031\n",
      "iteration 14000 / 15000: loss 3.341348\n",
      "iteration 14100 / 15000: loss 3.331984\n",
      "iteration 14200 / 15000: loss 3.305938\n",
      "iteration 14300 / 15000: loss 3.335773\n",
      "iteration 14400 / 15000: loss 3.196366\n",
      "iteration 14500 / 15000: loss 3.125318\n",
      "iteration 14600 / 15000: loss 3.042031\n",
      "iteration 14700 / 15000: loss 3.046093\n",
      "iteration 14800 / 15000: loss 3.023121\n",
      "iteration 14900 / 15000: loss 3.022243\n",
      "\n",
      "for r= 15000.0  and l= 8e-09\n",
      "iteration 0 / 15000: loss 467.189710\n",
      "iteration 100 / 15000: loss 444.965441\n",
      "iteration 200 / 15000: loss 424.134813\n",
      "iteration 300 / 15000: loss 403.827105\n",
      "iteration 400 / 15000: loss 385.149023\n",
      "iteration 500 / 15000: loss 367.058632\n",
      "iteration 600 / 15000: loss 349.753222\n",
      "iteration 700 / 15000: loss 333.181150\n",
      "iteration 800 / 15000: loss 317.631057\n",
      "iteration 900 / 15000: loss 302.884399\n",
      "iteration 1000 / 15000: loss 288.228380\n",
      "iteration 1100 / 15000: loss 274.869187\n",
      "iteration 1200 / 15000: loss 262.089288\n",
      "iteration 1300 / 15000: loss 249.848317\n",
      "iteration 1400 / 15000: loss 238.179720\n",
      "iteration 1500 / 15000: loss 227.162218\n",
      "iteration 1600 / 15000: loss 216.587360\n",
      "iteration 1700 / 15000: loss 206.518447\n",
      "iteration 1800 / 15000: loss 196.862058\n",
      "iteration 1900 / 15000: loss 187.539809\n",
      "iteration 2000 / 15000: loss 178.736473\n",
      "iteration 2100 / 15000: loss 170.420605\n",
      "iteration 2200 / 15000: loss 162.381500\n",
      "iteration 2300 / 15000: loss 155.019726\n",
      "iteration 2400 / 15000: loss 147.837332\n",
      "iteration 2500 / 15000: loss 141.127668\n",
      "iteration 2600 / 15000: loss 134.277936\n",
      "iteration 2700 / 15000: loss 128.278490\n",
      "iteration 2800 / 15000: loss 122.291876\n",
      "iteration 2900 / 15000: loss 116.545711\n",
      "iteration 3000 / 15000: loss 111.184878\n",
      "iteration 3100 / 15000: loss 106.115792\n",
      "iteration 3200 / 15000: loss 101.106742\n",
      "iteration 3300 / 15000: loss 96.557344\n",
      "iteration 3400 / 15000: loss 92.022582\n",
      "iteration 3500 / 15000: loss 87.759586\n",
      "iteration 3600 / 15000: loss 83.778846\n",
      "iteration 3700 / 15000: loss 79.952347\n",
      "iteration 3800 / 15000: loss 76.268618\n",
      "iteration 3900 / 15000: loss 72.778733\n",
      "iteration 4000 / 15000: loss 69.437967\n",
      "iteration 4100 / 15000: loss 66.249506\n",
      "iteration 4200 / 15000: loss 63.159567\n",
      "iteration 4300 / 15000: loss 60.332274\n",
      "iteration 4400 / 15000: loss 57.644113\n",
      "iteration 4500 / 15000: loss 54.869934\n",
      "iteration 4600 / 15000: loss 52.483140\n",
      "iteration 4700 / 15000: loss 50.103200\n",
      "iteration 4800 / 15000: loss 47.974621\n",
      "iteration 4900 / 15000: loss 45.760458\n",
      "iteration 5000 / 15000: loss 43.623283\n",
      "iteration 5100 / 15000: loss 41.699416\n",
      "iteration 5200 / 15000: loss 39.908553\n",
      "iteration 5300 / 15000: loss 38.121968\n",
      "iteration 5400 / 15000: loss 36.360865\n",
      "iteration 5500 / 15000: loss 34.801766\n",
      "iteration 5600 / 15000: loss 33.172770\n",
      "iteration 5700 / 15000: loss 31.745626\n",
      "iteration 5800 / 15000: loss 30.371559\n",
      "iteration 5900 / 15000: loss 29.007627\n",
      "iteration 6000 / 15000: loss 27.746039\n",
      "iteration 6100 / 15000: loss 26.514965\n",
      "iteration 6200 / 15000: loss 25.376831\n",
      "iteration 6300 / 15000: loss 24.362429\n",
      "iteration 6400 / 15000: loss 23.167688\n",
      "iteration 6500 / 15000: loss 22.352450\n",
      "iteration 6600 / 15000: loss 21.379396\n",
      "iteration 6700 / 15000: loss 20.382640\n",
      "iteration 6800 / 15000: loss 19.608714\n",
      "iteration 6900 / 15000: loss 18.710832\n",
      "iteration 7000 / 15000: loss 17.907681\n",
      "iteration 7100 / 15000: loss 17.112124\n",
      "iteration 7200 / 15000: loss 16.402520\n",
      "iteration 7300 / 15000: loss 15.778923\n",
      "iteration 7400 / 15000: loss 15.144702\n",
      "iteration 7500 / 15000: loss 14.542306\n",
      "iteration 7600 / 15000: loss 13.984843\n",
      "iteration 7700 / 15000: loss 13.326151\n",
      "iteration 7800 / 15000: loss 12.867697\n",
      "iteration 7900 / 15000: loss 12.301666\n",
      "iteration 8000 / 15000: loss 11.770511\n",
      "iteration 8100 / 15000: loss 11.451283\n",
      "iteration 8200 / 15000: loss 10.916195\n",
      "iteration 8300 / 15000: loss 10.570788\n",
      "iteration 8400 / 15000: loss 10.148368\n",
      "iteration 8500 / 15000: loss 9.749193\n",
      "iteration 8600 / 15000: loss 9.354950\n",
      "iteration 8700 / 15000: loss 9.066711\n",
      "iteration 8800 / 15000: loss 8.779229\n",
      "iteration 8900 / 15000: loss 8.427485\n",
      "iteration 9000 / 15000: loss 8.168317\n",
      "iteration 9100 / 15000: loss 7.830810\n",
      "iteration 9200 / 15000: loss 7.609960\n",
      "iteration 9300 / 15000: loss 7.269483\n",
      "iteration 9400 / 15000: loss 7.044225\n",
      "iteration 9500 / 15000: loss 6.894903\n",
      "iteration 9600 / 15000: loss 6.548552\n",
      "iteration 9700 / 15000: loss 6.411008\n",
      "iteration 9800 / 15000: loss 6.181598\n",
      "iteration 9900 / 15000: loss 6.008201\n",
      "iteration 10000 / 15000: loss 5.759443\n",
      "iteration 10100 / 15000: loss 5.638182\n",
      "iteration 10200 / 15000: loss 5.502531\n",
      "iteration 10300 / 15000: loss 5.238172\n",
      "iteration 10400 / 15000: loss 5.093001\n",
      "iteration 10500 / 15000: loss 5.033291\n",
      "iteration 10600 / 15000: loss 4.876778\n",
      "iteration 10700 / 15000: loss 4.737740\n",
      "iteration 10800 / 15000: loss 4.571740\n",
      "iteration 10900 / 15000: loss 4.478459\n",
      "iteration 11000 / 15000: loss 4.308761\n",
      "iteration 11100 / 15000: loss 4.240569\n",
      "iteration 11200 / 15000: loss 4.132661\n",
      "iteration 11300 / 15000: loss 4.053254\n",
      "iteration 11400 / 15000: loss 3.969707\n",
      "iteration 11500 / 15000: loss 3.802891\n",
      "iteration 11600 / 15000: loss 3.766225\n",
      "iteration 11700 / 15000: loss 3.689987\n",
      "iteration 11800 / 15000: loss 3.574203\n",
      "iteration 11900 / 15000: loss 3.508960\n",
      "iteration 12000 / 15000: loss 3.447758\n",
      "iteration 12100 / 15000: loss 3.397079\n",
      "iteration 12200 / 15000: loss 3.304854\n",
      "iteration 12300 / 15000: loss 3.302232\n",
      "iteration 12400 / 15000: loss 3.229581\n",
      "iteration 12500 / 15000: loss 3.214872\n",
      "iteration 12600 / 15000: loss 3.094203\n",
      "iteration 12700 / 15000: loss 3.019788\n",
      "iteration 12800 / 15000: loss 2.963345\n",
      "iteration 12900 / 15000: loss 2.992789\n",
      "iteration 13000 / 15000: loss 2.855331\n",
      "iteration 13100 / 15000: loss 2.915324\n",
      "iteration 13200 / 15000: loss 2.916068\n",
      "iteration 13300 / 15000: loss 2.853862\n",
      "iteration 13400 / 15000: loss 2.765962\n",
      "iteration 13500 / 15000: loss 2.770403\n",
      "iteration 13600 / 15000: loss 2.785034\n",
      "iteration 13700 / 15000: loss 2.703023\n",
      "iteration 13800 / 15000: loss 2.590132\n",
      "iteration 13900 / 15000: loss 2.558464\n",
      "iteration 14000 / 15000: loss 2.599073\n",
      "iteration 14100 / 15000: loss 2.536963\n",
      "iteration 14200 / 15000: loss 2.518716\n",
      "iteration 14300 / 15000: loss 2.559059\n",
      "iteration 14400 / 15000: loss 2.526691\n",
      "iteration 14500 / 15000: loss 2.517317\n",
      "iteration 14600 / 15000: loss 2.387580\n",
      "iteration 14700 / 15000: loss 2.389325\n",
      "iteration 14800 / 15000: loss 2.437425\n",
      "iteration 14900 / 15000: loss 2.409683\n",
      "\n",
      "for r= 20000.0  and l= 8e-09\n",
      "iteration 0 / 15000: loss 616.793439\n",
      "iteration 100 / 15000: loss 577.934194\n",
      "iteration 200 / 15000: loss 541.463624\n",
      "iteration 300 / 15000: loss 508.188847\n",
      "iteration 400 / 15000: loss 476.626099\n",
      "iteration 500 / 15000: loss 447.253549\n",
      "iteration 600 / 15000: loss 419.417137\n",
      "iteration 700 / 15000: loss 393.714793\n",
      "iteration 800 / 15000: loss 369.135513\n",
      "iteration 900 / 15000: loss 346.425020\n",
      "iteration 1000 / 15000: loss 324.636445\n",
      "iteration 1100 / 15000: loss 304.739376\n",
      "iteration 1200 / 15000: loss 285.964177\n",
      "iteration 1300 / 15000: loss 268.429635\n",
      "iteration 1400 / 15000: loss 251.587536\n",
      "iteration 1500 / 15000: loss 236.032950\n",
      "iteration 1600 / 15000: loss 221.544559\n",
      "iteration 1700 / 15000: loss 207.811317\n",
      "iteration 1800 / 15000: loss 195.262984\n",
      "iteration 1900 / 15000: loss 183.161388\n",
      "iteration 2000 / 15000: loss 171.748606\n",
      "iteration 2100 / 15000: loss 161.264670\n",
      "iteration 2200 / 15000: loss 151.429230\n",
      "iteration 2300 / 15000: loss 142.026809\n",
      "iteration 2400 / 15000: loss 133.304717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2500 / 15000: loss 125.333573\n",
      "iteration 2600 / 15000: loss 117.611146\n",
      "iteration 2700 / 15000: loss 110.374917\n",
      "iteration 2800 / 15000: loss 103.631927\n",
      "iteration 2900 / 15000: loss 97.302898\n",
      "iteration 3000 / 15000: loss 91.488882\n",
      "iteration 3100 / 15000: loss 85.804174\n",
      "iteration 3200 / 15000: loss 80.604982\n",
      "iteration 3300 / 15000: loss 75.735764\n",
      "iteration 3400 / 15000: loss 71.135645\n",
      "iteration 3500 / 15000: loss 66.857593\n",
      "iteration 3600 / 15000: loss 62.862016\n",
      "iteration 3700 / 15000: loss 59.073395\n",
      "iteration 3800 / 15000: loss 55.517065\n",
      "iteration 3900 / 15000: loss 52.099663\n",
      "iteration 4000 / 15000: loss 49.050348\n",
      "iteration 4100 / 15000: loss 46.075734\n",
      "iteration 4200 / 15000: loss 43.440599\n",
      "iteration 4300 / 15000: loss 40.831038\n",
      "iteration 4400 / 15000: loss 38.377714\n",
      "iteration 4500 / 15000: loss 36.211284\n",
      "iteration 4600 / 15000: loss 33.998740\n",
      "iteration 4700 / 15000: loss 32.059190\n",
      "iteration 4800 / 15000: loss 30.144802\n",
      "iteration 4900 / 15000: loss 28.465002\n",
      "iteration 5000 / 15000: loss 26.790895\n",
      "iteration 5100 / 15000: loss 25.336657\n",
      "iteration 5200 / 15000: loss 23.834062\n",
      "iteration 5300 / 15000: loss 22.422014\n",
      "iteration 5400 / 15000: loss 21.116727\n",
      "iteration 5500 / 15000: loss 20.057893\n",
      "iteration 5600 / 15000: loss 18.885877\n",
      "iteration 5700 / 15000: loss 17.822139\n",
      "iteration 5800 / 15000: loss 16.822443\n",
      "iteration 5900 / 15000: loss 15.966725\n",
      "iteration 6000 / 15000: loss 15.061287\n",
      "iteration 6100 / 15000: loss 14.264889\n",
      "iteration 6200 / 15000: loss 13.523371\n",
      "iteration 6300 / 15000: loss 12.763215\n",
      "iteration 6400 / 15000: loss 12.038448\n",
      "iteration 6500 / 15000: loss 11.474582\n",
      "iteration 6600 / 15000: loss 10.945558\n",
      "iteration 6700 / 15000: loss 10.439931\n",
      "iteration 6800 / 15000: loss 9.907678\n",
      "iteration 6900 / 15000: loss 9.386038\n",
      "iteration 7000 / 15000: loss 8.935759\n",
      "iteration 7100 / 15000: loss 8.502694\n",
      "iteration 7200 / 15000: loss 8.026943\n",
      "iteration 7300 / 15000: loss 7.773553\n",
      "iteration 7400 / 15000: loss 7.385975\n",
      "iteration 7500 / 15000: loss 7.005956\n",
      "iteration 7600 / 15000: loss 6.764314\n",
      "iteration 7700 / 15000: loss 6.450965\n",
      "iteration 7800 / 15000: loss 6.169456\n",
      "iteration 7900 / 15000: loss 5.901576\n",
      "iteration 8000 / 15000: loss 5.683692\n",
      "iteration 8100 / 15000: loss 5.424395\n",
      "iteration 8200 / 15000: loss 5.265094\n",
      "iteration 8300 / 15000: loss 5.048471\n",
      "iteration 8400 / 15000: loss 4.846070\n",
      "iteration 8500 / 15000: loss 4.703195\n",
      "iteration 8600 / 15000: loss 4.494116\n",
      "iteration 8700 / 15000: loss 4.395224\n",
      "iteration 8800 / 15000: loss 4.168255\n",
      "iteration 8900 / 15000: loss 4.163916\n",
      "iteration 9000 / 15000: loss 3.984308\n",
      "iteration 9100 / 15000: loss 3.836424\n",
      "iteration 9200 / 15000: loss 3.682869\n",
      "iteration 9300 / 15000: loss 3.632298\n",
      "iteration 9400 / 15000: loss 3.491828\n",
      "iteration 9500 / 15000: loss 3.460753\n",
      "iteration 9600 / 15000: loss 3.436495\n",
      "iteration 9700 / 15000: loss 3.250660\n",
      "iteration 9800 / 15000: loss 3.240678\n",
      "iteration 9900 / 15000: loss 3.135487\n",
      "iteration 10000 / 15000: loss 3.082351\n",
      "iteration 10100 / 15000: loss 3.038850\n",
      "iteration 10200 / 15000: loss 2.913648\n",
      "iteration 10300 / 15000: loss 2.849331\n",
      "iteration 10400 / 15000: loss 2.790717\n",
      "iteration 10500 / 15000: loss 2.803356\n",
      "iteration 10600 / 15000: loss 2.752723\n",
      "iteration 10700 / 15000: loss 2.691073\n",
      "iteration 10800 / 15000: loss 2.631233\n",
      "iteration 10900 / 15000: loss 2.640706\n",
      "iteration 11000 / 15000: loss 2.586919\n",
      "iteration 11100 / 15000: loss 2.523792\n",
      "iteration 11200 / 15000: loss 2.527112\n",
      "iteration 11300 / 15000: loss 2.471921\n",
      "iteration 11400 / 15000: loss 2.482755\n",
      "iteration 11500 / 15000: loss 2.411659\n",
      "iteration 11600 / 15000: loss 2.386426\n",
      "iteration 11700 / 15000: loss 2.388864\n",
      "iteration 11800 / 15000: loss 2.469773\n",
      "iteration 11900 / 15000: loss 2.340444\n",
      "iteration 12000 / 15000: loss 2.327674\n",
      "iteration 12100 / 15000: loss 2.411357\n",
      "iteration 12200 / 15000: loss 2.359167\n",
      "iteration 12300 / 15000: loss 2.236537\n",
      "iteration 12400 / 15000: loss 2.215542\n",
      "iteration 12500 / 15000: loss 2.239501\n",
      "iteration 12600 / 15000: loss 2.215200\n",
      "iteration 12700 / 15000: loss 2.283677\n",
      "iteration 12800 / 15000: loss 2.197853\n",
      "iteration 12900 / 15000: loss 2.237580\n",
      "iteration 13000 / 15000: loss 2.200626\n",
      "iteration 13100 / 15000: loss 2.224306\n",
      "iteration 13200 / 15000: loss 2.205735\n",
      "iteration 13300 / 15000: loss 2.224810\n",
      "iteration 13400 / 15000: loss 2.136642\n",
      "iteration 13500 / 15000: loss 2.166207\n",
      "iteration 13600 / 15000: loss 2.153901\n",
      "iteration 13700 / 15000: loss 2.173992\n",
      "iteration 13800 / 15000: loss 2.058823\n",
      "iteration 13900 / 15000: loss 2.064759\n",
      "iteration 14000 / 15000: loss 2.178613\n",
      "iteration 14100 / 15000: loss 2.199405\n",
      "iteration 14200 / 15000: loss 2.149739\n",
      "iteration 14300 / 15000: loss 2.098973\n",
      "iteration 14400 / 15000: loss 2.103623\n",
      "iteration 14500 / 15000: loss 2.138119\n",
      "iteration 14600 / 15000: loss 2.100203\n",
      "iteration 14700 / 15000: loss 2.098305\n",
      "iteration 14800 / 15000: loss 2.066957\n",
      "iteration 14900 / 15000: loss 2.071664\n",
      "\n",
      "for r= 8000.0  and l= 1e-08\n",
      "iteration 0 / 15000: loss 248.952559\n",
      "iteration 100 / 15000: loss 241.148310\n",
      "iteration 200 / 15000: loss 233.170732\n",
      "iteration 300 / 15000: loss 225.413550\n",
      "iteration 400 / 15000: loss 217.948812\n",
      "iteration 500 / 15000: loss 211.476121\n",
      "iteration 600 / 15000: loss 204.226131\n",
      "iteration 700 / 15000: loss 197.902537\n",
      "iteration 800 / 15000: loss 191.861118\n",
      "iteration 900 / 15000: loss 185.921429\n",
      "iteration 1000 / 15000: loss 179.767107\n",
      "iteration 1100 / 15000: loss 174.160429\n",
      "iteration 1200 / 15000: loss 168.625085\n",
      "iteration 1300 / 15000: loss 163.222313\n",
      "iteration 1400 / 15000: loss 158.357525\n",
      "iteration 1500 / 15000: loss 153.050260\n",
      "iteration 1600 / 15000: loss 148.426046\n",
      "iteration 1700 / 15000: loss 143.734910\n",
      "iteration 1800 / 15000: loss 139.446600\n",
      "iteration 1900 / 15000: loss 134.807574\n",
      "iteration 2000 / 15000: loss 130.690363\n",
      "iteration 2100 / 15000: loss 126.548767\n",
      "iteration 2200 / 15000: loss 122.748888\n",
      "iteration 2300 / 15000: loss 118.865020\n",
      "iteration 2400 / 15000: loss 115.114133\n",
      "iteration 2500 / 15000: loss 111.639069\n",
      "iteration 2600 / 15000: loss 108.193598\n",
      "iteration 2700 / 15000: loss 104.689920\n",
      "iteration 2800 / 15000: loss 101.342039\n",
      "iteration 2900 / 15000: loss 98.170306\n",
      "iteration 3000 / 15000: loss 95.185665\n",
      "iteration 3100 / 15000: loss 92.330520\n",
      "iteration 3200 / 15000: loss 89.310938\n",
      "iteration 3300 / 15000: loss 86.667214\n",
      "iteration 3400 / 15000: loss 84.010885\n",
      "iteration 3500 / 15000: loss 81.229152\n",
      "iteration 3600 / 15000: loss 78.731179\n",
      "iteration 3700 / 15000: loss 76.387275\n",
      "iteration 3800 / 15000: loss 74.012318\n",
      "iteration 3900 / 15000: loss 71.701854\n",
      "iteration 4000 / 15000: loss 69.540638\n",
      "iteration 4100 / 15000: loss 67.468575\n",
      "iteration 4200 / 15000: loss 65.255302\n",
      "iteration 4300 / 15000: loss 63.287316\n",
      "iteration 4400 / 15000: loss 61.400590\n",
      "iteration 4500 / 15000: loss 59.419631\n",
      "iteration 4600 / 15000: loss 57.519321\n",
      "iteration 4700 / 15000: loss 55.828933\n",
      "iteration 4800 / 15000: loss 54.242613\n",
      "iteration 4900 / 15000: loss 52.458659\n",
      "iteration 5000 / 15000: loss 50.868262\n",
      "iteration 5100 / 15000: loss 49.470711\n",
      "iteration 5200 / 15000: loss 47.901963\n",
      "iteration 5300 / 15000: loss 46.437162\n",
      "iteration 5400 / 15000: loss 45.037422\n",
      "iteration 5500 / 15000: loss 43.674461\n",
      "iteration 5600 / 15000: loss 42.273111\n",
      "iteration 5700 / 15000: loss 41.003426\n",
      "iteration 5800 / 15000: loss 39.796469\n",
      "iteration 5900 / 15000: loss 38.669399\n",
      "iteration 6000 / 15000: loss 37.512801\n",
      "iteration 6100 / 15000: loss 36.293286\n",
      "iteration 6200 / 15000: loss 35.128783\n",
      "iteration 6300 / 15000: loss 34.148817\n",
      "iteration 6400 / 15000: loss 33.251232\n",
      "iteration 6500 / 15000: loss 32.133950\n",
      "iteration 6600 / 15000: loss 31.175304\n",
      "iteration 6700 / 15000: loss 30.326228\n",
      "iteration 6800 / 15000: loss 29.405437\n",
      "iteration 6900 / 15000: loss 28.471870\n",
      "iteration 7000 / 15000: loss 27.620243\n",
      "iteration 7100 / 15000: loss 26.852771\n",
      "iteration 7200 / 15000: loss 26.084422\n",
      "iteration 7300 / 15000: loss 25.258192\n",
      "iteration 7400 / 15000: loss 24.577137\n",
      "iteration 7500 / 15000: loss 23.806028\n",
      "iteration 7600 / 15000: loss 23.181226\n",
      "iteration 7700 / 15000: loss 22.557709\n",
      "iteration 7800 / 15000: loss 21.791336\n",
      "iteration 7900 / 15000: loss 21.154330\n",
      "iteration 8000 / 15000: loss 20.570310\n",
      "iteration 8100 / 15000: loss 20.193761\n",
      "iteration 8200 / 15000: loss 19.471930\n",
      "iteration 8300 / 15000: loss 18.914439\n",
      "iteration 8400 / 15000: loss 18.271586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8500 / 15000: loss 17.944631\n",
      "iteration 8600 / 15000: loss 17.339287\n",
      "iteration 8700 / 15000: loss 16.822783\n",
      "iteration 8800 / 15000: loss 16.370656\n",
      "iteration 8900 / 15000: loss 15.973142\n",
      "iteration 9000 / 15000: loss 15.462131\n",
      "iteration 9100 / 15000: loss 15.162753\n",
      "iteration 9200 / 15000: loss 14.659836\n",
      "iteration 9300 / 15000: loss 14.327755\n",
      "iteration 9400 / 15000: loss 13.776145\n",
      "iteration 9500 / 15000: loss 13.429341\n",
      "iteration 9600 / 15000: loss 13.133707\n",
      "iteration 9700 / 15000: loss 12.809114\n",
      "iteration 9800 / 15000: loss 12.406277\n",
      "iteration 9900 / 15000: loss 12.064785\n",
      "iteration 10000 / 15000: loss 11.706566\n",
      "iteration 10100 / 15000: loss 11.432409\n",
      "iteration 10200 / 15000: loss 11.153941\n",
      "iteration 10300 / 15000: loss 10.852551\n",
      "iteration 10400 / 15000: loss 10.609739\n",
      "iteration 10500 / 15000: loss 10.302291\n",
      "iteration 10600 / 15000: loss 10.121624\n",
      "iteration 10700 / 15000: loss 9.797875\n",
      "iteration 10800 / 15000: loss 9.505260\n",
      "iteration 10900 / 15000: loss 9.285150\n",
      "iteration 11000 / 15000: loss 9.083255\n",
      "iteration 11100 / 15000: loss 8.822320\n",
      "iteration 11200 / 15000: loss 8.554395\n",
      "iteration 11300 / 15000: loss 8.438579\n",
      "iteration 11400 / 15000: loss 8.193883\n",
      "iteration 11500 / 15000: loss 8.029167\n",
      "iteration 11600 / 15000: loss 7.782981\n",
      "iteration 11700 / 15000: loss 7.666515\n",
      "iteration 11800 / 15000: loss 7.486177\n",
      "iteration 11900 / 15000: loss 7.309323\n",
      "iteration 12000 / 15000: loss 7.148985\n",
      "iteration 12100 / 15000: loss 6.966224\n",
      "iteration 12200 / 15000: loss 6.766195\n",
      "iteration 12300 / 15000: loss 6.650723\n",
      "iteration 12400 / 15000: loss 6.454970\n",
      "iteration 12500 / 15000: loss 6.371553\n",
      "iteration 12600 / 15000: loss 6.262846\n",
      "iteration 12700 / 15000: loss 6.110128\n",
      "iteration 12800 / 15000: loss 5.973109\n",
      "iteration 12900 / 15000: loss 5.827574\n",
      "iteration 13000 / 15000: loss 5.780610\n",
      "iteration 13100 / 15000: loss 5.568684\n",
      "iteration 13200 / 15000: loss 5.409827\n",
      "iteration 13300 / 15000: loss 5.329562\n",
      "iteration 13400 / 15000: loss 5.269396\n",
      "iteration 13500 / 15000: loss 5.168348\n",
      "iteration 13600 / 15000: loss 4.979565\n",
      "iteration 13700 / 15000: loss 4.904041\n",
      "iteration 13800 / 15000: loss 4.910567\n",
      "iteration 13900 / 15000: loss 4.722187\n",
      "iteration 14000 / 15000: loss 4.709401\n",
      "iteration 14100 / 15000: loss 4.567367\n",
      "iteration 14200 / 15000: loss 4.570771\n",
      "iteration 14300 / 15000: loss 4.344942\n",
      "iteration 14400 / 15000: loss 4.380327\n",
      "iteration 14500 / 15000: loss 4.282978\n",
      "iteration 14600 / 15000: loss 4.232958\n",
      "iteration 14700 / 15000: loss 4.152119\n",
      "iteration 14800 / 15000: loss 4.125148\n",
      "iteration 14900 / 15000: loss 3.993827\n",
      "\n",
      "for r= 10000.0  and l= 1e-08\n",
      "iteration 0 / 15000: loss 313.165750\n",
      "iteration 100 / 15000: loss 300.314801\n",
      "iteration 200 / 15000: loss 288.357370\n",
      "iteration 300 / 15000: loss 276.414470\n",
      "iteration 400 / 15000: loss 265.444892\n",
      "iteration 500 / 15000: loss 255.216111\n",
      "iteration 600 / 15000: loss 244.913309\n",
      "iteration 700 / 15000: loss 235.194707\n",
      "iteration 800 / 15000: loss 225.944970\n",
      "iteration 900 / 15000: loss 216.725447\n",
      "iteration 1000 / 15000: loss 208.245302\n",
      "iteration 1100 / 15000: loss 200.017570\n",
      "iteration 1200 / 15000: loss 192.264818\n",
      "iteration 1300 / 15000: loss 184.664048\n",
      "iteration 1400 / 15000: loss 177.583356\n",
      "iteration 1500 / 15000: loss 170.840628\n",
      "iteration 1600 / 15000: loss 163.950961\n",
      "iteration 1700 / 15000: loss 157.557414\n",
      "iteration 1800 / 15000: loss 151.082187\n",
      "iteration 1900 / 15000: loss 145.530706\n",
      "iteration 2000 / 15000: loss 140.067169\n",
      "iteration 2100 / 15000: loss 134.333163\n",
      "iteration 2200 / 15000: loss 129.269685\n",
      "iteration 2300 / 15000: loss 124.028314\n",
      "iteration 2400 / 15000: loss 119.209513\n",
      "iteration 2500 / 15000: loss 114.679718\n",
      "iteration 2600 / 15000: loss 110.116437\n",
      "iteration 2700 / 15000: loss 105.823452\n",
      "iteration 2800 / 15000: loss 101.720644\n",
      "iteration 2900 / 15000: loss 97.945225\n",
      "iteration 3000 / 15000: loss 94.135157\n",
      "iteration 3100 / 15000: loss 90.362745\n",
      "iteration 3200 / 15000: loss 86.986995\n",
      "iteration 3300 / 15000: loss 83.529803\n",
      "iteration 3400 / 15000: loss 80.338235\n",
      "iteration 3500 / 15000: loss 77.328208\n",
      "iteration 3600 / 15000: loss 74.201213\n",
      "iteration 3700 / 15000: loss 71.438174\n",
      "iteration 3800 / 15000: loss 68.826540\n",
      "iteration 3900 / 15000: loss 66.091627\n",
      "iteration 4000 / 15000: loss 63.639370\n",
      "iteration 4100 / 15000: loss 61.140343\n",
      "iteration 4200 / 15000: loss 58.727643\n",
      "iteration 4300 / 15000: loss 56.550935\n",
      "iteration 4400 / 15000: loss 54.377215\n",
      "iteration 4500 / 15000: loss 52.292903\n",
      "iteration 4600 / 15000: loss 50.450601\n",
      "iteration 4700 / 15000: loss 48.377827\n",
      "iteration 4800 / 15000: loss 46.641905\n",
      "iteration 4900 / 15000: loss 44.820411\n",
      "iteration 5000 / 15000: loss 43.226899\n",
      "iteration 5100 / 15000: loss 41.628356\n",
      "iteration 5200 / 15000: loss 39.991467\n",
      "iteration 5300 / 15000: loss 38.601571\n",
      "iteration 5400 / 15000: loss 36.975894\n",
      "iteration 5500 / 15000: loss 35.617809\n",
      "iteration 5600 / 15000: loss 34.327855\n",
      "iteration 5700 / 15000: loss 33.197593\n",
      "iteration 5800 / 15000: loss 31.826903\n",
      "iteration 5900 / 15000: loss 30.664330\n",
      "iteration 6000 / 15000: loss 29.582823\n",
      "iteration 6100 / 15000: loss 28.433074\n",
      "iteration 6200 / 15000: loss 27.281508\n",
      "iteration 6300 / 15000: loss 26.342918\n",
      "iteration 6400 / 15000: loss 25.500213\n",
      "iteration 6500 / 15000: loss 24.568699\n",
      "iteration 6600 / 15000: loss 23.530125\n",
      "iteration 6700 / 15000: loss 22.782715\n",
      "iteration 6800 / 15000: loss 21.899402\n",
      "iteration 6900 / 15000: loss 21.129858\n",
      "iteration 7000 / 15000: loss 20.471751\n",
      "iteration 7100 / 15000: loss 19.681267\n",
      "iteration 7200 / 15000: loss 18.989762\n",
      "iteration 7300 / 15000: loss 18.301486\n",
      "iteration 7400 / 15000: loss 17.642979\n",
      "iteration 7500 / 15000: loss 17.016609\n",
      "iteration 7600 / 15000: loss 16.455083\n",
      "iteration 7700 / 15000: loss 15.891091\n",
      "iteration 7800 / 15000: loss 15.368824\n",
      "iteration 7900 / 15000: loss 14.769197\n",
      "iteration 8000 / 15000: loss 14.446121\n",
      "iteration 8100 / 15000: loss 13.833572\n",
      "iteration 8200 / 15000: loss 13.359454\n",
      "iteration 8300 / 15000: loss 13.011063\n",
      "iteration 8400 / 15000: loss 12.442895\n",
      "iteration 8500 / 15000: loss 12.122789\n",
      "iteration 8600 / 15000: loss 11.689088\n",
      "iteration 8700 / 15000: loss 11.269623\n",
      "iteration 8800 / 15000: loss 11.002515\n",
      "iteration 8900 / 15000: loss 10.573401\n",
      "iteration 9000 / 15000: loss 10.294524\n",
      "iteration 9100 / 15000: loss 9.838856\n",
      "iteration 9200 / 15000: loss 9.552909\n",
      "iteration 9300 / 15000: loss 9.315063\n",
      "iteration 9400 / 15000: loss 9.067312\n",
      "iteration 9500 / 15000: loss 8.717145\n",
      "iteration 9600 / 15000: loss 8.489343\n",
      "iteration 9700 / 15000: loss 8.263981\n",
      "iteration 9800 / 15000: loss 7.920447\n",
      "iteration 9900 / 15000: loss 7.787786\n",
      "iteration 10000 / 15000: loss 7.504667\n",
      "iteration 10100 / 15000: loss 7.274209\n",
      "iteration 10200 / 15000: loss 7.096653\n",
      "iteration 10300 / 15000: loss 6.806904\n",
      "iteration 10400 / 15000: loss 6.740685\n",
      "iteration 10500 / 15000: loss 6.532186\n",
      "iteration 10600 / 15000: loss 6.355381\n",
      "iteration 10700 / 15000: loss 6.086577\n",
      "iteration 10800 / 15000: loss 6.031166\n",
      "iteration 10900 / 15000: loss 5.820057\n",
      "iteration 11000 / 15000: loss 5.720994\n",
      "iteration 11100 / 15000: loss 5.563167\n",
      "iteration 11200 / 15000: loss 5.406459\n",
      "iteration 11300 / 15000: loss 5.294616\n",
      "iteration 11400 / 15000: loss 5.077988\n",
      "iteration 11500 / 15000: loss 4.970709\n",
      "iteration 11600 / 15000: loss 4.927120\n",
      "iteration 11700 / 15000: loss 4.798675\n",
      "iteration 11800 / 15000: loss 4.696437\n",
      "iteration 11900 / 15000: loss 4.548092\n",
      "iteration 12000 / 15000: loss 4.434813\n",
      "iteration 12100 / 15000: loss 4.363414\n",
      "iteration 12200 / 15000: loss 4.287328\n",
      "iteration 12300 / 15000: loss 4.222258\n",
      "iteration 12400 / 15000: loss 4.018837\n",
      "iteration 12500 / 15000: loss 4.022075\n",
      "iteration 12600 / 15000: loss 3.988827\n",
      "iteration 12700 / 15000: loss 3.838316\n",
      "iteration 12800 / 15000: loss 3.802122\n",
      "iteration 12900 / 15000: loss 3.693540\n",
      "iteration 13000 / 15000: loss 3.681916\n",
      "iteration 13100 / 15000: loss 3.636906\n",
      "iteration 13200 / 15000: loss 3.576493\n",
      "iteration 13300 / 15000: loss 3.430465\n",
      "iteration 13400 / 15000: loss 3.390503\n",
      "iteration 13500 / 15000: loss 3.360574\n",
      "iteration 13600 / 15000: loss 3.310287\n",
      "iteration 13700 / 15000: loss 3.267523\n",
      "iteration 13800 / 15000: loss 3.207946\n",
      "iteration 13900 / 15000: loss 3.187586\n",
      "iteration 14000 / 15000: loss 3.117439\n",
      "iteration 14100 / 15000: loss 3.071544\n",
      "iteration 14200 / 15000: loss 3.028220\n",
      "iteration 14300 / 15000: loss 3.000845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14400 / 15000: loss 2.982854\n",
      "iteration 14500 / 15000: loss 2.937681\n",
      "iteration 14600 / 15000: loss 2.846720\n",
      "iteration 14700 / 15000: loss 2.784041\n",
      "iteration 14800 / 15000: loss 2.837165\n",
      "iteration 14900 / 15000: loss 2.743753\n",
      "\n",
      "for r= 12500.0  and l= 1e-08\n",
      "iteration 0 / 15000: loss 394.332046\n",
      "iteration 100 / 15000: loss 374.287492\n",
      "iteration 200 / 15000: loss 355.685120\n",
      "iteration 300 / 15000: loss 338.076294\n",
      "iteration 400 / 15000: loss 321.393105\n",
      "iteration 500 / 15000: loss 304.872364\n",
      "iteration 600 / 15000: loss 290.621560\n",
      "iteration 700 / 15000: loss 276.244724\n",
      "iteration 800 / 15000: loss 262.661887\n",
      "iteration 900 / 15000: loss 249.718945\n",
      "iteration 1000 / 15000: loss 237.658580\n",
      "iteration 1100 / 15000: loss 226.107787\n",
      "iteration 1200 / 15000: loss 215.079724\n",
      "iteration 1300 / 15000: loss 204.421957\n",
      "iteration 1400 / 15000: loss 194.596575\n",
      "iteration 1500 / 15000: loss 185.298366\n",
      "iteration 1600 / 15000: loss 175.941748\n",
      "iteration 1700 / 15000: loss 167.635029\n",
      "iteration 1800 / 15000: loss 159.514947\n",
      "iteration 1900 / 15000: loss 151.693951\n",
      "iteration 2000 / 15000: loss 144.435961\n",
      "iteration 2100 / 15000: loss 137.346460\n",
      "iteration 2200 / 15000: loss 130.715383\n",
      "iteration 2300 / 15000: loss 124.454354\n",
      "iteration 2400 / 15000: loss 118.487731\n",
      "iteration 2500 / 15000: loss 112.754668\n",
      "iteration 2600 / 15000: loss 107.260298\n",
      "iteration 2700 / 15000: loss 102.152718\n",
      "iteration 2800 / 15000: loss 97.192612\n",
      "iteration 2900 / 15000: loss 92.594046\n",
      "iteration 3000 / 15000: loss 88.135636\n",
      "iteration 3100 / 15000: loss 84.027304\n",
      "iteration 3200 / 15000: loss 79.871275\n",
      "iteration 3300 / 15000: loss 76.221530\n",
      "iteration 3400 / 15000: loss 72.423136\n",
      "iteration 3500 / 15000: loss 68.980065\n",
      "iteration 3600 / 15000: loss 65.676889\n",
      "iteration 3700 / 15000: loss 62.563768\n",
      "iteration 3800 / 15000: loss 59.525030\n",
      "iteration 3900 / 15000: loss 56.752275\n",
      "iteration 4000 / 15000: loss 54.028218\n",
      "iteration 4100 / 15000: loss 51.540575\n",
      "iteration 4200 / 15000: loss 49.078228\n",
      "iteration 4300 / 15000: loss 46.876937\n",
      "iteration 4400 / 15000: loss 44.599544\n",
      "iteration 4500 / 15000: loss 42.541493\n",
      "iteration 4600 / 15000: loss 40.573984\n",
      "iteration 4700 / 15000: loss 38.691005\n",
      "iteration 4800 / 15000: loss 36.878546\n",
      "iteration 4900 / 15000: loss 35.140998\n",
      "iteration 5000 / 15000: loss 33.476709\n",
      "iteration 5100 / 15000: loss 31.994373\n",
      "iteration 5200 / 15000: loss 30.535006\n",
      "iteration 5300 / 15000: loss 29.170922\n",
      "iteration 5400 / 15000: loss 27.881160\n",
      "iteration 5500 / 15000: loss 26.500188\n",
      "iteration 5600 / 15000: loss 25.347342\n",
      "iteration 5700 / 15000: loss 24.173799\n",
      "iteration 5800 / 15000: loss 23.122476\n",
      "iteration 5900 / 15000: loss 22.080607\n",
      "iteration 6000 / 15000: loss 21.183704\n",
      "iteration 6100 / 15000: loss 20.183616\n",
      "iteration 6200 / 15000: loss 19.276392\n",
      "iteration 6300 / 15000: loss 18.462178\n",
      "iteration 6400 / 15000: loss 17.649243\n",
      "iteration 6500 / 15000: loss 16.889301\n",
      "iteration 6600 / 15000: loss 16.211542\n",
      "iteration 6700 / 15000: loss 15.504324\n",
      "iteration 6800 / 15000: loss 14.737579\n",
      "iteration 6900 / 15000: loss 14.186991\n",
      "iteration 7000 / 15000: loss 13.509576\n",
      "iteration 7100 / 15000: loss 13.009093\n",
      "iteration 7200 / 15000: loss 12.414680\n",
      "iteration 7300 / 15000: loss 11.956005\n",
      "iteration 7400 / 15000: loss 11.437530\n",
      "iteration 7500 / 15000: loss 10.980882\n",
      "iteration 7600 / 15000: loss 10.546793\n",
      "iteration 7700 / 15000: loss 10.210105\n",
      "iteration 7800 / 15000: loss 9.736258\n",
      "iteration 7900 / 15000: loss 9.416935\n",
      "iteration 8000 / 15000: loss 9.077964\n",
      "iteration 8100 / 15000: loss 8.668096\n",
      "iteration 8200 / 15000: loss 8.281150\n",
      "iteration 8300 / 15000: loss 8.083704\n",
      "iteration 8400 / 15000: loss 7.760137\n",
      "iteration 8500 / 15000: loss 7.461912\n",
      "iteration 8600 / 15000: loss 7.215321\n",
      "iteration 8700 / 15000: loss 6.883398\n",
      "iteration 8800 / 15000: loss 6.764550\n",
      "iteration 8900 / 15000: loss 6.517829\n",
      "iteration 9000 / 15000: loss 6.251647\n",
      "iteration 9100 / 15000: loss 5.993956\n",
      "iteration 9200 / 15000: loss 5.870024\n",
      "iteration 9300 / 15000: loss 5.684393\n",
      "iteration 9400 / 15000: loss 5.457428\n",
      "iteration 9500 / 15000: loss 5.351062\n",
      "iteration 9600 / 15000: loss 5.164340\n",
      "iteration 9700 / 15000: loss 4.994824\n",
      "iteration 9800 / 15000: loss 4.855299\n",
      "iteration 9900 / 15000: loss 4.674184\n",
      "iteration 10000 / 15000: loss 4.493272\n",
      "iteration 10100 / 15000: loss 4.467052\n",
      "iteration 10200 / 15000: loss 4.440542\n",
      "iteration 10300 / 15000: loss 4.163488\n",
      "iteration 10400 / 15000: loss 4.079011\n",
      "iteration 10500 / 15000: loss 3.971487\n",
      "iteration 10600 / 15000: loss 3.907359\n",
      "iteration 10700 / 15000: loss 3.818334\n",
      "iteration 10800 / 15000: loss 3.703795\n",
      "iteration 10900 / 15000: loss 3.665776\n",
      "iteration 11000 / 15000: loss 3.598582\n",
      "iteration 11100 / 15000: loss 3.448325\n",
      "iteration 11200 / 15000: loss 3.480285\n",
      "iteration 11300 / 15000: loss 3.399776\n",
      "iteration 11400 / 15000: loss 3.300636\n",
      "iteration 11500 / 15000: loss 3.359821\n",
      "iteration 11600 / 15000: loss 3.236431\n",
      "iteration 11700 / 15000: loss 3.132896\n",
      "iteration 11800 / 15000: loss 3.075711\n",
      "iteration 11900 / 15000: loss 3.022900\n",
      "iteration 12000 / 15000: loss 2.942701\n",
      "iteration 12100 / 15000: loss 2.912770\n",
      "iteration 12200 / 15000: loss 2.903118\n",
      "iteration 12300 / 15000: loss 2.817966\n",
      "iteration 12400 / 15000: loss 2.808683\n",
      "iteration 12500 / 15000: loss 2.658165\n",
      "iteration 12600 / 15000: loss 2.690198\n",
      "iteration 12700 / 15000: loss 2.687826\n",
      "iteration 12800 / 15000: loss 2.641828\n",
      "iteration 12900 / 15000: loss 2.606181\n",
      "iteration 13000 / 15000: loss 2.574043\n",
      "iteration 13100 / 15000: loss 2.572578\n",
      "iteration 13200 / 15000: loss 2.545896\n",
      "iteration 13300 / 15000: loss 2.484663\n",
      "iteration 13400 / 15000: loss 2.441943\n",
      "iteration 13500 / 15000: loss 2.428124\n",
      "iteration 13600 / 15000: loss 2.392817\n",
      "iteration 13700 / 15000: loss 2.454903\n",
      "iteration 13800 / 15000: loss 2.393294\n",
      "iteration 13900 / 15000: loss 2.409054\n",
      "iteration 14000 / 15000: loss 2.380751\n",
      "iteration 14100 / 15000: loss 2.355084\n",
      "iteration 14200 / 15000: loss 2.410930\n",
      "iteration 14300 / 15000: loss 2.300274\n",
      "iteration 14400 / 15000: loss 2.339980\n",
      "iteration 14500 / 15000: loss 2.298614\n",
      "iteration 14600 / 15000: loss 2.256307\n",
      "iteration 14700 / 15000: loss 2.241553\n",
      "iteration 14800 / 15000: loss 2.279562\n",
      "iteration 14900 / 15000: loss 2.224592\n",
      "\n",
      "for r= 15000.0  and l= 1e-08\n",
      "iteration 0 / 15000: loss 464.434278\n",
      "iteration 100 / 15000: loss 437.214032\n",
      "iteration 200 / 15000: loss 411.839285\n",
      "iteration 300 / 15000: loss 387.740826\n",
      "iteration 400 / 15000: loss 364.766327\n",
      "iteration 500 / 15000: loss 344.118822\n",
      "iteration 600 / 15000: loss 323.401401\n",
      "iteration 700 / 15000: loss 304.614833\n",
      "iteration 800 / 15000: loss 286.629959\n",
      "iteration 900 / 15000: loss 270.037887\n",
      "iteration 1000 / 15000: loss 254.591727\n",
      "iteration 1100 / 15000: loss 239.663322\n",
      "iteration 1200 / 15000: loss 226.047867\n",
      "iteration 1300 / 15000: loss 212.436298\n",
      "iteration 1400 / 15000: loss 200.352913\n",
      "iteration 1500 / 15000: loss 188.723085\n",
      "iteration 1600 / 15000: loss 177.844575\n",
      "iteration 1700 / 15000: loss 167.533927\n",
      "iteration 1800 / 15000: loss 157.857975\n",
      "iteration 1900 / 15000: loss 148.670291\n",
      "iteration 2000 / 15000: loss 140.178595\n",
      "iteration 2100 / 15000: loss 132.102321\n",
      "iteration 2200 / 15000: loss 124.327900\n",
      "iteration 2300 / 15000: loss 117.175274\n",
      "iteration 2400 / 15000: loss 110.555566\n",
      "iteration 2500 / 15000: loss 104.049328\n",
      "iteration 2600 / 15000: loss 98.128951\n",
      "iteration 2700 / 15000: loss 92.564740\n",
      "iteration 2800 / 15000: loss 87.196893\n",
      "iteration 2900 / 15000: loss 82.292085\n",
      "iteration 3000 / 15000: loss 77.601816\n",
      "iteration 3100 / 15000: loss 73.209137\n",
      "iteration 3200 / 15000: loss 68.993956\n",
      "iteration 3300 / 15000: loss 65.063920\n",
      "iteration 3400 / 15000: loss 61.321191\n",
      "iteration 3500 / 15000: loss 57.929647\n",
      "iteration 3600 / 15000: loss 54.643872\n",
      "iteration 3700 / 15000: loss 51.521535\n",
      "iteration 3800 / 15000: loss 48.586038\n",
      "iteration 3900 / 15000: loss 46.051349\n",
      "iteration 4000 / 15000: loss 43.388895\n",
      "iteration 4100 / 15000: loss 40.948915\n",
      "iteration 4200 / 15000: loss 38.694761\n",
      "iteration 4300 / 15000: loss 36.527352\n",
      "iteration 4400 / 15000: loss 34.472574\n",
      "iteration 4500 / 15000: loss 32.574398\n",
      "iteration 4600 / 15000: loss 30.811582\n",
      "iteration 4700 / 15000: loss 29.100380\n",
      "iteration 4800 / 15000: loss 27.583493\n",
      "iteration 4900 / 15000: loss 26.100495\n",
      "iteration 5000 / 15000: loss 24.578590\n",
      "iteration 5100 / 15000: loss 23.221633\n",
      "iteration 5200 / 15000: loss 22.134428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5300 / 15000: loss 20.935840\n",
      "iteration 5400 / 15000: loss 19.828746\n",
      "iteration 5500 / 15000: loss 18.791723\n",
      "iteration 5600 / 15000: loss 17.847391\n",
      "iteration 5700 / 15000: loss 16.945846\n",
      "iteration 5800 / 15000: loss 16.057967\n",
      "iteration 5900 / 15000: loss 15.242687\n",
      "iteration 6000 / 15000: loss 14.465740\n",
      "iteration 6100 / 15000: loss 13.819959\n",
      "iteration 6200 / 15000: loss 13.079327\n",
      "iteration 6300 / 15000: loss 12.371308\n",
      "iteration 6400 / 15000: loss 11.808624\n",
      "iteration 6500 / 15000: loss 11.238529\n",
      "iteration 6600 / 15000: loss 10.703460\n",
      "iteration 6700 / 15000: loss 10.146521\n",
      "iteration 6800 / 15000: loss 9.656967\n",
      "iteration 6900 / 15000: loss 9.239845\n",
      "iteration 7000 / 15000: loss 8.796051\n",
      "iteration 7100 / 15000: loss 8.430696\n",
      "iteration 7200 / 15000: loss 8.060969\n",
      "iteration 7300 / 15000: loss 7.733655\n",
      "iteration 7400 / 15000: loss 7.350663\n",
      "iteration 7500 / 15000: loss 7.098541\n",
      "iteration 7600 / 15000: loss 6.804764\n",
      "iteration 7700 / 15000: loss 6.541239\n",
      "iteration 7800 / 15000: loss 6.225330\n",
      "iteration 7900 / 15000: loss 5.960839\n",
      "iteration 8000 / 15000: loss 5.755454\n",
      "iteration 8100 / 15000: loss 5.531162\n",
      "iteration 8200 / 15000: loss 5.296622\n",
      "iteration 8300 / 15000: loss 5.144867\n",
      "iteration 8400 / 15000: loss 4.915544\n",
      "iteration 8500 / 15000: loss 4.728686\n",
      "iteration 8600 / 15000: loss 4.636433\n",
      "iteration 8700 / 15000: loss 4.467290\n",
      "iteration 8800 / 15000: loss 4.307754\n",
      "iteration 8900 / 15000: loss 4.187353\n",
      "iteration 9000 / 15000: loss 4.069206\n",
      "iteration 9100 / 15000: loss 3.986988\n",
      "iteration 9200 / 15000: loss 3.835936\n",
      "iteration 9300 / 15000: loss 3.709354\n",
      "iteration 9400 / 15000: loss 3.655674\n",
      "iteration 9500 / 15000: loss 3.583929\n",
      "iteration 9600 / 15000: loss 3.500587\n",
      "iteration 9700 / 15000: loss 3.342291\n",
      "iteration 9800 / 15000: loss 3.329586\n",
      "iteration 9900 / 15000: loss 3.305460\n",
      "iteration 10000 / 15000: loss 3.112757\n",
      "iteration 10100 / 15000: loss 3.105666\n",
      "iteration 10200 / 15000: loss 3.054123\n",
      "iteration 10300 / 15000: loss 2.950589\n",
      "iteration 10400 / 15000: loss 2.997535\n",
      "iteration 10500 / 15000: loss 2.930020\n",
      "iteration 10600 / 15000: loss 2.824320\n",
      "iteration 10700 / 15000: loss 2.746625\n",
      "iteration 10800 / 15000: loss 2.685830\n",
      "iteration 10900 / 15000: loss 2.650536\n",
      "iteration 11000 / 15000: loss 2.697508\n",
      "iteration 11100 / 15000: loss 2.574872\n",
      "iteration 11200 / 15000: loss 2.592437\n",
      "iteration 11300 / 15000: loss 2.604773\n",
      "iteration 11400 / 15000: loss 2.473750\n",
      "iteration 11500 / 15000: loss 2.544744\n",
      "iteration 11600 / 15000: loss 2.410937\n",
      "iteration 11700 / 15000: loss 2.508319\n",
      "iteration 11800 / 15000: loss 2.444241\n",
      "iteration 11900 / 15000: loss 2.476525\n",
      "iteration 12000 / 15000: loss 2.340657\n",
      "iteration 12100 / 15000: loss 2.380050\n",
      "iteration 12200 / 15000: loss 2.342651\n",
      "iteration 12300 / 15000: loss 2.337454\n",
      "iteration 12400 / 15000: loss 2.303111\n",
      "iteration 12500 / 15000: loss 2.241064\n",
      "iteration 12600 / 15000: loss 2.189172\n",
      "iteration 12700 / 15000: loss 2.270810\n",
      "iteration 12800 / 15000: loss 2.268382\n",
      "iteration 12900 / 15000: loss 2.211294\n",
      "iteration 13000 / 15000: loss 2.223507\n",
      "iteration 13100 / 15000: loss 2.246430\n",
      "iteration 13200 / 15000: loss 2.139642\n",
      "iteration 13300 / 15000: loss 2.171540\n",
      "iteration 13400 / 15000: loss 2.261784\n",
      "iteration 13500 / 15000: loss 2.179634\n",
      "iteration 13600 / 15000: loss 2.144995\n",
      "iteration 13700 / 15000: loss 2.162182\n",
      "iteration 13800 / 15000: loss 2.161091\n",
      "iteration 13900 / 15000: loss 2.172239\n",
      "iteration 14000 / 15000: loss 2.142182\n",
      "iteration 14100 / 15000: loss 2.175410\n",
      "iteration 14200 / 15000: loss 2.110350\n",
      "iteration 14300 / 15000: loss 2.135335\n",
      "iteration 14400 / 15000: loss 2.157953\n",
      "iteration 14500 / 15000: loss 2.080191\n",
      "iteration 14600 / 15000: loss 2.095400\n",
      "iteration 14700 / 15000: loss 2.099687\n",
      "iteration 14800 / 15000: loss 2.098189\n",
      "iteration 14900 / 15000: loss 2.131033\n",
      "\n",
      "for r= 20000.0  and l= 1e-08\n",
      "iteration 0 / 15000: loss 625.337927\n",
      "iteration 100 / 15000: loss 577.028631\n",
      "iteration 200 / 15000: loss 532.790175\n",
      "iteration 300 / 15000: loss 491.658097\n",
      "iteration 400 / 15000: loss 453.585157\n",
      "iteration 500 / 15000: loss 418.605848\n",
      "iteration 600 / 15000: loss 386.337818\n",
      "iteration 700 / 15000: loss 356.710141\n",
      "iteration 800 / 15000: loss 329.391061\n",
      "iteration 900 / 15000: loss 304.076652\n",
      "iteration 1000 / 15000: loss 280.760679\n",
      "iteration 1100 / 15000: loss 259.202960\n",
      "iteration 1200 / 15000: loss 239.525112\n",
      "iteration 1300 / 15000: loss 221.014349\n",
      "iteration 1400 / 15000: loss 204.166863\n",
      "iteration 1500 / 15000: loss 188.639199\n",
      "iteration 1600 / 15000: loss 174.120554\n",
      "iteration 1700 / 15000: loss 161.025190\n",
      "iteration 1800 / 15000: loss 148.678975\n",
      "iteration 1900 / 15000: loss 137.402383\n",
      "iteration 2000 / 15000: loss 126.876324\n",
      "iteration 2100 / 15000: loss 117.223907\n",
      "iteration 2200 / 15000: loss 108.437534\n",
      "iteration 2300 / 15000: loss 100.125017\n",
      "iteration 2400 / 15000: loss 92.501135\n",
      "iteration 2500 / 15000: loss 85.664771\n",
      "iteration 2600 / 15000: loss 79.224976\n",
      "iteration 2700 / 15000: loss 73.094902\n",
      "iteration 2800 / 15000: loss 67.738532\n",
      "iteration 2900 / 15000: loss 62.700702\n",
      "iteration 3000 / 15000: loss 58.087670\n",
      "iteration 3100 / 15000: loss 53.671978\n",
      "iteration 3200 / 15000: loss 49.648260\n",
      "iteration 3300 / 15000: loss 46.108803\n",
      "iteration 3400 / 15000: loss 42.578623\n",
      "iteration 3500 / 15000: loss 39.442899\n",
      "iteration 3600 / 15000: loss 36.556782\n",
      "iteration 3700 / 15000: loss 33.891126\n",
      "iteration 3800 / 15000: loss 31.499096\n",
      "iteration 3900 / 15000: loss 29.223369\n",
      "iteration 4000 / 15000: loss 27.214763\n",
      "iteration 4100 / 15000: loss 25.226132\n",
      "iteration 4200 / 15000: loss 23.455334\n",
      "iteration 4300 / 15000: loss 21.769569\n",
      "iteration 4400 / 15000: loss 20.247933\n",
      "iteration 4500 / 15000: loss 18.834336\n",
      "iteration 4600 / 15000: loss 17.589868\n",
      "iteration 4700 / 15000: loss 16.343760\n",
      "iteration 4800 / 15000: loss 15.227380\n",
      "iteration 4900 / 15000: loss 14.279597\n",
      "iteration 5000 / 15000: loss 13.199260\n",
      "iteration 5100 / 15000: loss 12.441720\n",
      "iteration 5200 / 15000: loss 11.618636\n",
      "iteration 5300 / 15000: loss 10.837324\n",
      "iteration 5400 / 15000: loss 10.209913\n",
      "iteration 5500 / 15000: loss 9.554010\n",
      "iteration 5600 / 15000: loss 8.952193\n",
      "iteration 5700 / 15000: loss 8.431114\n",
      "iteration 5800 / 15000: loss 7.988977\n",
      "iteration 5900 / 15000: loss 7.601123\n",
      "iteration 6000 / 15000: loss 7.147545\n",
      "iteration 6100 / 15000: loss 6.725595\n",
      "iteration 6200 / 15000: loss 6.337332\n",
      "iteration 6300 / 15000: loss 5.988524\n",
      "iteration 6400 / 15000: loss 5.722574\n",
      "iteration 6500 / 15000: loss 5.425928\n",
      "iteration 6600 / 15000: loss 5.151647\n",
      "iteration 6700 / 15000: loss 4.845712\n",
      "iteration 6800 / 15000: loss 4.707725\n",
      "iteration 6900 / 15000: loss 4.531340\n",
      "iteration 7000 / 15000: loss 4.287971\n",
      "iteration 7100 / 15000: loss 4.115593\n",
      "iteration 7200 / 15000: loss 3.934938\n",
      "iteration 7300 / 15000: loss 3.837110\n",
      "iteration 7400 / 15000: loss 3.670519\n",
      "iteration 7500 / 15000: loss 3.663444\n",
      "iteration 7600 / 15000: loss 3.480746\n",
      "iteration 7700 / 15000: loss 3.346582\n",
      "iteration 7800 / 15000: loss 3.246166\n",
      "iteration 7900 / 15000: loss 3.118950\n",
      "iteration 8000 / 15000: loss 3.086952\n",
      "iteration 8100 / 15000: loss 3.006687\n",
      "iteration 8200 / 15000: loss 2.935108\n",
      "iteration 8300 / 15000: loss 2.848812\n",
      "iteration 8400 / 15000: loss 2.747870\n",
      "iteration 8500 / 15000: loss 2.697065\n",
      "iteration 8600 / 15000: loss 2.703672\n",
      "iteration 8700 / 15000: loss 2.611835\n",
      "iteration 8800 / 15000: loss 2.558160\n",
      "iteration 8900 / 15000: loss 2.537739\n",
      "iteration 9000 / 15000: loss 2.463242\n",
      "iteration 9100 / 15000: loss 2.500198\n",
      "iteration 9200 / 15000: loss 2.481639\n",
      "iteration 9300 / 15000: loss 2.400978\n",
      "iteration 9400 / 15000: loss 2.380339\n",
      "iteration 9500 / 15000: loss 2.354570\n",
      "iteration 9600 / 15000: loss 2.363397\n",
      "iteration 9700 / 15000: loss 2.320552\n",
      "iteration 9800 / 15000: loss 2.302824\n",
      "iteration 9900 / 15000: loss 2.330697\n",
      "iteration 10000 / 15000: loss 2.177361\n",
      "iteration 10100 / 15000: loss 2.323419\n",
      "iteration 10200 / 15000: loss 2.253963\n",
      "iteration 10300 / 15000: loss 2.187141\n",
      "iteration 10400 / 15000: loss 2.202049\n",
      "iteration 10500 / 15000: loss 2.192027\n",
      "iteration 10600 / 15000: loss 2.212352\n",
      "iteration 10700 / 15000: loss 2.174599\n",
      "iteration 10800 / 15000: loss 2.196747\n",
      "iteration 10900 / 15000: loss 2.163892\n",
      "iteration 11000 / 15000: loss 2.171572\n",
      "iteration 11100 / 15000: loss 2.152005\n",
      "iteration 11200 / 15000: loss 2.151386\n",
      "iteration 11300 / 15000: loss 2.110701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11400 / 15000: loss 2.103973\n",
      "iteration 11500 / 15000: loss 2.160950\n",
      "iteration 11600 / 15000: loss 2.057186\n",
      "iteration 11700 / 15000: loss 2.096889\n",
      "iteration 11800 / 15000: loss 2.068514\n",
      "iteration 11900 / 15000: loss 2.097960\n",
      "iteration 12000 / 15000: loss 2.130682\n",
      "iteration 12100 / 15000: loss 2.083882\n",
      "iteration 12200 / 15000: loss 2.001674\n",
      "iteration 12300 / 15000: loss 2.171168\n",
      "iteration 12400 / 15000: loss 2.094818\n",
      "iteration 12500 / 15000: loss 2.124340\n",
      "iteration 12600 / 15000: loss 2.017529\n",
      "iteration 12700 / 15000: loss 2.101124\n",
      "iteration 12800 / 15000: loss 2.062949\n",
      "iteration 12900 / 15000: loss 2.074625\n",
      "iteration 13000 / 15000: loss 2.097737\n",
      "iteration 13100 / 15000: loss 2.136568\n",
      "iteration 13200 / 15000: loss 2.116429\n",
      "iteration 13300 / 15000: loss 2.120943\n",
      "iteration 13400 / 15000: loss 2.006867\n",
      "iteration 13500 / 15000: loss 2.120566\n",
      "iteration 13600 / 15000: loss 2.058775\n",
      "iteration 13700 / 15000: loss 2.067711\n",
      "iteration 13800 / 15000: loss 2.150959\n",
      "iteration 13900 / 15000: loss 2.086069\n",
      "iteration 14000 / 15000: loss 2.108029\n",
      "iteration 14100 / 15000: loss 2.088264\n",
      "iteration 14200 / 15000: loss 2.110054\n",
      "iteration 14300 / 15000: loss 2.045169\n",
      "iteration 14400 / 15000: loss 2.088762\n",
      "iteration 14500 / 15000: loss 2.046881\n",
      "iteration 14600 / 15000: loss 2.116824\n",
      "iteration 14700 / 15000: loss 2.067926\n",
      "iteration 14800 / 15000: loss 2.093621\n",
      "iteration 14900 / 15000: loss 2.102354\n",
      "\n",
      "for r= 8000.0  and l= 1.5e-08\n",
      "iteration 0 / 15000: loss 250.478603\n",
      "iteration 100 / 15000: loss 237.866440\n",
      "iteration 200 / 15000: loss 226.770672\n",
      "iteration 300 / 15000: loss 215.868446\n",
      "iteration 400 / 15000: loss 206.080250\n",
      "iteration 500 / 15000: loss 195.895047\n",
      "iteration 600 / 15000: loss 186.551416\n",
      "iteration 700 / 15000: loss 177.656171\n",
      "iteration 800 / 15000: loss 169.481654\n",
      "iteration 900 / 15000: loss 161.469120\n",
      "iteration 1000 / 15000: loss 154.144986\n",
      "iteration 1100 / 15000: loss 146.604569\n",
      "iteration 1200 / 15000: loss 139.956620\n",
      "iteration 1300 / 15000: loss 133.450925\n",
      "iteration 1400 / 15000: loss 127.212752\n",
      "iteration 1500 / 15000: loss 121.097875\n",
      "iteration 1600 / 15000: loss 115.969352\n",
      "iteration 1700 / 15000: loss 110.346981\n",
      "iteration 1800 / 15000: loss 104.979203\n",
      "iteration 1900 / 15000: loss 100.077627\n",
      "iteration 2000 / 15000: loss 95.734448\n",
      "iteration 2100 / 15000: loss 91.346163\n",
      "iteration 2200 / 15000: loss 87.014963\n",
      "iteration 2300 / 15000: loss 83.075410\n",
      "iteration 2400 / 15000: loss 79.112108\n",
      "iteration 2500 / 15000: loss 75.325693\n",
      "iteration 2600 / 15000: loss 72.037855\n",
      "iteration 2700 / 15000: loss 68.723277\n",
      "iteration 2800 / 15000: loss 65.499755\n",
      "iteration 2900 / 15000: loss 62.565847\n",
      "iteration 3000 / 15000: loss 59.651201\n",
      "iteration 3100 / 15000: loss 56.989798\n",
      "iteration 3200 / 15000: loss 54.409370\n",
      "iteration 3300 / 15000: loss 51.848732\n",
      "iteration 3400 / 15000: loss 49.520582\n",
      "iteration 3500 / 15000: loss 47.212332\n",
      "iteration 3600 / 15000: loss 45.098517\n",
      "iteration 3700 / 15000: loss 43.221578\n",
      "iteration 3800 / 15000: loss 41.138293\n",
      "iteration 3900 / 15000: loss 39.380339\n",
      "iteration 4000 / 15000: loss 37.525374\n",
      "iteration 4100 / 15000: loss 35.816780\n",
      "iteration 4200 / 15000: loss 34.344575\n",
      "iteration 4300 / 15000: loss 32.755244\n",
      "iteration 4400 / 15000: loss 31.314682\n",
      "iteration 4500 / 15000: loss 29.950560\n",
      "iteration 4600 / 15000: loss 28.641162\n",
      "iteration 4700 / 15000: loss 27.492681\n",
      "iteration 4800 / 15000: loss 26.204443\n",
      "iteration 4900 / 15000: loss 25.013445\n",
      "iteration 5000 / 15000: loss 23.914881\n",
      "iteration 5100 / 15000: loss 22.874780\n",
      "iteration 5200 / 15000: loss 21.892163\n",
      "iteration 5300 / 15000: loss 20.981114\n",
      "iteration 5400 / 15000: loss 20.047256\n",
      "iteration 5500 / 15000: loss 19.345931\n",
      "iteration 5600 / 15000: loss 18.382874\n",
      "iteration 5700 / 15000: loss 17.665328\n",
      "iteration 5800 / 15000: loss 16.859035\n",
      "iteration 5900 / 15000: loss 16.178282\n",
      "iteration 6000 / 15000: loss 15.589026\n",
      "iteration 6100 / 15000: loss 14.975397\n",
      "iteration 6200 / 15000: loss 14.348598\n",
      "iteration 6300 / 15000: loss 13.726874\n",
      "iteration 6400 / 15000: loss 13.185118\n",
      "iteration 6500 / 15000: loss 12.491676\n",
      "iteration 6600 / 15000: loss 12.055209\n",
      "iteration 6700 / 15000: loss 11.584886\n",
      "iteration 6800 / 15000: loss 11.206967\n",
      "iteration 6900 / 15000: loss 10.786206\n",
      "iteration 7000 / 15000: loss 10.326798\n",
      "iteration 7100 / 15000: loss 9.859393\n",
      "iteration 7200 / 15000: loss 9.660564\n",
      "iteration 7300 / 15000: loss 9.280457\n",
      "iteration 7400 / 15000: loss 8.875771\n",
      "iteration 7500 / 15000: loss 8.561968\n",
      "iteration 7600 / 15000: loss 8.182703\n",
      "iteration 7700 / 15000: loss 7.984575\n",
      "iteration 7800 / 15000: loss 7.598101\n",
      "iteration 7900 / 15000: loss 7.347543\n",
      "iteration 8000 / 15000: loss 7.191148\n",
      "iteration 8100 / 15000: loss 6.936212\n",
      "iteration 8200 / 15000: loss 6.623649\n",
      "iteration 8300 / 15000: loss 6.411087\n",
      "iteration 8400 / 15000: loss 6.144059\n",
      "iteration 8500 / 15000: loss 5.963970\n",
      "iteration 8600 / 15000: loss 5.777241\n",
      "iteration 8700 / 15000: loss 5.574580\n",
      "iteration 8800 / 15000: loss 5.485609\n",
      "iteration 8900 / 15000: loss 5.300155\n",
      "iteration 9000 / 15000: loss 5.153329\n",
      "iteration 9100 / 15000: loss 5.060446\n",
      "iteration 9200 / 15000: loss 4.931171\n",
      "iteration 9300 / 15000: loss 4.775981\n",
      "iteration 9400 / 15000: loss 4.653818\n",
      "iteration 9500 / 15000: loss 4.550941\n",
      "iteration 9600 / 15000: loss 4.366281\n",
      "iteration 9700 / 15000: loss 4.221529\n",
      "iteration 9800 / 15000: loss 4.101083\n",
      "iteration 9900 / 15000: loss 4.030593\n",
      "iteration 10000 / 15000: loss 3.973910\n",
      "iteration 10100 / 15000: loss 3.835309\n",
      "iteration 10200 / 15000: loss 3.851631\n",
      "iteration 10300 / 15000: loss 3.646922\n",
      "iteration 10400 / 15000: loss 3.600448\n",
      "iteration 10500 / 15000: loss 3.564147\n",
      "iteration 10600 / 15000: loss 3.497024\n",
      "iteration 10700 / 15000: loss 3.361364\n",
      "iteration 10800 / 15000: loss 3.225604\n",
      "iteration 10900 / 15000: loss 3.228665\n",
      "iteration 11000 / 15000: loss 3.178624\n",
      "iteration 11100 / 15000: loss 3.065586\n",
      "iteration 11200 / 15000: loss 3.063474\n",
      "iteration 11300 / 15000: loss 2.982739\n",
      "iteration 11400 / 15000: loss 3.009958\n",
      "iteration 11500 / 15000: loss 2.939520\n",
      "iteration 11600 / 15000: loss 2.862550\n",
      "iteration 11700 / 15000: loss 2.920695\n",
      "iteration 11800 / 15000: loss 2.745570\n",
      "iteration 11900 / 15000: loss 2.735433\n",
      "iteration 12000 / 15000: loss 2.726719\n",
      "iteration 12100 / 15000: loss 2.674000\n",
      "iteration 12200 / 15000: loss 2.684098\n",
      "iteration 12300 / 15000: loss 2.631271\n",
      "iteration 12400 / 15000: loss 2.588037\n",
      "iteration 12500 / 15000: loss 2.459262\n",
      "iteration 12600 / 15000: loss 2.527137\n",
      "iteration 12700 / 15000: loss 2.448924\n",
      "iteration 12800 / 15000: loss 2.450712\n",
      "iteration 12900 / 15000: loss 2.577862\n",
      "iteration 13000 / 15000: loss 2.449498\n",
      "iteration 13100 / 15000: loss 2.389340\n",
      "iteration 13200 / 15000: loss 2.333024\n",
      "iteration 13300 / 15000: loss 2.335055\n",
      "iteration 13400 / 15000: loss 2.260307\n",
      "iteration 13500 / 15000: loss 2.354920\n",
      "iteration 13600 / 15000: loss 2.247901\n",
      "iteration 13700 / 15000: loss 2.303059\n",
      "iteration 13800 / 15000: loss 2.364551\n",
      "iteration 13900 / 15000: loss 2.288305\n",
      "iteration 14000 / 15000: loss 2.314062\n",
      "iteration 14100 / 15000: loss 2.210485\n",
      "iteration 14200 / 15000: loss 2.292163\n",
      "iteration 14300 / 15000: loss 2.258614\n",
      "iteration 14400 / 15000: loss 2.209063\n",
      "iteration 14500 / 15000: loss 2.263049\n",
      "iteration 14600 / 15000: loss 2.195954\n",
      "iteration 14700 / 15000: loss 2.186110\n",
      "iteration 14800 / 15000: loss 2.175713\n",
      "iteration 14900 / 15000: loss 2.175215\n",
      "\n",
      "for r= 10000.0  and l= 1.5e-08\n",
      "iteration 0 / 15000: loss 314.165825\n",
      "iteration 100 / 15000: loss 295.901257\n",
      "iteration 200 / 15000: loss 278.271506\n",
      "iteration 300 / 15000: loss 261.589999\n",
      "iteration 400 / 15000: loss 246.540197\n",
      "iteration 500 / 15000: loss 232.028559\n",
      "iteration 600 / 15000: loss 218.873717\n",
      "iteration 700 / 15000: loss 205.631903\n",
      "iteration 800 / 15000: loss 193.837823\n",
      "iteration 900 / 15000: loss 182.542929\n",
      "iteration 1000 / 15000: loss 171.917307\n",
      "iteration 1100 / 15000: loss 161.954379\n",
      "iteration 1200 / 15000: loss 152.650312\n",
      "iteration 1300 / 15000: loss 144.120239\n",
      "iteration 1400 / 15000: loss 135.399238\n",
      "iteration 1500 / 15000: loss 127.649073\n",
      "iteration 1600 / 15000: loss 120.341046\n",
      "iteration 1700 / 15000: loss 113.341832\n",
      "iteration 1800 / 15000: loss 106.742276\n",
      "iteration 1900 / 15000: loss 100.545165\n",
      "iteration 2000 / 15000: loss 95.054921\n",
      "iteration 2100 / 15000: loss 89.298739\n",
      "iteration 2200 / 15000: loss 84.294614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2300 / 15000: loss 79.327010\n",
      "iteration 2400 / 15000: loss 74.984616\n",
      "iteration 2500 / 15000: loss 70.537571\n",
      "iteration 2600 / 15000: loss 66.638225\n",
      "iteration 2700 / 15000: loss 62.839579\n",
      "iteration 2800 / 15000: loss 59.301317\n",
      "iteration 2900 / 15000: loss 55.958578\n",
      "iteration 3000 / 15000: loss 52.726221\n",
      "iteration 3100 / 15000: loss 49.646606\n",
      "iteration 3200 / 15000: loss 46.942334\n",
      "iteration 3300 / 15000: loss 44.434263\n",
      "iteration 3400 / 15000: loss 41.815513\n",
      "iteration 3500 / 15000: loss 39.616235\n",
      "iteration 3600 / 15000: loss 37.247963\n",
      "iteration 3700 / 15000: loss 35.289774\n",
      "iteration 3800 / 15000: loss 33.320278\n",
      "iteration 3900 / 15000: loss 31.443478\n",
      "iteration 4000 / 15000: loss 29.753177\n",
      "iteration 4100 / 15000: loss 28.143571\n",
      "iteration 4200 / 15000: loss 26.602537\n",
      "iteration 4300 / 15000: loss 25.139565\n",
      "iteration 4400 / 15000: loss 23.788585\n",
      "iteration 4500 / 15000: loss 22.614474\n",
      "iteration 4600 / 15000: loss 21.313683\n",
      "iteration 4700 / 15000: loss 20.237147\n",
      "iteration 4800 / 15000: loss 19.155505\n",
      "iteration 4900 / 15000: loss 18.081403\n",
      "iteration 5000 / 15000: loss 17.177465\n",
      "iteration 5100 / 15000: loss 16.289714\n",
      "iteration 5200 / 15000: loss 15.373539\n",
      "iteration 5300 / 15000: loss 14.682757\n",
      "iteration 5400 / 15000: loss 13.960808\n",
      "iteration 5500 / 15000: loss 13.213741\n",
      "iteration 5600 / 15000: loss 12.560436\n",
      "iteration 5700 / 15000: loss 11.975654\n",
      "iteration 5800 / 15000: loss 11.430413\n",
      "iteration 5900 / 15000: loss 10.898112\n",
      "iteration 6000 / 15000: loss 10.388868\n",
      "iteration 6100 / 15000: loss 9.881755\n",
      "iteration 6200 / 15000: loss 9.343556\n",
      "iteration 6300 / 15000: loss 8.990375\n",
      "iteration 6400 / 15000: loss 8.523482\n",
      "iteration 6500 / 15000: loss 8.249296\n",
      "iteration 6600 / 15000: loss 7.814714\n",
      "iteration 6700 / 15000: loss 7.419227\n",
      "iteration 6800 / 15000: loss 7.066166\n",
      "iteration 6900 / 15000: loss 6.847208\n",
      "iteration 7000 / 15000: loss 6.495910\n",
      "iteration 7100 / 15000: loss 6.297139\n",
      "iteration 7200 / 15000: loss 6.108756\n",
      "iteration 7300 / 15000: loss 5.808752\n",
      "iteration 7400 / 15000: loss 5.621823\n",
      "iteration 7500 / 15000: loss 5.348122\n",
      "iteration 7600 / 15000: loss 5.195283\n",
      "iteration 7700 / 15000: loss 5.019266\n",
      "iteration 7800 / 15000: loss 4.868395\n",
      "iteration 7900 / 15000: loss 4.690144\n",
      "iteration 8000 / 15000: loss 4.469936\n",
      "iteration 8100 / 15000: loss 4.345519\n",
      "iteration 8200 / 15000: loss 4.253367\n",
      "iteration 8300 / 15000: loss 4.032695\n",
      "iteration 8400 / 15000: loss 3.922886\n",
      "iteration 8500 / 15000: loss 3.858879\n",
      "iteration 8600 / 15000: loss 3.723320\n",
      "iteration 8700 / 15000: loss 3.711927\n",
      "iteration 8800 / 15000: loss 3.476131\n",
      "iteration 8900 / 15000: loss 3.487034\n",
      "iteration 9000 / 15000: loss 3.351192\n",
      "iteration 9100 / 15000: loss 3.266129\n",
      "iteration 9200 / 15000: loss 3.151166\n",
      "iteration 9300 / 15000: loss 3.155632\n",
      "iteration 9400 / 15000: loss 3.101703\n",
      "iteration 9500 / 15000: loss 2.966904\n",
      "iteration 9600 / 15000: loss 2.971415\n",
      "iteration 9700 / 15000: loss 2.860854\n",
      "iteration 9800 / 15000: loss 2.876478\n",
      "iteration 9900 / 15000: loss 2.768291\n",
      "iteration 10000 / 15000: loss 2.737501\n",
      "iteration 10100 / 15000: loss 2.750731\n",
      "iteration 10200 / 15000: loss 2.738289\n",
      "iteration 10300 / 15000: loss 2.637456\n",
      "iteration 10400 / 15000: loss 2.634893\n",
      "iteration 10500 / 15000: loss 2.623992\n",
      "iteration 10600 / 15000: loss 2.530257\n",
      "iteration 10700 / 15000: loss 2.418090\n",
      "iteration 10800 / 15000: loss 2.458183\n",
      "iteration 10900 / 15000: loss 2.448232\n",
      "iteration 11000 / 15000: loss 2.399689\n",
      "iteration 11100 / 15000: loss 2.355997\n",
      "iteration 11200 / 15000: loss 2.401312\n",
      "iteration 11300 / 15000: loss 2.376539\n",
      "iteration 11400 / 15000: loss 2.278997\n",
      "iteration 11500 / 15000: loss 2.301082\n",
      "iteration 11600 / 15000: loss 2.249947\n",
      "iteration 11700 / 15000: loss 2.269837\n",
      "iteration 11800 / 15000: loss 2.261957\n",
      "iteration 11900 / 15000: loss 2.308368\n",
      "iteration 12000 / 15000: loss 2.256147\n",
      "iteration 12100 / 15000: loss 2.185467\n",
      "iteration 12200 / 15000: loss 2.235011\n",
      "iteration 12300 / 15000: loss 2.132231\n",
      "iteration 12400 / 15000: loss 2.173414\n",
      "iteration 12500 / 15000: loss 2.165804\n",
      "iteration 12600 / 15000: loss 2.134504\n",
      "iteration 12700 / 15000: loss 2.200442\n",
      "iteration 12800 / 15000: loss 2.227630\n",
      "iteration 12900 / 15000: loss 2.138771\n",
      "iteration 13000 / 15000: loss 2.125372\n",
      "iteration 13100 / 15000: loss 2.093130\n",
      "iteration 13200 / 15000: loss 2.046528\n",
      "iteration 13300 / 15000: loss 2.046213\n",
      "iteration 13400 / 15000: loss 2.166155\n",
      "iteration 13500 / 15000: loss 2.053077\n",
      "iteration 13600 / 15000: loss 1.999758\n",
      "iteration 13700 / 15000: loss 2.033237\n",
      "iteration 13800 / 15000: loss 2.045208\n",
      "iteration 13900 / 15000: loss 2.172097\n",
      "iteration 14000 / 15000: loss 2.065844\n",
      "iteration 14100 / 15000: loss 2.041837\n",
      "iteration 14200 / 15000: loss 2.098366\n",
      "iteration 14300 / 15000: loss 2.127642\n",
      "iteration 14400 / 15000: loss 2.061725\n",
      "iteration 14500 / 15000: loss 2.087232\n",
      "iteration 14600 / 15000: loss 2.032137\n",
      "iteration 14700 / 15000: loss 2.046124\n",
      "iteration 14800 / 15000: loss 2.002765\n",
      "iteration 14900 / 15000: loss 2.157104\n",
      "\n",
      "for r= 12500.0  and l= 1.5e-08\n",
      "iteration 0 / 15000: loss 392.137397\n",
      "iteration 100 / 15000: loss 363.536941\n",
      "iteration 200 / 15000: loss 337.030987\n",
      "iteration 300 / 15000: loss 312.616926\n",
      "iteration 400 / 15000: loss 289.625096\n",
      "iteration 500 / 15000: loss 268.432089\n",
      "iteration 600 / 15000: loss 248.860759\n",
      "iteration 700 / 15000: loss 231.093415\n",
      "iteration 800 / 15000: loss 214.104936\n",
      "iteration 900 / 15000: loss 198.977411\n",
      "iteration 1000 / 15000: loss 184.669660\n",
      "iteration 1100 / 15000: loss 171.321654\n",
      "iteration 1200 / 15000: loss 158.668084\n",
      "iteration 1300 / 15000: loss 147.614776\n",
      "iteration 1400 / 15000: loss 136.873935\n",
      "iteration 1500 / 15000: loss 127.268752\n",
      "iteration 1600 / 15000: loss 118.119048\n",
      "iteration 1700 / 15000: loss 109.608124\n",
      "iteration 1800 / 15000: loss 101.722761\n",
      "iteration 1900 / 15000: loss 94.555330\n",
      "iteration 2000 / 15000: loss 87.766306\n",
      "iteration 2100 / 15000: loss 81.517046\n",
      "iteration 2200 / 15000: loss 75.681890\n",
      "iteration 2300 / 15000: loss 70.297088\n",
      "iteration 2400 / 15000: loss 65.461079\n",
      "iteration 2500 / 15000: loss 60.893092\n",
      "iteration 2600 / 15000: loss 56.551423\n",
      "iteration 2700 / 15000: loss 52.671171\n",
      "iteration 2800 / 15000: loss 48.909034\n",
      "iteration 2900 / 15000: loss 45.435681\n",
      "iteration 3000 / 15000: loss 42.338375\n",
      "iteration 3100 / 15000: loss 39.544302\n",
      "iteration 3200 / 15000: loss 36.741141\n",
      "iteration 3300 / 15000: loss 34.284431\n",
      "iteration 3400 / 15000: loss 31.846415\n",
      "iteration 3500 / 15000: loss 29.723709\n",
      "iteration 3600 / 15000: loss 27.704829\n",
      "iteration 3700 / 15000: loss 25.854541\n",
      "iteration 3800 / 15000: loss 24.169820\n",
      "iteration 3900 / 15000: loss 22.475004\n",
      "iteration 4000 / 15000: loss 21.098845\n",
      "iteration 4100 / 15000: loss 19.633758\n",
      "iteration 4200 / 15000: loss 18.323369\n",
      "iteration 4300 / 15000: loss 17.131429\n",
      "iteration 4400 / 15000: loss 16.080291\n",
      "iteration 4500 / 15000: loss 15.072131\n",
      "iteration 4600 / 15000: loss 14.218058\n",
      "iteration 4700 / 15000: loss 13.216000\n",
      "iteration 4800 / 15000: loss 12.449073\n",
      "iteration 4900 / 15000: loss 11.719126\n",
      "iteration 5000 / 15000: loss 10.962656\n",
      "iteration 5100 / 15000: loss 10.263757\n",
      "iteration 5200 / 15000: loss 9.690091\n",
      "iteration 5300 / 15000: loss 9.166654\n",
      "iteration 5400 / 15000: loss 8.583232\n",
      "iteration 5500 / 15000: loss 8.196896\n",
      "iteration 5600 / 15000: loss 7.712139\n",
      "iteration 5700 / 15000: loss 7.311786\n",
      "iteration 5800 / 15000: loss 6.881985\n",
      "iteration 5900 / 15000: loss 6.581925\n",
      "iteration 6000 / 15000: loss 6.259769\n",
      "iteration 6100 / 15000: loss 5.918010\n",
      "iteration 6200 / 15000: loss 5.673158\n",
      "iteration 6300 / 15000: loss 5.292971\n",
      "iteration 6400 / 15000: loss 5.067377\n",
      "iteration 6500 / 15000: loss 4.885736\n",
      "iteration 6600 / 15000: loss 4.770589\n",
      "iteration 6700 / 15000: loss 4.503585\n",
      "iteration 6800 / 15000: loss 4.393260\n",
      "iteration 6900 / 15000: loss 4.171693\n",
      "iteration 7000 / 15000: loss 3.981430\n",
      "iteration 7100 / 15000: loss 3.874312\n",
      "iteration 7200 / 15000: loss 3.763423\n",
      "iteration 7300 / 15000: loss 3.591624\n",
      "iteration 7400 / 15000: loss 3.485647\n",
      "iteration 7500 / 15000: loss 3.450691\n",
      "iteration 7600 / 15000: loss 3.317703\n",
      "iteration 7700 / 15000: loss 3.145444\n",
      "iteration 7800 / 15000: loss 3.137100\n",
      "iteration 7900 / 15000: loss 2.967460\n",
      "iteration 8000 / 15000: loss 2.954730\n",
      "iteration 8100 / 15000: loss 2.892941\n",
      "iteration 8200 / 15000: loss 2.729476\n",
      "iteration 8300 / 15000: loss 2.788340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8400 / 15000: loss 2.721516\n",
      "iteration 8500 / 15000: loss 2.721139\n",
      "iteration 8600 / 15000: loss 2.625956\n",
      "iteration 8700 / 15000: loss 2.582343\n",
      "iteration 8800 / 15000: loss 2.499026\n",
      "iteration 8900 / 15000: loss 2.570549\n",
      "iteration 9000 / 15000: loss 2.440020\n",
      "iteration 9100 / 15000: loss 2.443060\n",
      "iteration 9200 / 15000: loss 2.367991\n",
      "iteration 9300 / 15000: loss 2.315162\n",
      "iteration 9400 / 15000: loss 2.379797\n",
      "iteration 9500 / 15000: loss 2.340723\n",
      "iteration 9600 / 15000: loss 2.326879\n",
      "iteration 9700 / 15000: loss 2.263904\n",
      "iteration 9800 / 15000: loss 2.281071\n",
      "iteration 9900 / 15000: loss 2.231064\n",
      "iteration 10000 / 15000: loss 2.257640\n",
      "iteration 10100 / 15000: loss 2.162496\n",
      "iteration 10200 / 15000: loss 2.195443\n",
      "iteration 10300 / 15000: loss 2.129672\n",
      "iteration 10400 / 15000: loss 2.152423\n",
      "iteration 10500 / 15000: loss 2.207986\n",
      "iteration 10600 / 15000: loss 2.145585\n",
      "iteration 10700 / 15000: loss 2.145175\n",
      "iteration 10800 / 15000: loss 2.095873\n",
      "iteration 10900 / 15000: loss 2.179182\n",
      "iteration 11000 / 15000: loss 2.104304\n",
      "iteration 11100 / 15000: loss 2.109145\n",
      "iteration 11200 / 15000: loss 2.100952\n",
      "iteration 11300 / 15000: loss 2.109437\n",
      "iteration 11400 / 15000: loss 2.007905\n",
      "iteration 11500 / 15000: loss 2.047497\n",
      "iteration 11600 / 15000: loss 2.082208\n",
      "iteration 11700 / 15000: loss 2.034668\n",
      "iteration 11800 / 15000: loss 2.144463\n",
      "iteration 11900 / 15000: loss 2.181660\n",
      "iteration 12000 / 15000: loss 2.100406\n",
      "iteration 12100 / 15000: loss 2.061663\n",
      "iteration 12200 / 15000: loss 2.016679\n",
      "iteration 12300 / 15000: loss 2.064170\n",
      "iteration 12400 / 15000: loss 2.092242\n",
      "iteration 12500 / 15000: loss 2.019254\n",
      "iteration 12600 / 15000: loss 2.096541\n",
      "iteration 12700 / 15000: loss 1.960257\n",
      "iteration 12800 / 15000: loss 2.091714\n",
      "iteration 12900 / 15000: loss 2.021457\n",
      "iteration 13000 / 15000: loss 2.036037\n",
      "iteration 13100 / 15000: loss 2.031392\n",
      "iteration 13200 / 15000: loss 2.024845\n",
      "iteration 13300 / 15000: loss 2.096622\n",
      "iteration 13400 / 15000: loss 2.135073\n",
      "iteration 13500 / 15000: loss 2.135023\n",
      "iteration 13600 / 15000: loss 2.035905\n",
      "iteration 13700 / 15000: loss 2.046237\n",
      "iteration 13800 / 15000: loss 2.026432\n",
      "iteration 13900 / 15000: loss 2.017519\n",
      "iteration 14000 / 15000: loss 2.083209\n",
      "iteration 14100 / 15000: loss 1.949329\n",
      "iteration 14200 / 15000: loss 2.031947\n",
      "iteration 14300 / 15000: loss 2.048458\n",
      "iteration 14400 / 15000: loss 2.014697\n",
      "iteration 14500 / 15000: loss 2.048530\n",
      "iteration 14600 / 15000: loss 1.955556\n",
      "iteration 14700 / 15000: loss 2.094521\n",
      "iteration 14800 / 15000: loss 2.010616\n",
      "iteration 14900 / 15000: loss 1.986090\n",
      "\n",
      "for r= 15000.0  and l= 1.5e-08\n",
      "iteration 0 / 15000: loss 469.143548\n",
      "iteration 100 / 15000: loss 427.666615\n",
      "iteration 200 / 15000: loss 391.146036\n",
      "iteration 300 / 15000: loss 357.341154\n",
      "iteration 400 / 15000: loss 326.386972\n",
      "iteration 500 / 15000: loss 298.164608\n",
      "iteration 600 / 15000: loss 272.629635\n",
      "iteration 700 / 15000: loss 249.151414\n",
      "iteration 800 / 15000: loss 227.839756\n",
      "iteration 900 / 15000: loss 208.035812\n",
      "iteration 1000 / 15000: loss 190.506937\n",
      "iteration 1100 / 15000: loss 174.075126\n",
      "iteration 1200 / 15000: loss 159.196578\n",
      "iteration 1300 / 15000: loss 145.426931\n",
      "iteration 1400 / 15000: loss 133.336245\n",
      "iteration 1500 / 15000: loss 121.792492\n",
      "iteration 1600 / 15000: loss 111.513462\n",
      "iteration 1700 / 15000: loss 102.066096\n",
      "iteration 1800 / 15000: loss 93.485132\n",
      "iteration 1900 / 15000: loss 85.596525\n",
      "iteration 2000 / 15000: loss 78.274746\n",
      "iteration 2100 / 15000: loss 71.779456\n",
      "iteration 2200 / 15000: loss 65.626058\n",
      "iteration 2300 / 15000: loss 60.145948\n",
      "iteration 2400 / 15000: loss 55.161555\n",
      "iteration 2500 / 15000: loss 50.512281\n",
      "iteration 2600 / 15000: loss 46.356167\n",
      "iteration 2700 / 15000: loss 42.519705\n",
      "iteration 2800 / 15000: loss 39.005289\n",
      "iteration 2900 / 15000: loss 35.760693\n",
      "iteration 3000 / 15000: loss 32.877797\n",
      "iteration 3100 / 15000: loss 30.274501\n",
      "iteration 3200 / 15000: loss 27.777255\n",
      "iteration 3300 / 15000: loss 25.735165\n",
      "iteration 3400 / 15000: loss 23.555741\n",
      "iteration 3500 / 15000: loss 21.673704\n",
      "iteration 3600 / 15000: loss 20.005015\n",
      "iteration 3700 / 15000: loss 18.477669\n",
      "iteration 3800 / 15000: loss 17.030296\n",
      "iteration 3900 / 15000: loss 15.731453\n",
      "iteration 4000 / 15000: loss 14.583890\n",
      "iteration 4100 / 15000: loss 13.457868\n",
      "iteration 4200 / 15000: loss 12.526697\n",
      "iteration 4300 / 15000: loss 11.578777\n",
      "iteration 4400 / 15000: loss 10.745309\n",
      "iteration 4500 / 15000: loss 9.991244\n",
      "iteration 4600 / 15000: loss 9.289638\n",
      "iteration 4700 / 15000: loss 8.708232\n",
      "iteration 4800 / 15000: loss 8.137792\n",
      "iteration 4900 / 15000: loss 7.612760\n",
      "iteration 5000 / 15000: loss 7.147211\n",
      "iteration 5100 / 15000: loss 6.616463\n",
      "iteration 5200 / 15000: loss 6.278468\n",
      "iteration 5300 / 15000: loss 5.878106\n",
      "iteration 5400 / 15000: loss 5.572322\n",
      "iteration 5500 / 15000: loss 5.345289\n",
      "iteration 5600 / 15000: loss 5.075316\n",
      "iteration 5700 / 15000: loss 4.669218\n",
      "iteration 5800 / 15000: loss 4.547540\n",
      "iteration 5900 / 15000: loss 4.263275\n",
      "iteration 6000 / 15000: loss 4.065498\n",
      "iteration 6100 / 15000: loss 3.875559\n",
      "iteration 6200 / 15000: loss 3.762062\n",
      "iteration 6300 / 15000: loss 3.605603\n",
      "iteration 6400 / 15000: loss 3.429617\n",
      "iteration 6500 / 15000: loss 3.300125\n",
      "iteration 6600 / 15000: loss 3.204745\n",
      "iteration 6700 / 15000: loss 3.181279\n",
      "iteration 6800 / 15000: loss 3.038549\n",
      "iteration 6900 / 15000: loss 2.964075\n",
      "iteration 7000 / 15000: loss 2.887826\n",
      "iteration 7100 / 15000: loss 2.878163\n",
      "iteration 7200 / 15000: loss 2.744344\n",
      "iteration 7300 / 15000: loss 2.659415\n",
      "iteration 7400 / 15000: loss 2.556269\n",
      "iteration 7500 / 15000: loss 2.631002\n",
      "iteration 7600 / 15000: loss 2.473486\n",
      "iteration 7700 / 15000: loss 2.386489\n",
      "iteration 7800 / 15000: loss 2.443102\n",
      "iteration 7900 / 15000: loss 2.422105\n",
      "iteration 8000 / 15000: loss 2.348092\n",
      "iteration 8100 / 15000: loss 2.303966\n",
      "iteration 8200 / 15000: loss 2.348218\n",
      "iteration 8300 / 15000: loss 2.322102\n",
      "iteration 8400 / 15000: loss 2.398389\n",
      "iteration 8500 / 15000: loss 2.287817\n",
      "iteration 8600 / 15000: loss 2.214274\n",
      "iteration 8700 / 15000: loss 2.192559\n",
      "iteration 8800 / 15000: loss 2.149813\n",
      "iteration 8900 / 15000: loss 2.175780\n",
      "iteration 9000 / 15000: loss 2.138118\n",
      "iteration 9100 / 15000: loss 2.198534\n",
      "iteration 9200 / 15000: loss 2.122149\n",
      "iteration 9300 / 15000: loss 2.094339\n",
      "iteration 9400 / 15000: loss 2.194531\n",
      "iteration 9500 / 15000: loss 2.124307\n",
      "iteration 9600 / 15000: loss 2.092689\n",
      "iteration 9700 / 15000: loss 2.084619\n",
      "iteration 9800 / 15000: loss 2.111188\n",
      "iteration 9900 / 15000: loss 2.086998\n",
      "iteration 10000 / 15000: loss 2.111233\n",
      "iteration 10100 / 15000: loss 2.006751\n",
      "iteration 10200 / 15000: loss 2.114877\n",
      "iteration 10300 / 15000: loss 2.139000\n",
      "iteration 10400 / 15000: loss 2.175496\n",
      "iteration 10500 / 15000: loss 2.062784\n",
      "iteration 10600 / 15000: loss 2.081712\n",
      "iteration 10700 / 15000: loss 2.074731\n",
      "iteration 10800 / 15000: loss 2.078155\n",
      "iteration 10900 / 15000: loss 2.024505\n",
      "iteration 11000 / 15000: loss 2.089970\n",
      "iteration 11100 / 15000: loss 2.093373\n",
      "iteration 11200 / 15000: loss 2.050729\n",
      "iteration 11300 / 15000: loss 2.016050\n",
      "iteration 11400 / 15000: loss 2.042602\n",
      "iteration 11500 / 15000: loss 2.029949\n",
      "iteration 11600 / 15000: loss 2.108245\n",
      "iteration 11700 / 15000: loss 2.071371\n",
      "iteration 11800 / 15000: loss 2.042535\n",
      "iteration 11900 / 15000: loss 1.998237\n",
      "iteration 12000 / 15000: loss 2.098485\n",
      "iteration 12100 / 15000: loss 2.006574\n",
      "iteration 12200 / 15000: loss 2.044232\n",
      "iteration 12300 / 15000: loss 2.041958\n",
      "iteration 12400 / 15000: loss 2.041590\n",
      "iteration 12500 / 15000: loss 2.086865\n",
      "iteration 12600 / 15000: loss 2.033342\n",
      "iteration 12700 / 15000: loss 2.018126\n",
      "iteration 12800 / 15000: loss 2.043874\n",
      "iteration 12900 / 15000: loss 2.025537\n",
      "iteration 13000 / 15000: loss 2.003322\n",
      "iteration 13100 / 15000: loss 2.036636\n",
      "iteration 13200 / 15000: loss 2.046864\n",
      "iteration 13300 / 15000: loss 2.066330\n",
      "iteration 13400 / 15000: loss 1.966704\n",
      "iteration 13500 / 15000: loss 2.041319\n",
      "iteration 13600 / 15000: loss 1.951961\n",
      "iteration 13700 / 15000: loss 2.068355\n",
      "iteration 13800 / 15000: loss 2.000091\n",
      "iteration 13900 / 15000: loss 2.122950\n",
      "iteration 14000 / 15000: loss 1.975753\n",
      "iteration 14100 / 15000: loss 2.058869\n",
      "iteration 14200 / 15000: loss 2.007950\n",
      "iteration 14300 / 15000: loss 2.080087\n",
      "iteration 14400 / 15000: loss 2.030464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14500 / 15000: loss 2.037950\n",
      "iteration 14600 / 15000: loss 1.951226\n",
      "iteration 14700 / 15000: loss 2.032467\n",
      "iteration 14800 / 15000: loss 2.078940\n",
      "iteration 14900 / 15000: loss 2.060788\n",
      "\n",
      "for r= 20000.0  and l= 1.5e-08\n",
      "iteration 0 / 15000: loss 625.036141\n",
      "iteration 100 / 15000: loss 553.139329\n",
      "iteration 200 / 15000: loss 490.559323\n",
      "iteration 300 / 15000: loss 435.097778\n",
      "iteration 400 / 15000: loss 385.839151\n",
      "iteration 500 / 15000: loss 342.226560\n",
      "iteration 600 / 15000: loss 303.446008\n",
      "iteration 700 / 15000: loss 269.333862\n",
      "iteration 800 / 15000: loss 238.847575\n",
      "iteration 900 / 15000: loss 212.278055\n",
      "iteration 1000 / 15000: loss 188.218920\n",
      "iteration 1100 / 15000: loss 167.135471\n",
      "iteration 1200 / 15000: loss 148.222244\n",
      "iteration 1300 / 15000: loss 131.683509\n",
      "iteration 1400 / 15000: loss 117.109106\n",
      "iteration 1500 / 15000: loss 104.065969\n",
      "iteration 1600 / 15000: loss 92.542719\n",
      "iteration 1700 / 15000: loss 82.240609\n",
      "iteration 1800 / 15000: loss 73.114446\n",
      "iteration 1900 / 15000: loss 65.048361\n",
      "iteration 2000 / 15000: loss 57.900906\n",
      "iteration 2100 / 15000: loss 51.631242\n",
      "iteration 2200 / 15000: loss 45.943731\n",
      "iteration 2300 / 15000: loss 40.969508\n",
      "iteration 2400 / 15000: loss 36.546559\n",
      "iteration 2500 / 15000: loss 32.556940\n",
      "iteration 2600 / 15000: loss 29.229278\n",
      "iteration 2700 / 15000: loss 26.083836\n",
      "iteration 2800 / 15000: loss 23.397136\n",
      "iteration 2900 / 15000: loss 20.955667\n",
      "iteration 3000 / 15000: loss 18.832382\n",
      "iteration 3100 / 15000: loss 16.891625\n",
      "iteration 3200 / 15000: loss 15.174853\n",
      "iteration 3300 / 15000: loss 13.807367\n",
      "iteration 3400 / 15000: loss 12.479612\n",
      "iteration 3500 / 15000: loss 11.256395\n",
      "iteration 3600 / 15000: loss 10.264749\n",
      "iteration 3700 / 15000: loss 9.316955\n",
      "iteration 3800 / 15000: loss 8.424714\n",
      "iteration 3900 / 15000: loss 7.721999\n",
      "iteration 4000 / 15000: loss 7.048546\n",
      "iteration 4100 / 15000: loss 6.477358\n",
      "iteration 4200 / 15000: loss 6.035284\n",
      "iteration 4300 / 15000: loss 5.624421\n",
      "iteration 4400 / 15000: loss 5.125466\n",
      "iteration 4500 / 15000: loss 4.868553\n",
      "iteration 4600 / 15000: loss 4.498919\n",
      "iteration 4700 / 15000: loss 4.233837\n",
      "iteration 4800 / 15000: loss 3.962931\n",
      "iteration 4900 / 15000: loss 3.767046\n",
      "iteration 5000 / 15000: loss 3.522166\n",
      "iteration 5100 / 15000: loss 3.425035\n",
      "iteration 5200 / 15000: loss 3.260465\n",
      "iteration 5300 / 15000: loss 3.146963\n",
      "iteration 5400 / 15000: loss 2.980739\n",
      "iteration 5500 / 15000: loss 2.794691\n",
      "iteration 5600 / 15000: loss 2.804908\n",
      "iteration 5700 / 15000: loss 2.751334\n",
      "iteration 5800 / 15000: loss 2.653616\n",
      "iteration 5900 / 15000: loss 2.549792\n",
      "iteration 6000 / 15000: loss 2.495576\n",
      "iteration 6100 / 15000: loss 2.506443\n",
      "iteration 6200 / 15000: loss 2.415295\n",
      "iteration 6300 / 15000: loss 2.352582\n",
      "iteration 6400 / 15000: loss 2.369586\n",
      "iteration 6500 / 15000: loss 2.307728\n",
      "iteration 6600 / 15000: loss 2.355658\n",
      "iteration 6700 / 15000: loss 2.294376\n",
      "iteration 6800 / 15000: loss 2.218494\n",
      "iteration 6900 / 15000: loss 2.218087\n",
      "iteration 7000 / 15000: loss 2.234221\n",
      "iteration 7100 / 15000: loss 2.127378\n",
      "iteration 7200 / 15000: loss 2.145732\n",
      "iteration 7300 / 15000: loss 2.155933\n",
      "iteration 7400 / 15000: loss 2.212849\n",
      "iteration 7500 / 15000: loss 2.190358\n",
      "iteration 7600 / 15000: loss 2.105111\n",
      "iteration 7700 / 15000: loss 2.108954\n",
      "iteration 7800 / 15000: loss 2.184347\n",
      "iteration 7900 / 15000: loss 2.044771\n",
      "iteration 8000 / 15000: loss 2.078747\n",
      "iteration 8100 / 15000: loss 2.145418\n",
      "iteration 8200 / 15000: loss 2.133564\n",
      "iteration 8300 / 15000: loss 2.091259\n",
      "iteration 8400 / 15000: loss 2.124226\n",
      "iteration 8500 / 15000: loss 2.111955\n",
      "iteration 8600 / 15000: loss 2.082544\n",
      "iteration 8700 / 15000: loss 2.143861\n",
      "iteration 8800 / 15000: loss 2.061357\n",
      "iteration 8900 / 15000: loss 2.113448\n",
      "iteration 9000 / 15000: loss 2.106128\n",
      "iteration 9100 / 15000: loss 2.053729\n",
      "iteration 9200 / 15000: loss 2.103214\n",
      "iteration 9300 / 15000: loss 2.077025\n",
      "iteration 9400 / 15000: loss 2.007832\n",
      "iteration 9500 / 15000: loss 2.136264\n",
      "iteration 9600 / 15000: loss 2.069052\n",
      "iteration 9700 / 15000: loss 2.137890\n",
      "iteration 9800 / 15000: loss 2.086686\n",
      "iteration 9900 / 15000: loss 2.055197\n",
      "iteration 10000 / 15000: loss 2.104994\n",
      "iteration 10100 / 15000: loss 2.103820\n",
      "iteration 10200 / 15000: loss 2.115586\n",
      "iteration 10300 / 15000: loss 2.022152\n",
      "iteration 10400 / 15000: loss 2.071501\n",
      "iteration 10500 / 15000: loss 2.053391\n",
      "iteration 10600 / 15000: loss 2.044080\n",
      "iteration 10700 / 15000: loss 2.081988\n",
      "iteration 10800 / 15000: loss 2.089860\n",
      "iteration 10900 / 15000: loss 2.064892\n",
      "iteration 11000 / 15000: loss 2.057579\n",
      "iteration 11100 / 15000: loss 2.144331\n",
      "iteration 11200 / 15000: loss 2.096030\n",
      "iteration 11300 / 15000: loss 2.056026\n",
      "iteration 11400 / 15000: loss 2.042397\n",
      "iteration 11500 / 15000: loss 2.132585\n",
      "iteration 11600 / 15000: loss 2.015774\n",
      "iteration 11700 / 15000: loss 2.033135\n",
      "iteration 11800 / 15000: loss 2.079010\n",
      "iteration 11900 / 15000: loss 2.085431\n",
      "iteration 12000 / 15000: loss 2.021447\n",
      "iteration 12100 / 15000: loss 2.033341\n",
      "iteration 12200 / 15000: loss 2.047948\n",
      "iteration 12300 / 15000: loss 2.110255\n",
      "iteration 12400 / 15000: loss 2.096632\n",
      "iteration 12500 / 15000: loss 2.019750\n",
      "iteration 12600 / 15000: loss 2.047600\n",
      "iteration 12700 / 15000: loss 2.052750\n",
      "iteration 12800 / 15000: loss 2.074110\n",
      "iteration 12900 / 15000: loss 2.065811\n",
      "iteration 13000 / 15000: loss 2.030768\n",
      "iteration 13100 / 15000: loss 2.063247\n",
      "iteration 13200 / 15000: loss 2.058947\n",
      "iteration 13300 / 15000: loss 2.124322\n",
      "iteration 13400 / 15000: loss 2.071370\n",
      "iteration 13500 / 15000: loss 2.063441\n",
      "iteration 13600 / 15000: loss 2.121716\n",
      "iteration 13700 / 15000: loss 2.048249\n",
      "iteration 13800 / 15000: loss 2.133165\n",
      "iteration 13900 / 15000: loss 2.132657\n",
      "iteration 14000 / 15000: loss 2.048475\n",
      "iteration 14100 / 15000: loss 2.094649\n",
      "iteration 14200 / 15000: loss 2.084854\n",
      "iteration 14300 / 15000: loss 2.067451\n",
      "iteration 14400 / 15000: loss 2.051026\n",
      "iteration 14500 / 15000: loss 2.033300\n",
      "iteration 14600 / 15000: loss 2.090012\n",
      "iteration 14700 / 15000: loss 2.090029\n",
      "iteration 14800 / 15000: loss 2.077453\n",
      "iteration 14900 / 15000: loss 2.134577\n",
      "\n",
      "for r= 8000.0  and l= 2.5e-08\n",
      "iteration 0 / 15000: loss 248.998470\n",
      "iteration 100 / 15000: loss 229.223405\n",
      "iteration 200 / 15000: loss 211.417380\n",
      "iteration 300 / 15000: loss 195.025549\n",
      "iteration 400 / 15000: loss 179.679039\n",
      "iteration 500 / 15000: loss 166.208783\n",
      "iteration 600 / 15000: loss 153.329632\n",
      "iteration 700 / 15000: loss 141.442586\n",
      "iteration 800 / 15000: loss 130.765538\n",
      "iteration 900 / 15000: loss 120.564129\n",
      "iteration 1000 / 15000: loss 111.608719\n",
      "iteration 1100 / 15000: loss 103.088044\n",
      "iteration 1200 / 15000: loss 95.150980\n",
      "iteration 1300 / 15000: loss 87.992299\n",
      "iteration 1400 / 15000: loss 81.236195\n",
      "iteration 1500 / 15000: loss 75.263134\n",
      "iteration 1600 / 15000: loss 69.494401\n",
      "iteration 1700 / 15000: loss 64.377891\n",
      "iteration 1800 / 15000: loss 59.697331\n",
      "iteration 1900 / 15000: loss 55.075951\n",
      "iteration 2000 / 15000: loss 50.858180\n",
      "iteration 2100 / 15000: loss 47.080583\n",
      "iteration 2200 / 15000: loss 43.591254\n",
      "iteration 2300 / 15000: loss 40.379221\n",
      "iteration 2400 / 15000: loss 37.454726\n",
      "iteration 2500 / 15000: loss 34.697125\n",
      "iteration 2600 / 15000: loss 32.167368\n",
      "iteration 2700 / 15000: loss 29.793040\n",
      "iteration 2800 / 15000: loss 27.639586\n",
      "iteration 2900 / 15000: loss 25.846251\n",
      "iteration 3000 / 15000: loss 23.774620\n",
      "iteration 3100 / 15000: loss 22.206776\n",
      "iteration 3200 / 15000: loss 20.579610\n",
      "iteration 3300 / 15000: loss 19.238751\n",
      "iteration 3400 / 15000: loss 17.774840\n",
      "iteration 3500 / 15000: loss 16.630533\n",
      "iteration 3600 / 15000: loss 15.454621\n",
      "iteration 3700 / 15000: loss 14.415279\n",
      "iteration 3800 / 15000: loss 13.430352\n",
      "iteration 3900 / 15000: loss 12.619603\n",
      "iteration 4000 / 15000: loss 11.719606\n",
      "iteration 4100 / 15000: loss 10.985793\n",
      "iteration 4200 / 15000: loss 10.316303\n",
      "iteration 4300 / 15000: loss 9.657838\n",
      "iteration 4400 / 15000: loss 9.076661\n",
      "iteration 4500 / 15000: loss 8.518686\n",
      "iteration 4600 / 15000: loss 8.010399\n",
      "iteration 4700 / 15000: loss 7.629581\n",
      "iteration 4800 / 15000: loss 7.151889\n",
      "iteration 4900 / 15000: loss 6.676848\n",
      "iteration 5000 / 15000: loss 6.345082\n",
      "iteration 5100 / 15000: loss 6.105486\n",
      "iteration 5200 / 15000: loss 5.701767\n",
      "iteration 5300 / 15000: loss 5.411850\n",
      "iteration 5400 / 15000: loss 5.162185\n",
      "iteration 5500 / 15000: loss 4.901638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5600 / 15000: loss 4.685175\n",
      "iteration 5700 / 15000: loss 4.425569\n",
      "iteration 5800 / 15000: loss 4.283418\n",
      "iteration 5900 / 15000: loss 4.082707\n",
      "iteration 6000 / 15000: loss 3.955824\n",
      "iteration 6100 / 15000: loss 3.871159\n",
      "iteration 6200 / 15000: loss 3.689510\n",
      "iteration 6300 / 15000: loss 3.479508\n",
      "iteration 6400 / 15000: loss 3.476748\n",
      "iteration 6500 / 15000: loss 3.203249\n",
      "iteration 6600 / 15000: loss 3.218730\n",
      "iteration 6700 / 15000: loss 3.132688\n",
      "iteration 6800 / 15000: loss 2.984019\n",
      "iteration 6900 / 15000: loss 2.935398\n",
      "iteration 7000 / 15000: loss 2.897929\n",
      "iteration 7100 / 15000: loss 2.821878\n",
      "iteration 7200 / 15000: loss 2.727642\n",
      "iteration 7300 / 15000: loss 2.617744\n",
      "iteration 7400 / 15000: loss 2.612250\n",
      "iteration 7500 / 15000: loss 2.577441\n",
      "iteration 7600 / 15000: loss 2.521955\n",
      "iteration 7700 / 15000: loss 2.490753\n",
      "iteration 7800 / 15000: loss 2.397434\n",
      "iteration 7900 / 15000: loss 2.475892\n",
      "iteration 8000 / 15000: loss 2.380420\n",
      "iteration 8100 / 15000: loss 2.359590\n",
      "iteration 8200 / 15000: loss 2.330449\n",
      "iteration 8300 / 15000: loss 2.273315\n",
      "iteration 8400 / 15000: loss 2.248528\n",
      "iteration 8500 / 15000: loss 2.255482\n",
      "iteration 8600 / 15000: loss 2.207530\n",
      "iteration 8700 / 15000: loss 2.221422\n",
      "iteration 8800 / 15000: loss 2.215873\n",
      "iteration 8900 / 15000: loss 2.194234\n",
      "iteration 9000 / 15000: loss 2.166713\n",
      "iteration 9100 / 15000: loss 2.091372\n",
      "iteration 9200 / 15000: loss 2.088404\n",
      "iteration 9300 / 15000: loss 2.149264\n",
      "iteration 9400 / 15000: loss 2.100553\n",
      "iteration 9500 / 15000: loss 2.105530\n",
      "iteration 9600 / 15000: loss 2.064632\n",
      "iteration 9700 / 15000: loss 2.123017\n",
      "iteration 9800 / 15000: loss 2.169811\n",
      "iteration 9900 / 15000: loss 2.072368\n",
      "iteration 10000 / 15000: loss 2.038418\n",
      "iteration 10100 / 15000: loss 2.069249\n",
      "iteration 10200 / 15000: loss 1.983038\n",
      "iteration 10300 / 15000: loss 2.032986\n",
      "iteration 10400 / 15000: loss 2.023293\n",
      "iteration 10500 / 15000: loss 2.022147\n",
      "iteration 10600 / 15000: loss 2.062096\n",
      "iteration 10700 / 15000: loss 2.011874\n",
      "iteration 10800 / 15000: loss 2.021021\n",
      "iteration 10900 / 15000: loss 1.985185\n",
      "iteration 11000 / 15000: loss 2.101945\n",
      "iteration 11100 / 15000: loss 1.986870\n",
      "iteration 11200 / 15000: loss 2.042586\n",
      "iteration 11300 / 15000: loss 2.008677\n",
      "iteration 11400 / 15000: loss 2.022349\n",
      "iteration 11500 / 15000: loss 1.979748\n",
      "iteration 11600 / 15000: loss 2.082442\n",
      "iteration 11700 / 15000: loss 1.901921\n",
      "iteration 11800 / 15000: loss 1.975806\n",
      "iteration 11900 / 15000: loss 2.001065\n",
      "iteration 12000 / 15000: loss 2.007885\n",
      "iteration 12100 / 15000: loss 1.991929\n",
      "iteration 12200 / 15000: loss 1.987635\n",
      "iteration 12300 / 15000: loss 1.971071\n",
      "iteration 12400 / 15000: loss 2.006662\n",
      "iteration 12500 / 15000: loss 1.981238\n",
      "iteration 12600 / 15000: loss 2.010786\n",
      "iteration 12700 / 15000: loss 2.020942\n",
      "iteration 12800 / 15000: loss 1.952039\n",
      "iteration 12900 / 15000: loss 1.974139\n",
      "iteration 13000 / 15000: loss 1.946997\n",
      "iteration 13100 / 15000: loss 1.950670\n",
      "iteration 13200 / 15000: loss 2.061608\n",
      "iteration 13300 / 15000: loss 1.974787\n",
      "iteration 13400 / 15000: loss 1.969061\n",
      "iteration 13500 / 15000: loss 1.973251\n",
      "iteration 13600 / 15000: loss 1.943751\n",
      "iteration 13700 / 15000: loss 1.982291\n",
      "iteration 13800 / 15000: loss 1.943630\n",
      "iteration 13900 / 15000: loss 2.014900\n",
      "iteration 14000 / 15000: loss 2.024788\n",
      "iteration 14100 / 15000: loss 2.015231\n",
      "iteration 14200 / 15000: loss 1.931413\n",
      "iteration 14300 / 15000: loss 1.946390\n",
      "iteration 14400 / 15000: loss 2.006637\n",
      "iteration 14500 / 15000: loss 1.945681\n",
      "iteration 14600 / 15000: loss 1.970067\n",
      "iteration 14700 / 15000: loss 1.975090\n",
      "iteration 14800 / 15000: loss 1.968164\n",
      "iteration 14900 / 15000: loss 2.030690\n",
      "\n",
      "for r= 10000.0  and l= 2.5e-08\n",
      "iteration 0 / 15000: loss 316.384962\n",
      "iteration 100 / 15000: loss 286.068759\n",
      "iteration 200 / 15000: loss 258.213495\n",
      "iteration 300 / 15000: loss 233.402930\n",
      "iteration 400 / 15000: loss 211.148873\n",
      "iteration 500 / 15000: loss 191.215873\n",
      "iteration 600 / 15000: loss 173.267150\n",
      "iteration 700 / 15000: loss 156.355680\n",
      "iteration 800 / 15000: loss 141.670342\n",
      "iteration 900 / 15000: loss 128.460749\n",
      "iteration 1000 / 15000: loss 116.370158\n",
      "iteration 1100 / 15000: loss 105.266188\n",
      "iteration 1200 / 15000: loss 95.518640\n",
      "iteration 1300 / 15000: loss 86.502383\n",
      "iteration 1400 / 15000: loss 78.418287\n",
      "iteration 1500 / 15000: loss 70.985972\n",
      "iteration 1600 / 15000: loss 64.594115\n",
      "iteration 1700 / 15000: loss 58.500399\n",
      "iteration 1800 / 15000: loss 52.996705\n",
      "iteration 1900 / 15000: loss 48.259165\n",
      "iteration 2000 / 15000: loss 43.829730\n",
      "iteration 2100 / 15000: loss 39.750798\n",
      "iteration 2200 / 15000: loss 36.161219\n",
      "iteration 2300 / 15000: loss 32.868070\n",
      "iteration 2400 / 15000: loss 29.977526\n",
      "iteration 2500 / 15000: loss 27.301663\n",
      "iteration 2600 / 15000: loss 25.023063\n",
      "iteration 2700 / 15000: loss 22.651279\n",
      "iteration 2800 / 15000: loss 20.635428\n",
      "iteration 2900 / 15000: loss 18.957160\n",
      "iteration 3000 / 15000: loss 17.335861\n",
      "iteration 3100 / 15000: loss 15.879636\n",
      "iteration 3200 / 15000: loss 14.475098\n",
      "iteration 3300 / 15000: loss 13.302617\n",
      "iteration 3400 / 15000: loss 12.193195\n",
      "iteration 3500 / 15000: loss 11.293759\n",
      "iteration 3600 / 15000: loss 10.344715\n",
      "iteration 3700 / 15000: loss 9.531892\n",
      "iteration 3800 / 15000: loss 8.795082\n",
      "iteration 3900 / 15000: loss 8.163983\n",
      "iteration 4000 / 15000: loss 7.637263\n",
      "iteration 4100 / 15000: loss 7.071148\n",
      "iteration 4200 / 15000: loss 6.626491\n",
      "iteration 4300 / 15000: loss 6.137404\n",
      "iteration 4400 / 15000: loss 5.765118\n",
      "iteration 4500 / 15000: loss 5.386705\n",
      "iteration 4600 / 15000: loss 5.010950\n",
      "iteration 4700 / 15000: loss 4.695170\n",
      "iteration 4800 / 15000: loss 4.589252\n",
      "iteration 4900 / 15000: loss 4.326559\n",
      "iteration 5000 / 15000: loss 3.974350\n",
      "iteration 5100 / 15000: loss 3.894698\n",
      "iteration 5200 / 15000: loss 3.666951\n",
      "iteration 5300 / 15000: loss 3.498273\n",
      "iteration 5400 / 15000: loss 3.312503\n",
      "iteration 5500 / 15000: loss 3.288851\n",
      "iteration 5600 / 15000: loss 3.108639\n",
      "iteration 5700 / 15000: loss 3.070152\n",
      "iteration 5800 / 15000: loss 2.916835\n",
      "iteration 5900 / 15000: loss 2.786979\n",
      "iteration 6000 / 15000: loss 2.775869\n",
      "iteration 6100 / 15000: loss 2.673554\n",
      "iteration 6200 / 15000: loss 2.604746\n",
      "iteration 6300 / 15000: loss 2.631760\n",
      "iteration 6400 / 15000: loss 2.486845\n",
      "iteration 6500 / 15000: loss 2.430643\n",
      "iteration 6600 / 15000: loss 2.446045\n",
      "iteration 6700 / 15000: loss 2.442441\n",
      "iteration 6800 / 15000: loss 2.366229\n",
      "iteration 6900 / 15000: loss 2.268229\n",
      "iteration 7000 / 15000: loss 2.289766\n",
      "iteration 7100 / 15000: loss 2.239156\n",
      "iteration 7200 / 15000: loss 2.271043\n",
      "iteration 7300 / 15000: loss 2.177054\n",
      "iteration 7400 / 15000: loss 2.207754\n",
      "iteration 7500 / 15000: loss 2.148666\n",
      "iteration 7600 / 15000: loss 2.220980\n",
      "iteration 7700 / 15000: loss 2.170741\n",
      "iteration 7800 / 15000: loss 2.145409\n",
      "iteration 7900 / 15000: loss 2.118179\n",
      "iteration 8000 / 15000: loss 2.150026\n",
      "iteration 8100 / 15000: loss 2.096385\n",
      "iteration 8200 / 15000: loss 2.038996\n",
      "iteration 8300 / 15000: loss 2.089663\n",
      "iteration 8400 / 15000: loss 2.001542\n",
      "iteration 8500 / 15000: loss 2.039925\n",
      "iteration 8600 / 15000: loss 2.067661\n",
      "iteration 8700 / 15000: loss 2.008145\n",
      "iteration 8800 / 15000: loss 2.147677\n",
      "iteration 8900 / 15000: loss 2.055732\n",
      "iteration 9000 / 15000: loss 2.073154\n",
      "iteration 9100 / 15000: loss 2.085747\n",
      "iteration 9200 / 15000: loss 2.098175\n",
      "iteration 9300 / 15000: loss 1.911199\n",
      "iteration 9400 / 15000: loss 2.051739\n",
      "iteration 9500 / 15000: loss 2.001487\n",
      "iteration 9600 / 15000: loss 2.090930\n",
      "iteration 9700 / 15000: loss 2.017643\n",
      "iteration 9800 / 15000: loss 1.937106\n",
      "iteration 9900 / 15000: loss 2.019567\n",
      "iteration 10000 / 15000: loss 1.961751\n",
      "iteration 10100 / 15000: loss 2.064297\n",
      "iteration 10200 / 15000: loss 2.081967\n",
      "iteration 10300 / 15000: loss 1.994905\n",
      "iteration 10400 / 15000: loss 1.984320\n",
      "iteration 10500 / 15000: loss 2.102222\n",
      "iteration 10600 / 15000: loss 2.117033\n",
      "iteration 10700 / 15000: loss 2.081307\n",
      "iteration 10800 / 15000: loss 2.021713\n",
      "iteration 10900 / 15000: loss 2.097600\n",
      "iteration 11000 / 15000: loss 2.003679\n",
      "iteration 11100 / 15000: loss 2.005180\n",
      "iteration 11200 / 15000: loss 2.021869\n",
      "iteration 11300 / 15000: loss 2.020788\n",
      "iteration 11400 / 15000: loss 1.979226\n",
      "iteration 11500 / 15000: loss 2.073632\n",
      "iteration 11600 / 15000: loss 1.950557\n",
      "iteration 11700 / 15000: loss 2.009210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11800 / 15000: loss 1.980310\n",
      "iteration 11900 / 15000: loss 1.986480\n",
      "iteration 12000 / 15000: loss 2.002272\n",
      "iteration 12100 / 15000: loss 2.021015\n",
      "iteration 12200 / 15000: loss 2.040435\n",
      "iteration 12300 / 15000: loss 2.038004\n",
      "iteration 12400 / 15000: loss 1.968547\n",
      "iteration 12500 / 15000: loss 1.941861\n",
      "iteration 12600 / 15000: loss 2.031197\n",
      "iteration 12700 / 15000: loss 1.968365\n",
      "iteration 12800 / 15000: loss 2.009829\n",
      "iteration 12900 / 15000: loss 2.012788\n",
      "iteration 13000 / 15000: loss 1.925341\n",
      "iteration 13100 / 15000: loss 2.012950\n",
      "iteration 13200 / 15000: loss 1.983737\n",
      "iteration 13300 / 15000: loss 2.026584\n",
      "iteration 13400 / 15000: loss 1.977539\n",
      "iteration 13500 / 15000: loss 2.040528\n",
      "iteration 13600 / 15000: loss 2.031436\n",
      "iteration 13700 / 15000: loss 1.986413\n",
      "iteration 13800 / 15000: loss 2.027106\n",
      "iteration 13900 / 15000: loss 1.982297\n",
      "iteration 14000 / 15000: loss 2.039781\n",
      "iteration 14100 / 15000: loss 2.025144\n",
      "iteration 14200 / 15000: loss 2.041558\n",
      "iteration 14300 / 15000: loss 2.066206\n",
      "iteration 14400 / 15000: loss 2.081771\n",
      "iteration 14500 / 15000: loss 1.965591\n",
      "iteration 14600 / 15000: loss 1.960623\n",
      "iteration 14700 / 15000: loss 2.085837\n",
      "iteration 14800 / 15000: loss 1.965677\n",
      "iteration 14900 / 15000: loss 2.026796\n",
      "\n",
      "for r= 12500.0  and l= 2.5e-08\n",
      "iteration 0 / 15000: loss 394.947653\n",
      "iteration 100 / 15000: loss 348.637489\n",
      "iteration 200 / 15000: loss 306.832434\n",
      "iteration 300 / 15000: loss 270.638029\n",
      "iteration 400 / 15000: loss 238.699664\n",
      "iteration 500 / 15000: loss 210.516876\n",
      "iteration 600 / 15000: loss 186.146938\n",
      "iteration 700 / 15000: loss 164.151973\n",
      "iteration 800 / 15000: loss 145.080783\n",
      "iteration 900 / 15000: loss 128.187447\n",
      "iteration 1000 / 15000: loss 113.175204\n",
      "iteration 1100 / 15000: loss 99.961076\n",
      "iteration 1200 / 15000: loss 88.288300\n",
      "iteration 1300 / 15000: loss 78.241499\n",
      "iteration 1400 / 15000: loss 69.197186\n",
      "iteration 1500 / 15000: loss 61.284505\n",
      "iteration 1600 / 15000: loss 54.341766\n",
      "iteration 1700 / 15000: loss 48.161629\n",
      "iteration 1800 / 15000: loss 42.750477\n",
      "iteration 1900 / 15000: loss 37.988743\n",
      "iteration 2000 / 15000: loss 33.616696\n",
      "iteration 2100 / 15000: loss 29.976196\n",
      "iteration 2200 / 15000: loss 26.601179\n",
      "iteration 2300 / 15000: loss 23.820435\n",
      "iteration 2400 / 15000: loss 21.176185\n",
      "iteration 2500 / 15000: loss 18.891462\n",
      "iteration 2600 / 15000: loss 16.885715\n",
      "iteration 2700 / 15000: loss 15.134587\n",
      "iteration 2800 / 15000: loss 13.610858\n",
      "iteration 2900 / 15000: loss 12.162296\n",
      "iteration 3000 / 15000: loss 11.023797\n",
      "iteration 3100 / 15000: loss 9.982800\n",
      "iteration 3200 / 15000: loss 9.079903\n",
      "iteration 3300 / 15000: loss 8.273769\n",
      "iteration 3400 / 15000: loss 7.447457\n",
      "iteration 3500 / 15000: loss 6.835743\n",
      "iteration 3600 / 15000: loss 6.260431\n",
      "iteration 3700 / 15000: loss 5.787485\n",
      "iteration 3800 / 15000: loss 5.328037\n",
      "iteration 3900 / 15000: loss 5.014705\n",
      "iteration 4000 / 15000: loss 4.607613\n",
      "iteration 4100 / 15000: loss 4.336334\n",
      "iteration 4200 / 15000: loss 4.017281\n",
      "iteration 4300 / 15000: loss 3.750362\n",
      "iteration 4400 / 15000: loss 3.528364\n",
      "iteration 4500 / 15000: loss 3.407663\n",
      "iteration 4600 / 15000: loss 3.259178\n",
      "iteration 4700 / 15000: loss 3.100926\n",
      "iteration 4800 / 15000: loss 2.901926\n",
      "iteration 4900 / 15000: loss 2.819157\n",
      "iteration 5000 / 15000: loss 2.788861\n",
      "iteration 5100 / 15000: loss 2.734261\n",
      "iteration 5200 / 15000: loss 2.607152\n",
      "iteration 5300 / 15000: loss 2.475596\n",
      "iteration 5400 / 15000: loss 2.470218\n",
      "iteration 5500 / 15000: loss 2.464936\n",
      "iteration 5600 / 15000: loss 2.352331\n",
      "iteration 5700 / 15000: loss 2.375541\n",
      "iteration 5800 / 15000: loss 2.320330\n",
      "iteration 5900 / 15000: loss 2.245931\n",
      "iteration 6000 / 15000: loss 2.264694\n",
      "iteration 6100 / 15000: loss 2.177069\n",
      "iteration 6200 / 15000: loss 2.158308\n",
      "iteration 6300 / 15000: loss 2.166167\n",
      "iteration 6400 / 15000: loss 2.133191\n",
      "iteration 6500 / 15000: loss 2.153749\n",
      "iteration 6600 / 15000: loss 2.082279\n",
      "iteration 6700 / 15000: loss 2.126775\n",
      "iteration 6800 / 15000: loss 2.096343\n",
      "iteration 6900 / 15000: loss 2.052770\n",
      "iteration 7000 / 15000: loss 2.115201\n",
      "iteration 7100 / 15000: loss 2.069168\n",
      "iteration 7200 / 15000: loss 2.178810\n",
      "iteration 7300 / 15000: loss 2.083098\n",
      "iteration 7400 / 15000: loss 2.015834\n",
      "iteration 7500 / 15000: loss 2.100053\n",
      "iteration 7600 / 15000: loss 2.110307\n",
      "iteration 7700 / 15000: loss 2.109521\n",
      "iteration 7800 / 15000: loss 2.127905\n",
      "iteration 7900 / 15000: loss 2.053668\n",
      "iteration 8000 / 15000: loss 2.040685\n",
      "iteration 8100 / 15000: loss 1.994288\n",
      "iteration 8200 / 15000: loss 2.028368\n",
      "iteration 8300 / 15000: loss 2.052506\n",
      "iteration 8400 / 15000: loss 2.051422\n",
      "iteration 8500 / 15000: loss 1.980436\n",
      "iteration 8600 / 15000: loss 2.080982\n",
      "iteration 8700 / 15000: loss 2.028980\n",
      "iteration 8800 / 15000: loss 2.009320\n",
      "iteration 8900 / 15000: loss 1.956623\n",
      "iteration 9000 / 15000: loss 1.960345\n",
      "iteration 9100 / 15000: loss 1.954845\n",
      "iteration 9200 / 15000: loss 1.987674\n",
      "iteration 9300 / 15000: loss 2.039755\n",
      "iteration 9400 / 15000: loss 2.026917\n",
      "iteration 9500 / 15000: loss 2.036421\n",
      "iteration 9600 / 15000: loss 1.899652\n",
      "iteration 9700 / 15000: loss 1.995962\n",
      "iteration 9800 / 15000: loss 1.981140\n",
      "iteration 9900 / 15000: loss 1.986870\n",
      "iteration 10000 / 15000: loss 2.070557\n",
      "iteration 10100 / 15000: loss 2.002128\n",
      "iteration 10200 / 15000: loss 2.006799\n",
      "iteration 10300 / 15000: loss 2.015666\n",
      "iteration 10400 / 15000: loss 2.001604\n",
      "iteration 10500 / 15000: loss 2.008366\n",
      "iteration 10600 / 15000: loss 2.026994\n",
      "iteration 10700 / 15000: loss 2.009864\n",
      "iteration 10800 / 15000: loss 2.071114\n",
      "iteration 10900 / 15000: loss 1.941050\n",
      "iteration 11000 / 15000: loss 2.030545\n",
      "iteration 11100 / 15000: loss 2.044343\n",
      "iteration 11200 / 15000: loss 1.991726\n",
      "iteration 11300 / 15000: loss 1.992813\n",
      "iteration 11400 / 15000: loss 1.988395\n",
      "iteration 11500 / 15000: loss 2.014692\n",
      "iteration 11600 / 15000: loss 2.081463\n",
      "iteration 11700 / 15000: loss 2.035421\n",
      "iteration 11800 / 15000: loss 2.038573\n",
      "iteration 11900 / 15000: loss 2.029489\n",
      "iteration 12000 / 15000: loss 2.042253\n",
      "iteration 12100 / 15000: loss 2.060616\n",
      "iteration 12200 / 15000: loss 1.975510\n",
      "iteration 12300 / 15000: loss 2.067108\n",
      "iteration 12400 / 15000: loss 2.024161\n",
      "iteration 12500 / 15000: loss 2.008110\n",
      "iteration 12600 / 15000: loss 2.009765\n",
      "iteration 12700 / 15000: loss 2.095145\n",
      "iteration 12800 / 15000: loss 1.991200\n",
      "iteration 12900 / 15000: loss 2.012729\n",
      "iteration 13000 / 15000: loss 2.048840\n",
      "iteration 13100 / 15000: loss 2.020543\n",
      "iteration 13200 / 15000: loss 2.009077\n",
      "iteration 13300 / 15000: loss 1.974734\n",
      "iteration 13400 / 15000: loss 1.964563\n",
      "iteration 13500 / 15000: loss 2.032187\n",
      "iteration 13600 / 15000: loss 1.981127\n",
      "iteration 13700 / 15000: loss 2.097907\n",
      "iteration 13800 / 15000: loss 2.046052\n",
      "iteration 13900 / 15000: loss 1.950330\n",
      "iteration 14000 / 15000: loss 2.017223\n",
      "iteration 14100 / 15000: loss 2.014449\n",
      "iteration 14200 / 15000: loss 1.989175\n",
      "iteration 14300 / 15000: loss 1.991839\n",
      "iteration 14400 / 15000: loss 2.091553\n",
      "iteration 14500 / 15000: loss 1.974637\n",
      "iteration 14600 / 15000: loss 2.052859\n",
      "iteration 14700 / 15000: loss 2.040288\n",
      "iteration 14800 / 15000: loss 2.056316\n",
      "iteration 14900 / 15000: loss 2.024132\n",
      "\n",
      "for r= 15000.0  and l= 2.5e-08\n",
      "iteration 0 / 15000: loss 473.462648\n",
      "iteration 100 / 15000: loss 406.898740\n",
      "iteration 200 / 15000: loss 349.280300\n",
      "iteration 300 / 15000: loss 300.876289\n",
      "iteration 400 / 15000: loss 258.906957\n",
      "iteration 500 / 15000: loss 222.947956\n",
      "iteration 600 / 15000: loss 192.331034\n",
      "iteration 700 / 15000: loss 165.375702\n",
      "iteration 800 / 15000: loss 142.697032\n",
      "iteration 900 / 15000: loss 122.995335\n",
      "iteration 1000 / 15000: loss 106.147343\n",
      "iteration 1100 / 15000: loss 91.503330\n",
      "iteration 1200 / 15000: loss 79.035352\n",
      "iteration 1300 / 15000: loss 68.218350\n",
      "iteration 1400 / 15000: loss 58.889089\n",
      "iteration 1500 / 15000: loss 51.044719\n",
      "iteration 1600 / 15000: loss 44.174298\n",
      "iteration 1700 / 15000: loss 38.202203\n",
      "iteration 1800 / 15000: loss 33.216378\n",
      "iteration 1900 / 15000: loss 28.820783\n",
      "iteration 2000 / 15000: loss 25.146250\n",
      "iteration 2100 / 15000: loss 21.915618\n",
      "iteration 2200 / 15000: loss 19.135068\n",
      "iteration 2300 / 15000: loss 16.667713\n",
      "iteration 2400 / 15000: loss 14.668600\n",
      "iteration 2500 / 15000: loss 12.956263\n",
      "iteration 2600 / 15000: loss 11.277975\n",
      "iteration 2700 / 15000: loss 10.096396\n",
      "iteration 2800 / 15000: loss 9.024806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2900 / 15000: loss 7.972953\n",
      "iteration 3000 / 15000: loss 7.117849\n",
      "iteration 3100 / 15000: loss 6.405995\n",
      "iteration 3200 / 15000: loss 5.807674\n",
      "iteration 3300 / 15000: loss 5.298691\n",
      "iteration 3400 / 15000: loss 4.860607\n",
      "iteration 3500 / 15000: loss 4.428950\n",
      "iteration 3600 / 15000: loss 4.070945\n",
      "iteration 3700 / 15000: loss 3.830449\n",
      "iteration 3800 / 15000: loss 3.550514\n",
      "iteration 3900 / 15000: loss 3.382174\n",
      "iteration 4000 / 15000: loss 3.213259\n",
      "iteration 4100 / 15000: loss 3.015075\n",
      "iteration 4200 / 15000: loss 2.838260\n",
      "iteration 4300 / 15000: loss 2.708908\n",
      "iteration 4400 / 15000: loss 2.696269\n",
      "iteration 4500 / 15000: loss 2.619547\n",
      "iteration 4600 / 15000: loss 2.504394\n",
      "iteration 4700 / 15000: loss 2.492182\n",
      "iteration 4800 / 15000: loss 2.364983\n",
      "iteration 4900 / 15000: loss 2.228491\n",
      "iteration 5000 / 15000: loss 2.289638\n",
      "iteration 5100 / 15000: loss 2.326215\n",
      "iteration 5200 / 15000: loss 2.156134\n",
      "iteration 5300 / 15000: loss 2.240287\n",
      "iteration 5400 / 15000: loss 2.159936\n",
      "iteration 5500 / 15000: loss 2.127915\n",
      "iteration 5600 / 15000: loss 2.149546\n",
      "iteration 5700 / 15000: loss 2.146708\n",
      "iteration 5800 / 15000: loss 2.121428\n",
      "iteration 5900 / 15000: loss 2.071786\n",
      "iteration 6000 / 15000: loss 2.089667\n",
      "iteration 6100 / 15000: loss 2.058950\n",
      "iteration 6200 / 15000: loss 2.063014\n",
      "iteration 6300 / 15000: loss 2.097857\n",
      "iteration 6400 / 15000: loss 2.109577\n",
      "iteration 6500 / 15000: loss 2.042589\n",
      "iteration 6600 / 15000: loss 2.047925\n",
      "iteration 6700 / 15000: loss 2.094670\n",
      "iteration 6800 / 15000: loss 2.026407\n",
      "iteration 6900 / 15000: loss 2.042126\n",
      "iteration 7000 / 15000: loss 2.033943\n",
      "iteration 7100 / 15000: loss 2.025139\n",
      "iteration 7200 / 15000: loss 2.089922\n",
      "iteration 7300 / 15000: loss 2.079562\n",
      "iteration 7400 / 15000: loss 2.075198\n",
      "iteration 7500 / 15000: loss 2.006495\n",
      "iteration 7600 / 15000: loss 2.043794\n",
      "iteration 7700 / 15000: loss 2.075387\n",
      "iteration 7800 / 15000: loss 2.028864\n",
      "iteration 7900 / 15000: loss 2.023609\n",
      "iteration 8000 / 15000: loss 1.996975\n",
      "iteration 8100 / 15000: loss 1.998492\n",
      "iteration 8200 / 15000: loss 2.037658\n",
      "iteration 8300 / 15000: loss 1.997764\n",
      "iteration 8400 / 15000: loss 2.074155\n",
      "iteration 8500 / 15000: loss 2.046689\n",
      "iteration 8600 / 15000: loss 2.042758\n",
      "iteration 8700 / 15000: loss 2.023429\n",
      "iteration 8800 / 15000: loss 1.973901\n",
      "iteration 8900 / 15000: loss 1.975047\n",
      "iteration 9000 / 15000: loss 2.123586\n",
      "iteration 9100 / 15000: loss 2.032036\n",
      "iteration 9200 / 15000: loss 2.067964\n",
      "iteration 9300 / 15000: loss 1.978577\n",
      "iteration 9400 / 15000: loss 2.015347\n",
      "iteration 9500 / 15000: loss 2.096713\n",
      "iteration 9600 / 15000: loss 1.997783\n",
      "iteration 9700 / 15000: loss 2.045371\n",
      "iteration 9800 / 15000: loss 2.044083\n",
      "iteration 9900 / 15000: loss 2.029009\n",
      "iteration 10000 / 15000: loss 2.068198\n",
      "iteration 10100 / 15000: loss 2.053082\n",
      "iteration 10200 / 15000: loss 1.967753\n",
      "iteration 10300 / 15000: loss 2.007960\n",
      "iteration 10400 / 15000: loss 2.055219\n",
      "iteration 10500 / 15000: loss 2.082780\n",
      "iteration 10600 / 15000: loss 2.082846\n",
      "iteration 10700 / 15000: loss 2.017602\n",
      "iteration 10800 / 15000: loss 2.032649\n",
      "iteration 10900 / 15000: loss 2.061158\n",
      "iteration 11000 / 15000: loss 2.003045\n",
      "iteration 11100 / 15000: loss 2.047975\n",
      "iteration 11200 / 15000: loss 1.992492\n",
      "iteration 11300 / 15000: loss 2.086829\n",
      "iteration 11400 / 15000: loss 2.040288\n",
      "iteration 11500 / 15000: loss 1.963752\n",
      "iteration 11600 / 15000: loss 1.993856\n",
      "iteration 11700 / 15000: loss 2.011064\n",
      "iteration 11800 / 15000: loss 2.030667\n",
      "iteration 11900 / 15000: loss 2.058123\n",
      "iteration 12000 / 15000: loss 2.044904\n",
      "iteration 12100 / 15000: loss 1.971462\n",
      "iteration 12200 / 15000: loss 2.045557\n",
      "iteration 12300 / 15000: loss 2.061319\n",
      "iteration 12400 / 15000: loss 1.990869\n",
      "iteration 12500 / 15000: loss 2.016083\n",
      "iteration 12600 / 15000: loss 2.010059\n",
      "iteration 12700 / 15000: loss 2.087300\n",
      "iteration 12800 / 15000: loss 2.043700\n",
      "iteration 12900 / 15000: loss 2.021229\n",
      "iteration 13000 / 15000: loss 1.995220\n",
      "iteration 13100 / 15000: loss 2.068418\n",
      "iteration 13200 / 15000: loss 1.996935\n",
      "iteration 13300 / 15000: loss 2.044375\n",
      "iteration 13400 / 15000: loss 2.041004\n",
      "iteration 13500 / 15000: loss 2.025863\n",
      "iteration 13600 / 15000: loss 2.088784\n",
      "iteration 13700 / 15000: loss 2.015258\n",
      "iteration 13800 / 15000: loss 2.028170\n",
      "iteration 13900 / 15000: loss 1.997430\n",
      "iteration 14000 / 15000: loss 2.080026\n",
      "iteration 14100 / 15000: loss 2.035008\n",
      "iteration 14200 / 15000: loss 2.044628\n",
      "iteration 14300 / 15000: loss 2.079967\n",
      "iteration 14400 / 15000: loss 1.999579\n",
      "iteration 14500 / 15000: loss 1.961111\n",
      "iteration 14600 / 15000: loss 2.030854\n",
      "iteration 14700 / 15000: loss 2.033735\n",
      "iteration 14800 / 15000: loss 2.083143\n",
      "iteration 14900 / 15000: loss 2.010663\n",
      "\n",
      "for r= 20000.0  and l= 2.5e-08\n",
      "iteration 0 / 15000: loss 622.410662\n",
      "iteration 100 / 15000: loss 508.659292\n",
      "iteration 200 / 15000: loss 416.788734\n",
      "iteration 300 / 15000: loss 341.210494\n",
      "iteration 400 / 15000: loss 279.349285\n",
      "iteration 500 / 15000: loss 229.002398\n",
      "iteration 600 / 15000: loss 187.742896\n",
      "iteration 700 / 15000: loss 153.908494\n",
      "iteration 800 / 15000: loss 126.297268\n",
      "iteration 900 / 15000: loss 103.734053\n",
      "iteration 1000 / 15000: loss 85.106612\n",
      "iteration 1100 / 15000: loss 70.097420\n",
      "iteration 1200 / 15000: loss 57.782353\n",
      "iteration 1300 / 15000: loss 47.630404\n",
      "iteration 1400 / 15000: loss 39.312390\n",
      "iteration 1500 / 15000: loss 32.559790\n",
      "iteration 1600 / 15000: loss 27.013028\n",
      "iteration 1700 / 15000: loss 22.413754\n",
      "iteration 1800 / 15000: loss 18.701134\n",
      "iteration 1900 / 15000: loss 15.644115\n",
      "iteration 2000 / 15000: loss 13.289720\n",
      "iteration 2100 / 15000: loss 11.204300\n",
      "iteration 2200 / 15000: loss 9.522447\n",
      "iteration 2300 / 15000: loss 8.201153\n",
      "iteration 2400 / 15000: loss 7.101202\n",
      "iteration 2500 / 15000: loss 6.154059\n",
      "iteration 2600 / 15000: loss 5.476716\n",
      "iteration 2700 / 15000: loss 4.809025\n",
      "iteration 2800 / 15000: loss 4.350227\n",
      "iteration 2900 / 15000: loss 3.883376\n",
      "iteration 3000 / 15000: loss 3.499093\n",
      "iteration 3100 / 15000: loss 3.304389\n",
      "iteration 3200 / 15000: loss 3.134918\n",
      "iteration 3300 / 15000: loss 2.911783\n",
      "iteration 3400 / 15000: loss 2.759553\n",
      "iteration 3500 / 15000: loss 2.603569\n",
      "iteration 3600 / 15000: loss 2.544392\n",
      "iteration 3700 / 15000: loss 2.438660\n",
      "iteration 3800 / 15000: loss 2.347105\n",
      "iteration 3900 / 15000: loss 2.304457\n",
      "iteration 4000 / 15000: loss 2.280903\n",
      "iteration 4100 / 15000: loss 2.206047\n",
      "iteration 4200 / 15000: loss 2.160911\n",
      "iteration 4300 / 15000: loss 2.149863\n",
      "iteration 4400 / 15000: loss 2.119499\n",
      "iteration 4500 / 15000: loss 2.138160\n",
      "iteration 4600 / 15000: loss 2.155558\n",
      "iteration 4700 / 15000: loss 2.061735\n",
      "iteration 4800 / 15000: loss 2.108774\n",
      "iteration 4900 / 15000: loss 2.092744\n",
      "iteration 5000 / 15000: loss 2.085507\n",
      "iteration 5100 / 15000: loss 2.100118\n",
      "iteration 5200 / 15000: loss 2.085296\n",
      "iteration 5300 / 15000: loss 2.068092\n",
      "iteration 5400 / 15000: loss 2.104199\n",
      "iteration 5500 / 15000: loss 2.141654\n",
      "iteration 5600 / 15000: loss 2.124845\n",
      "iteration 5700 / 15000: loss 2.083092\n",
      "iteration 5800 / 15000: loss 2.092774\n",
      "iteration 5900 / 15000: loss 2.042556\n",
      "iteration 6000 / 15000: loss 2.035972\n",
      "iteration 6100 / 15000: loss 2.022505\n",
      "iteration 6200 / 15000: loss 2.128125\n",
      "iteration 6300 / 15000: loss 2.139995\n",
      "iteration 6400 / 15000: loss 2.004918\n",
      "iteration 6500 / 15000: loss 2.026236\n",
      "iteration 6600 / 15000: loss 2.127927\n",
      "iteration 6700 / 15000: loss 2.053649\n",
      "iteration 6800 / 15000: loss 2.039754\n",
      "iteration 6900 / 15000: loss 2.105902\n",
      "iteration 7000 / 15000: loss 2.042742\n",
      "iteration 7100 / 15000: loss 2.017049\n",
      "iteration 7200 / 15000: loss 2.153541\n",
      "iteration 7300 / 15000: loss 2.065171\n",
      "iteration 7400 / 15000: loss 2.030265\n",
      "iteration 7500 / 15000: loss 2.127805\n",
      "iteration 7600 / 15000: loss 2.028536\n",
      "iteration 7700 / 15000: loss 2.094717\n",
      "iteration 7800 / 15000: loss 2.092512\n",
      "iteration 7900 / 15000: loss 2.129376\n",
      "iteration 8000 / 15000: loss 2.078796\n",
      "iteration 8100 / 15000: loss 2.087130\n",
      "iteration 8200 / 15000: loss 2.067279\n",
      "iteration 8300 / 15000: loss 2.035088\n",
      "iteration 8400 / 15000: loss 2.075657\n",
      "iteration 8500 / 15000: loss 2.125669\n",
      "iteration 8600 / 15000: loss 2.115176\n",
      "iteration 8700 / 15000: loss 2.061528\n",
      "iteration 8800 / 15000: loss 2.053264\n",
      "iteration 8900 / 15000: loss 2.085098\n",
      "iteration 9000 / 15000: loss 2.063853\n",
      "iteration 9100 / 15000: loss 1.993921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9200 / 15000: loss 1.975530\n",
      "iteration 9300 / 15000: loss 2.071610\n",
      "iteration 9400 / 15000: loss 2.105057\n",
      "iteration 9500 / 15000: loss 2.096677\n",
      "iteration 9600 / 15000: loss 2.094325\n",
      "iteration 9700 / 15000: loss 2.055727\n",
      "iteration 9800 / 15000: loss 2.095745\n",
      "iteration 9900 / 15000: loss 2.089795\n",
      "iteration 10000 / 15000: loss 2.030305\n",
      "iteration 10100 / 15000: loss 2.091648\n",
      "iteration 10200 / 15000: loss 2.037734\n",
      "iteration 10300 / 15000: loss 2.043212\n",
      "iteration 10400 / 15000: loss 2.037177\n",
      "iteration 10500 / 15000: loss 2.082056\n",
      "iteration 10600 / 15000: loss 2.045447\n",
      "iteration 10700 / 15000: loss 2.039743\n",
      "iteration 10800 / 15000: loss 2.074437\n",
      "iteration 10900 / 15000: loss 2.081903\n",
      "iteration 11000 / 15000: loss 2.052803\n",
      "iteration 11100 / 15000: loss 2.083270\n",
      "iteration 11200 / 15000: loss 2.063293\n",
      "iteration 11300 / 15000: loss 2.090298\n",
      "iteration 11400 / 15000: loss 2.101534\n",
      "iteration 11500 / 15000: loss 2.093414\n",
      "iteration 11600 / 15000: loss 2.058043\n",
      "iteration 11700 / 15000: loss 2.083858\n",
      "iteration 11800 / 15000: loss 2.101826\n",
      "iteration 11900 / 15000: loss 2.003159\n",
      "iteration 12000 / 15000: loss 2.017379\n",
      "iteration 12100 / 15000: loss 2.078220\n",
      "iteration 12200 / 15000: loss 2.056004\n",
      "iteration 12300 / 15000: loss 2.099039\n",
      "iteration 12400 / 15000: loss 2.108834\n",
      "iteration 12500 / 15000: loss 2.065728\n",
      "iteration 12600 / 15000: loss 2.052334\n",
      "iteration 12700 / 15000: loss 2.079300\n",
      "iteration 12800 / 15000: loss 2.090994\n",
      "iteration 12900 / 15000: loss 2.037237\n",
      "iteration 13000 / 15000: loss 2.033852\n",
      "iteration 13100 / 15000: loss 2.099314\n",
      "iteration 13200 / 15000: loss 2.097259\n",
      "iteration 13300 / 15000: loss 2.078311\n",
      "iteration 13400 / 15000: loss 2.000373\n",
      "iteration 13500 / 15000: loss 2.021975\n",
      "iteration 13600 / 15000: loss 2.062984\n",
      "iteration 13700 / 15000: loss 2.075355\n",
      "iteration 13800 / 15000: loss 2.061887\n",
      "iteration 13900 / 15000: loss 2.030532\n",
      "iteration 14000 / 15000: loss 2.022483\n",
      "iteration 14100 / 15000: loss 2.092120\n",
      "iteration 14200 / 15000: loss 2.073933\n",
      "iteration 14300 / 15000: loss 2.105228\n",
      "iteration 14400 / 15000: loss 2.119473\n",
      "iteration 14500 / 15000: loss 2.068126\n",
      "iteration 14600 / 15000: loss 2.090593\n",
      "iteration 14700 / 15000: loss 2.092727\n",
      "iteration 14800 / 15000: loss 2.062462\n",
      "iteration 14900 / 15000: loss 2.096802\n",
      "\n",
      "lr 8.000000e-09 reg 8.000000e+03 train accuracy: 0.334918 val accuracy: 0.336000\n",
      "lr 8.000000e-09 reg 1.000000e+04 train accuracy: 0.344980 val accuracy: 0.350000\n",
      "lr 8.000000e-09 reg 1.250000e+04 train accuracy: 0.343898 val accuracy: 0.355000\n",
      "lr 8.000000e-09 reg 1.500000e+04 train accuracy: 0.344184 val accuracy: 0.362000\n",
      "lr 8.000000e-09 reg 2.000000e+04 train accuracy: 0.335449 val accuracy: 0.351000\n",
      "lr 1.000000e-08 reg 8.000000e+03 train accuracy: 0.353755 val accuracy: 0.372000\n",
      "lr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.353592 val accuracy: 0.369000\n",
      "lr 1.000000e-08 reg 1.250000e+04 train accuracy: 0.350980 val accuracy: 0.368000\n",
      "lr 1.000000e-08 reg 1.500000e+04 train accuracy: 0.345429 val accuracy: 0.362000\n",
      "lr 1.000000e-08 reg 2.000000e+04 train accuracy: 0.337878 val accuracy: 0.349000\n",
      "lr 1.500000e-08 reg 8.000000e+03 train accuracy: 0.362633 val accuracy: 0.377000\n",
      "lr 1.500000e-08 reg 1.000000e+04 train accuracy: 0.358694 val accuracy: 0.377000\n",
      "lr 1.500000e-08 reg 1.250000e+04 train accuracy: 0.350408 val accuracy: 0.370000\n",
      "lr 1.500000e-08 reg 1.500000e+04 train accuracy: 0.345939 val accuracy: 0.362000\n",
      "lr 1.500000e-08 reg 2.000000e+04 train accuracy: 0.335612 val accuracy: 0.345000\n",
      "lr 2.500000e-08 reg 8.000000e+03 train accuracy: 0.365673 val accuracy: 0.382000\n",
      "lr 2.500000e-08 reg 1.000000e+04 train accuracy: 0.357143 val accuracy: 0.370000\n",
      "lr 2.500000e-08 reg 1.250000e+04 train accuracy: 0.351735 val accuracy: 0.362000\n",
      "lr 2.500000e-08 reg 1.500000e+04 train accuracy: 0.344163 val accuracy: 0.359000\n",
      "lr 2.500000e-08 reg 2.000000e+04 train accuracy: 0.338347 val accuracy: 0.353000\n",
      "best validation accuracy achieved during cross-validation: 0.382000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [0.08e-7,0.1e-7,0.15e-7,0.25e-7]\n",
    "regularization_strengths = [0.8e4,1.0e4,1.25e4,1.5e4,2e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "num_iters=15000\n",
    "\n",
    "for l in learning_rates:\n",
    "    for r in regularization_strengths:\n",
    "        print(\"for r=\",r,\" and l=\",l)\n",
    "        softmax = Softmax()\n",
    "\n",
    "        softmax.train(X_train, y_train, learning_rate=l, reg=r,\n",
    "                              num_iters=num_iters, verbose=True)\n",
    "        \n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        training_accuracy = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        validation_accuracy=np.mean(y_val == y_val_pred)\n",
    "        results[(l,r)]=(training_accuracy,validation_accuracy) \n",
    "        if validation_accuracy > best_val:\n",
    "            best_val = validation_accuracy\n",
    "            best_softmax = softmax\n",
    "            \n",
    "        print()\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.368000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF8CAYAAADrUz6WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21b06ddc320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
